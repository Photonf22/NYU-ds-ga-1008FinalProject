{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe63928-13f7-4745-83a0-e1bfc251f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from x_transformers import Encoder, Decoder\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9427c2ca-f982-4cb5-b4e8-08df5e65bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PatchEmbed class, adapted from https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632 I think, but I dont have medium premium so idk\n",
    "- This class is used to convert the image into patches using a convolutional layer\n",
    "'''\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding\"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=64):\n",
    "        super().__init__()\n",
    "        if isinstance(img_size, int):\n",
    "            img_size = img_size, img_size\n",
    "        if isinstance(patch_size, int):\n",
    "            patch_size = patch_size, patch_size\n",
    "        #calculate the number of patches\n",
    "        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "\n",
    "        #convolutional layer to convert the image into patches\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        #flatten the patches\n",
    "        x = rearrange(x, 'b e h w -> b (h w) e')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e64aa1-e01b-4171-b7fe-3439959a04c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = self.conv(x)\n",
    "    #flatten the patches\n",
    "    x = rearrange(x, 'b e h w -> b (h w) e')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0149057",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Lightweight Predictor Module using VIT to predict target patches from context patches'''\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, depth):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.predictor = Decoder(dim = embed_dim, depth = depth, heads = num_heads)\n",
    "    def forward(self, context_encoding, target_masks):\n",
    "        x = torch.cat((context_encoding, target_masks), dim = 1)\n",
    "        x = self.predictor(x)\n",
    "        #return last len(target_masks) tokens\n",
    "        l = x.shape[1]\n",
    "        return x[:, l - target_masks.shape[1]:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75dc5177",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Main Model Class'''\n",
    "class IJEPA_base(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim, enc_depth, pred_depth, num_heads, post_emb_norm=False, M = 4, mode=\"train\", layer_dropout=0.):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.mode = mode\n",
    "        self.layer_dropout = layer_dropout\n",
    "\n",
    "        #define the patch embedding and positional embedding\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        self.patch_dim  = (self.patch_embed.patch_shape[0], self.patch_embed.patch_shape[1])\n",
    "        self.num_tokens = self.patch_embed.patch_shape[0] * self.patch_embed.patch_shape[1]\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_tokens, embed_dim))\n",
    "\n",
    "        #define the cls and mask tokens\n",
    "        self.mask_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.mask_token, 0.02)\n",
    "\n",
    "        #define the encoder and decoder, as well as the layer normalization and dropout\n",
    "        self.post_emb_norm = nn.LayerNorm(embed_dim) if post_emb_norm else nn.Identity()\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.teacher_encoder = Encoder(\n",
    "            dim=embed_dim,\n",
    "            heads=num_heads,\n",
    "            depth=enc_depth, \n",
    "            layer_dropout=self.layer_dropout,\n",
    "        )  \n",
    "        self.student_encoder = copy.deepcopy(self.teacher_encoder).cuda()\n",
    "        self.predictor = Predictor(embed_dim, num_heads, pred_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed9757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() \n",
    "def get_target_block(self, target_encoder, x, patch_dim, aspect_ratio, scale, M):  \n",
    "    #get the target block\n",
    "    target_encoder = target_encoder.eval()\n",
    "    x = target_encoder(x)\n",
    "    x = self.norm(x)\n",
    "    #get the patch dimensions\n",
    "    patch_h, patch_w = patch_dim\n",
    "    #get the number of patches\n",
    "    num_patches = patch_h * patch_w\n",
    "    #get the number of patches in the target block\n",
    "    num_patches_block = int(patch_h * patch_w * scale)\n",
    "    #get the height and width of the target block with aspect ratio\n",
    "    block_h = int(torch.sqrt(torch.tensor(num_patches_block / aspect_ratio)))\n",
    "    block_w = int(aspect_ratio * block_h)\n",
    "    #get the patches in the target block\n",
    "    target_block = torch.zeros((M, x.shape[0], block_h*block_w, x.shape[2]))\n",
    "    target_patches = []\n",
    "    all_patches = []\n",
    "    for z in range(M):\n",
    "        #get the starting patch\n",
    "        start_patch_h = torch.randint(0, patch_h - block_h+1, (1,)).item()\n",
    "        start_patch_w = torch.randint(0, patch_w - block_w+1, (1,)).item()\n",
    "        start_patch = start_patch_h * patch_w + start_patch_w\n",
    "\n",
    "        patches = []\n",
    "        #get the patches in the target block\n",
    "        for i in range(block_h):\n",
    "            for j in range(block_w):\n",
    "                patches.append(start_patch + i * patch_w + j)\n",
    "                if start_patch + i * patch_w + j not in all_patches:\n",
    "                    all_patches.append(start_patch + i * patch_w + j)\n",
    "                \n",
    "        #get the target block\n",
    "        target_patches.append(patches)\n",
    "        target_block[z] = x[:, patches, :]\n",
    "    return target_block.cuda(), target_patches, all_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "169cebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_block(self, x, patch_dim, aspect_ratio, scale, target_patches):\n",
    "    patch_h, patch_w = patch_dim\n",
    "    #get the number of patches in the target block\n",
    "    num_patches_block = int(patch_h * patch_w * scale)\n",
    "    #get the height and width of the target block with aspect ratio\n",
    "    block_h = int(torch.sqrt(torch.tensor(num_patches_block / aspect_ratio)))\n",
    "    block_w = int(aspect_ratio * block_h)\n",
    "    #get the starting patch\n",
    "    start_patch_h = torch.randint(0, patch_h - block_h+1, (1,)).item()\n",
    "    start_patch_w = torch.randint(0, patch_w - block_w+1, (1,)).item()\n",
    "    start_patch = start_patch_h * patch_w + start_patch_w\n",
    "    #get the patches in the context_block\n",
    "    patches = []\n",
    "    for i in range(block_h):\n",
    "        for j in range(block_w):\n",
    "            if start_patch + i * patch_w + j not in target_patches: #remove the target patches\n",
    "                patches.append(start_patch + i * patch_w + j)\n",
    "    return x[:, patches, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e004360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, target_aspect_ratio=1, target_scale=1, context_aspect_ratio=1, context_scale=1):\n",
    "    #get the patch embeddings\n",
    "    x = self.patch_embed(x)\n",
    "    b, n, e = x.shape\n",
    "    #add the positional embeddings\n",
    "    x = x + self.pos_embedding\n",
    "    #normalize the embeddings\n",
    "    x = self.post_emb_norm(x)\n",
    "    #if mode is test, we get return full embedding:\n",
    "    if self.mode == 'test':\n",
    "        return self.student_encoder(x)\n",
    "    # #get target embeddings\n",
    "    target_blocks, target_patches, all_patches = self.get_target_block(self.teacher_encoder, x, self.patch_dim, target_aspect_ratio, target_scale, self.M)\n",
    "    m, b, n, e = target_blocks.shape\n",
    "    #get context embedding\n",
    "\n",
    "    context_block = self.get_context_block(x, self.patch_dim, context_aspect_ratio, context_scale, all_patches)\n",
    "    context_encoding = self.student_encoder(context_block)\n",
    "    context_encoding = self.norm(context_encoding)\n",
    "\n",
    "\n",
    "    prediction_blocks = torch.zeros((m, b, n, e)).cuda()\n",
    "    #get the prediction blocks, predict each target block separately\n",
    "    for i in range(m):\n",
    "        target_masks = self.mask_token.repeat(b, n, 1)\n",
    "        target_pos_embedding = self.pos_embedding[:, target_patches[i], :]\n",
    "        target_masks = target_masks + target_pos_embedding\n",
    "        prediction_blocks[i] = self.predictor(context_encoding, target_masks)\n",
    "\n",
    "    return prediction_blocks, target_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f6681e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    "    ModelSummary,\n",
    ")\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "# from model import IJEPA_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92098121",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dummy Dataset'''\n",
    "class IJEPACIFARDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset_path,\n",
    "                 stage='train',\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        img1 =torch.randn(3, 224, 224)\n",
    "        self.data = img1.repeat(100, 1, 1, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77afd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Placeholder for datamodule in pytorch lightning'''\n",
    "'''\n",
    "Placeholder for datamodule in pytorch lightning\n",
    "'''\n",
    "class D2VDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 dataset_path,\n",
    "                 batch_size=16,\n",
    "                 num_workers=4,\n",
    "                 pin_memory=True,\n",
    "                 shuffle=True\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dataset_path = dataset_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = IJEPACIFARDataset(dataset_path=self.dataset_path, stage='train')\n",
    "        self.val_dataset = IJEPACIFARDataset(dataset_path=self.dataset_path, stage='val')\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ed21b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pytorch lightning model\n",
    "'''\n",
    "class IJEPA(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=224,\n",
    "            patch_size=16,\n",
    "            in_chans=3, \n",
    "            embed_dim=64,\n",
    "            enc_heads=8,\n",
    "            enc_depth=8,\n",
    "            decoder_depth=6,\n",
    "            lr=1e-6,\n",
    "            weight_decay=0.05,\n",
    "            target_aspect_ratio = (0.75,1.5),\n",
    "            target_scale = (0.15, .2),\n",
    "            context_aspect_ratio = 1,\n",
    "            context_scale = (0.85,1.0),\n",
    "            M = 4, #number of different target blocks\n",
    "            m=0.996, #momentum\n",
    "            m_start_end = (.996, 1.)\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        #define models\n",
    "        self.model = IJEPA_base(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, \n",
    "                                enc_depth = enc_depth, num_heads=enc_heads, pred_depth=decoder_depth, M=M)\n",
    "\n",
    "        #define hyperparameters\n",
    "        self.M = M\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.m = m\n",
    "        self.target_aspect_ratio = target_aspect_ratio\n",
    "        self.target_scale = target_scale\n",
    "        self.context_aspect_ratio = context_aspect_ratio\n",
    "        self.context_scale = context_scale\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_tokens = (img_size // patch_size) ** 2\n",
    "        self.m_start_end = m_start_end\n",
    "\n",
    "        #define loss\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x, target_aspect_ratio, target_scale, context_aspect_ratio, context_scale):\n",
    "        return self.model(x, target_aspect_ratio, target_scale, context_aspect_ratio, context_scale)\n",
    "    \n",
    "    '''Update momentum for teacher encoder'''\n",
    "    def update_momentum(self, m):\n",
    "        student_model = self.model.student_encoder.eval()\n",
    "        teacher_model = self.model.teacher_encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            for student_param, teacher_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
    "                teacher_param.data.mul_(other=m).add_(other=student_param.data, alpha=1 - m)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        #generate random target and context aspect ratio and scale\n",
    "        target_aspect_ratio = np.random.uniform(self.target_aspect_ratio[0], self.target_aspect_ratio[1])\n",
    "        target_scale = np.random.uniform(self.target_scale[0], self.target_scale[1])\n",
    "        context_aspect_ratio = self.context_aspect_ratio\n",
    "        context_scale = np.random.uniform(self.context_scale[0], self.context_scale[1])\n",
    "\n",
    "        y_student, y_teacher = self(x, target_aspect_ratio, target_scale, context_aspect_ratio, context_scale)\n",
    "        loss = self.criterion(y_student, y_teacher)\n",
    "        self.log('train_loss', loss)\n",
    "                    \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        target_aspect_ratio = np.random.uniform(self.target_aspect_ratio[0], self.target_aspect_ratio[1])\n",
    "        target_scale = np.random.uniform(self.target_scale[0], self.target_scale[1])\n",
    "        context_aspect_ratio = self.context_aspect_ratio\n",
    "        context_scale = np.random.uniform(self.context_scale[0], self.context_scale[1])\n",
    "\n",
    "        y_student, y_teacher = self(x, target_aspect_ratio, target_scale, context_aspect_ratio, context_scale)\n",
    "        loss = self.criterion(y_student, y_teacher)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx):\n",
    "        target_aspect_ratio = np.random.uniform(self.target_aspect_ratio[0], self.target_aspect_ratio[1])\n",
    "        target_scale = np.random.uniform(self.target_scale[0], self.target_scale[1])\n",
    "        context_aspect_ratio = self.context_aspect_ratio\n",
    "        context_scale = 1\n",
    "        self.model.mode = \"test\"\n",
    "\n",
    "        return self(batch, target_aspect_ratio, target_scale, context_aspect_ratio, context_scale) #just get teacher embedding\n",
    "\n",
    "    def on_after_backward(self):\n",
    "        self.update_momentum(self.m)\n",
    "        self.m += (self.m_start_end[1] - self.m_start_end[0]) / self.trainer.estimated_stepping_batches\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.lr,\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c97e8ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long/PhD/Environments/ijepa/lib/python3.12/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/home/long/PhD/Environments/ijepa/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA RTX 3500 Ada Generation Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/long/PhD/Environments/ijepa/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "/home/long/PhD/Environments/ijepa/lib/python3.12/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name                  | Type       | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model                 | IJEPA_base | 3.7 M  | train\n",
      "1 | model.patch_embed     | PatchEmbed | 49.2 K | train\n",
      "2 | model.post_emb_norm   | Identity   | 0      | train\n",
      "3 | model.norm            | LayerNorm  | 128    | train\n",
      "4 | model.teacher_encoder | Encoder    | 1.3 M  | train\n",
      "5 | model.student_encoder | Encoder    | 1.3 M  | train\n",
      "6 | model.predictor       | Predictor  | 985 K  | train\n",
      "7 | criterion             | MSELoss    | 0      | train\n",
      "-------------------------------------------------------------\n",
      "3.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.7 M     Total params\n",
      "14.706    Total estimated model params size (MB)\n",
      "644       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19430d176dda4d619b871e740491c0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Module [IJEPA_base] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      7\u001b[39m model_summary = ModelSummary(max_depth=\u001b[32m2\u001b[39m)\n\u001b[32m      9\u001b[39m trainer = pl.Trainer(\n\u001b[32m     10\u001b[39m     accelerator=\u001b[33m'\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     11\u001b[39m     devices=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     gradient_clip_val=\u001b[32m.1\u001b[39m,\n\u001b[32m     16\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:560\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:49\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     52\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:598\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    591\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    592\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    593\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    594\u001b[39m     ckpt_path,\n\u001b[32m    595\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    596\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    597\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    601\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1011\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1008\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1016\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1053\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1052\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1055\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1082\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1079\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1081\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1082\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1084\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1086\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:145\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:437\u001b[39m, in \u001b[36m_EvaluationLoop._evaluation_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    431\u001b[39m hook_name = \u001b[33m\"\u001b[39m\u001b[33mtest_step\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.testing \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    432\u001b[39m step_args = (\n\u001b[32m    433\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    436\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_processed()\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:329\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    332\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[39m, in \u001b[36mStrategy.validation_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mIJEPA.validation_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m     79\u001b[39m context_aspect_ratio = \u001b[38;5;28mself\u001b[39m.context_aspect_ratio\n\u001b[32m     80\u001b[39m context_scale = np.random.uniform(\u001b[38;5;28mself\u001b[39m.context_scale[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.context_scale[\u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m y_student, y_teacher = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(y_student, y_teacher)\n\u001b[32m     84\u001b[39m \u001b[38;5;28mself\u001b[39m.log(\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mIJEPA.forward\u001b[39m\u001b[34m(self, x, target_aspect_ratio, target_scale, context_aspect_ratio, context_scale)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, target_aspect_ratio, target_scale, context_aspect_ratio, context_scale):\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_scale\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/torch/nn/modules/module.py:399\u001b[39m, in \u001b[36m_forward_unimplemented\u001b[39m\u001b[34m(self, *input)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, *\u001b[38;5;28minput\u001b[39m: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    389\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m    391\u001b[39m \u001b[33;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m \u001b[33;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    400\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] is missing the required \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m function\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    401\u001b[39m     )\n",
      "\u001b[31mNotImplementedError\u001b[39m: Module [IJEPA_base] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset = D2VDataModule(dataset_path='data')\n",
    "\n",
    "    model = IJEPA(img_size=224, patch_size=16, in_chans=3, embed_dim=64, enc_heads=8, enc_depth=8, decoder_depth=6, lr=1e-3)\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "    model_summary = ModelSummary(max_depth=2)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        devices=1,\n",
    "        precision=16,\n",
    "        max_epochs=10,\n",
    "        callbacks=[lr_monitor, model_summary],\n",
    "        gradient_clip_val=.1,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b5987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ijepa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
