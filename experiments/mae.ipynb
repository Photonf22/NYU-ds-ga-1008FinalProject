{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "source": [
    "This example requires the following dependencies to be installed:\n",
    "pip install lightly[timm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rS-dY-5jnG8T",
   "metadata": {
    "id": "rS-dY-5jnG8T"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "1"
   },
   "outputs": [],
   "source": [
    "#!pip install lightly[timm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g5IhAntGsbP2",
   "metadata": {
    "id": "g5IhAntGsbP2"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "class RawImageDataset(Dataset):\n",
    "    \"\"\"Dataset that loads images directly from raw files.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, image_extensions=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "        if image_extensions is None:\n",
    "            image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.JPEG', '*.JPG', '*.PNG']\n",
    "\n",
    "        # Find all image files\n",
    "        self.image_paths = []\n",
    "        print(f\"Searching for images in: {self.root_dir}\")\n",
    "\n",
    "        for pattern in image_extensions:\n",
    "            found = glob.glob(str(self.root_dir / '**' / pattern), recursive=True)\n",
    "            self.image_paths.extend(found)\n",
    "            if found:\n",
    "                print(f\"  Found {len(found)} {pattern} files\")\n",
    "\n",
    "        self.image_paths.sort()\n",
    "        print(f\"Total images found: {len(self.image_paths)}\")\n",
    "\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"\\nWarning: No images found. Directory structure (first 20 items):\")\n",
    "            for item in sorted(self.root_dir.rglob('*'))[:20]:\n",
    "                print(f\"  {item}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        img_path = self.image_paths[idx]\n",
    "\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            img = Image.new('RGB', (96, 96), color='black')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            # import pdb; pdb.set_trace()\n",
    "            # print(img.shape,\"old image shape\")\n",
    "            # img = img[0]\n",
    "            # print(img.shape,\"new image shape\")\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "def download_and_extract_dataset(repo_id, cache_dir=None, max_workers=4):\n",
    "    \"\"\"Download and extract dataset from HuggingFace.\"\"\"\n",
    "\n",
    "    print(f\"Downloading dataset from {repo_id}...\")\n",
    "\n",
    "    try:\n",
    "        local_dir = snapshot_download(\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\",\n",
    "            cache_dir=cache_dir,\n",
    "            max_workers=max_workers,\n",
    "            resume_download=True,\n",
    "        )\n",
    "        print(f\"Dataset downloaded to: {local_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "        print(\"Retrying with single worker...\")\n",
    "        local_dir = snapshot_download(\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\",\n",
    "            cache_dir=cache_dir,\n",
    "            max_workers=1,\n",
    "            resume_download=True,\n",
    "        )\n",
    "        print(f\"Dataset downloaded to: {local_dir}\")\n",
    "\n",
    "    # Extract zip files if present\n",
    "    local_path = Path(local_dir)\n",
    "    zip_files = list(local_path.glob('*.zip'))\n",
    "\n",
    "    if zip_files:\n",
    "        print(f\"\\nFound {len(zip_files)} zip files. Extracting...\")\n",
    "        extract_dir = local_path / 'extracted'\n",
    "        extract_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # for zip_file in zip_files:\n",
    "        #     print(f\"  Extracting {zip_file.name}...\")\n",
    "        #     try:\n",
    "        #         with zipfile.ZipFile(zip_file, 'r') as zf:\n",
    "        #             zf.extractall(extract_dir)\n",
    "        #         print(\"    ✓ Extracted successfully\")\n",
    "        #     except Exception as e:\n",
    "        #         print(f\"    ✗ Error: {e}\")\n",
    "\n",
    "        return extract_dir\n",
    "    else:\n",
    "        print(\"No zip files found, using directory as-is\")\n",
    "        return local_path\n",
    "\n",
    "\n",
    "def get_mae_transform():\n",
    "    \"\"\"Get MAE-compatible transform.\"\"\"\n",
    "    from lightly.transforms import MAETransform\n",
    "    return MAETransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crOS747Gs5Ds",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "68c2ae9e7b694d06b54f4d024728eaa2",
      "4a5bee40ea824c3ab07c5db6c38b1ac5",
      "e65b5ab1013a40ec9a51a9f634756a6f",
      "0d9aeeb9448f4dbca32099a74dc50d87",
      "be98e9fee6c34964a9ade441c8cff257",
      "903e45db6299410c8f2c23786fc30423",
      "c01b079103944ccd8e40309494385418",
      "29808084c82149379bd4e300c7e5b266",
      "ca69d9cae0064a15b2b887740ecf591a",
      "e4aa1dba7ce54730a302875831cfe11f",
      "f733db29b4214296a9311a76ffb24ea8"
     ]
    },
    "id": "crOS747Gs5Ds",
    "outputId": "c3b038a2-1fee-4d14-ab97-43d1732c171f"
   },
   "outputs": [],
   "source": [
    "# Download and extract dataset\n",
    "# data_dir = download_and_extract_dataset(\n",
    "#     repo_id=\"tsbpp/fall2025_deeplearning\",\n",
    "#     cache_dir=None,\n",
    "#     max_workers=4\n",
    "# )\n",
    "data_dir = Path('./data/devel')\n",
    "\n",
    "# Create transform\n",
    "transform = get_mae_transform()\n",
    "\n",
    "# Create dataset\n",
    "dataset = RawImageDataset(data_dir, transform=transform)\n",
    "print(f\"\\nDataset ready with {len(dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "2"
   },
   "outputs": [],
   "source": [
    "# Note: The model and training settings do not follow the reference settings\n",
    "# from the paper. The settings are chosen such that the example can easily be\n",
    "# run on a small dataset with a single GPU.\n",
    "import torch\n",
    "import torchvision\n",
    "from timm.models.vision_transformer import vit_base_patch32_224\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "3"
   },
   "outputs": [],
   "source": [
    "from lightly.models import utils\n",
    "from lightly.models.modules import MAEDecoderTIMM, MaskedVisionTransformerTIMM\n",
    "from lightly.transforms import MAETransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "4"
   },
   "outputs": [],
   "source": [
    "class MAE(nn.Module):\n",
    "    def __init__(self, vit):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_dim = 512\n",
    "        self.mask_ratio = 0.75\n",
    "        self.patch_size = vit.patch_embed.patch_size[0]\n",
    "\n",
    "        self.backbone = MaskedVisionTransformerTIMM(vit=vit)\n",
    "        self.sequence_length = self.backbone.sequence_length\n",
    "        self.decoder = MAEDecoderTIMM(\n",
    "            num_patches=vit.patch_embed.num_patches,\n",
    "            patch_size=self.patch_size,\n",
    "            embed_dim=vit.embed_dim,\n",
    "            decoder_embed_dim=decoder_dim,\n",
    "            decoder_depth=1,\n",
    "            decoder_num_heads=16,\n",
    "            mlp_ratio=4.0,\n",
    "            proj_drop_rate=0.0,\n",
    "            attn_drop_rate=0.0,\n",
    "        )\n",
    "\n",
    "    def forward_encoder(self, images, idx_keep=None):\n",
    "        return self.backbone.encode(images=images, idx_keep=idx_keep)\n",
    "\n",
    "    def forward_decoder(self, x_encoded, idx_keep, idx_mask):\n",
    "        # build decoder input\n",
    "        batch_size = x_encoded.shape[0]\n",
    "        x_decode = self.decoder.embed(x_encoded)\n",
    "        x_masked = utils.repeat_token(\n",
    "            self.decoder.mask_token, (batch_size, self.sequence_length)\n",
    "        )\n",
    "        x_masked = utils.set_at_index(x_masked, idx_keep, x_decode.type_as(x_masked))\n",
    "\n",
    "        # decoder forward pass\n",
    "        x_decoded = self.decoder.decode(x_masked)\n",
    "\n",
    "        # predict pixel values for masked tokens\n",
    "        x_pred = utils.get_at_index(x_decoded, idx_mask)\n",
    "        x_pred = self.decoder.predict(x_pred)\n",
    "        return x_pred\n",
    "\n",
    "    def forward(self, images):\n",
    "        batch_size = images.shape[0]\n",
    "        idx_keep, idx_mask = utils.random_token_mask(\n",
    "            size=(batch_size, self.sequence_length),\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            device=images.device,\n",
    "        )\n",
    "        x_encoded = self.forward_encoder(images=images, idx_keep=idx_keep)\n",
    "        x_pred = self.forward_decoder(\n",
    "            x_encoded=x_encoded, idx_keep=idx_keep, idx_mask=idx_mask\n",
    "        )\n",
    "\n",
    "        # get image patches for masked tokens\n",
    "        patches = utils.patchify(images, self.patch_size)\n",
    "        # must adjust idx_mask for missing class token\n",
    "        target = utils.get_at_index(patches, idx_mask - 1)\n",
    "        return x_pred, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "5"
   },
   "outputs": [],
   "source": [
    "vit = vit_base_patch32_224()\n",
    "model = MAE(vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6",
    "outputId": "24a16192-12db-4103-d463-d610715b0f4b"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eEn8ZnZmuCGm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEn8ZnZmuCGm",
    "outputId": "2c9c8df2-524d-4c76-9be2-fbc18c5e971e"
   },
   "outputs": [],
   "source": [
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "7",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "transform = MAETransform()\n",
    "# we ignore object detection annotations by setting target_transform to return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "8"
   },
   "outputs": [],
   "source": [
    "def target_transform(t):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "9",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dataset = torchvision.datasets.VOCDetection(\n",
    "#     \"datasets/pascal_voc\",\n",
    "#     download=True,\n",
    "#     transform=transform,\n",
    "#     target_transform=target_transform,\n",
    "# )\n",
    "# or create a dataset from a folder containing images or videos:\n",
    "# dataset = LightlyDataset(\"path/to/folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "10"
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8,\n",
    "    # transform=transform,\n",
    "    # target_transform=target_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "11"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1.5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "12",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Starting Training\")\n",
    "# for epoch in range(10):\n",
    "#     total_loss = 0\n",
    "#     for batch in dataloader:\n",
    "#         views = batch[0]\n",
    "#         images = views[0].to(device)  # views contains only a single view\n",
    "#         predictions, targets = model(images)\n",
    "#         loss = criterion(predictions, targets)\n",
    "#         total_loss += loss.detach()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k0HA-gnWtz1O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k0HA-gnWtz1O",
    "outputId": "a9079324-255a-4e2a-ed75-f274379667e8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "\n",
    "# # ---------- Drive setup ----------\n",
    "# try:\n",
    "#     from google.colab import drive\n",
    "#     drive.mount('/content/drive')\n",
    "#     DRIVE_ROOT = Path(\"/content/drive/MyDrive\")\n",
    "#     IS_COLAB = True\n",
    "#     print(\"✓ Running on Colab, Drive mounted.\")\n",
    "# except Exception:\n",
    "#     DRIVE_ROOT = Path(\"./saved_models\")\n",
    "#     IS_COLAB = False\n",
    "#     print(\"⚠️ Not on Colab, using local folder ./saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bPKUNjfDtyLJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bPKUNjfDtyLJ",
    "outputId": "93ba726c-6fa7-4e94-9636-93a9b169408f"
   },
   "outputs": [],
   "source": [
    "# ---------- Project / save dir ----------\n",
    "PROJECT_NAME = \"mae\"  # wandb project AND folder name\n",
    "DRIVE_ROOT = \"outputs\"\n",
    "save_dir = Path(DRIVE_ROOT) / Path(PROJECT_NAME)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Save directory ready: {save_dir}\")\n",
    "\n",
    "# ---------- wandb init ----------\n",
    "\n",
    "wandb.init(\n",
    "    entity=\"lquan9\",\n",
    "    project=PROJECT_NAME,\n",
    "    name=\"mae-run-1\",      # change run name if you like\n",
    ")\n",
    "\n",
    "# ---------- Training loop ----------\n",
    "num_epochs = 100\n",
    "print(\"Starting Training\")\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "global_step = 0\n",
    "step_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        views = batch[0]\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # images = views[0].to(device)\n",
    "        images = views.to(device)\n",
    "\n",
    "        # Forward\n",
    "        predictions, targets = model(images)\n",
    "        loss = criterion(predictions, targets)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # ---- wandb STEP LOGGING ----\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"loss/step\": loss.item(),\n",
    "                \"time/step_sec\": time.time() - step_start,\n",
    "                \"step\": global_step,\n",
    "                \"epoch\": epoch,\n",
    "            },\n",
    "            step=global_step,\n",
    "        )\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")\n",
    "\n",
    "    # ---- wandb logging ----\n",
    "    wandb.log({\n",
    "        \"loss/train\": avg_loss,\n",
    "        \"epoch\": epoch,\n",
    "    })\n",
    "\n",
    "    # ---- Save checkpoint to Drive/mae/ (always same filename) ----\n",
    "    ckpt_path = save_dir / f\"{PROJECT_NAME}_latest.pt\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"avg_loss\": avg_loss,\n",
    "        },\n",
    "        ckpt_path,\n",
    "    )\n",
    "    print(f\"✓ Saved checkpoint: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q4HGSbeYujt0",
   "metadata": {
    "id": "q4HGSbeYujt0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d9aeeb9448f4dbca32099a74dc50d87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4aa1dba7ce54730a302875831cfe11f",
      "placeholder": "​",
      "style": "IPY_MODEL_f733db29b4214296a9311a76ffb24ea8",
      "value": " 6/6 [00:00&lt;00:00, 145.19it/s]"
     }
    },
    "29808084c82149379bd4e300c7e5b266": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a5bee40ea824c3ab07c5db6c38b1ac5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_903e45db6299410c8f2c23786fc30423",
      "placeholder": "​",
      "style": "IPY_MODEL_c01b079103944ccd8e40309494385418",
      "value": "Fetching 6 files: 100%"
     }
    },
    "68c2ae9e7b694d06b54f4d024728eaa2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4a5bee40ea824c3ab07c5db6c38b1ac5",
       "IPY_MODEL_e65b5ab1013a40ec9a51a9f634756a6f",
       "IPY_MODEL_0d9aeeb9448f4dbca32099a74dc50d87"
      ],
      "layout": "IPY_MODEL_be98e9fee6c34964a9ade441c8cff257"
     }
    },
    "903e45db6299410c8f2c23786fc30423": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be98e9fee6c34964a9ade441c8cff257": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c01b079103944ccd8e40309494385418": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca69d9cae0064a15b2b887740ecf591a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e4aa1dba7ce54730a302875831cfe11f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e65b5ab1013a40ec9a51a9f634756a6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29808084c82149379bd4e300c7e5b266",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca69d9cae0064a15b2b887740ecf591a",
      "value": 6
     }
    },
    "f733db29b4214296a9311a76ffb24ea8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
