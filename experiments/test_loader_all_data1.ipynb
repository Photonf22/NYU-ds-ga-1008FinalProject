{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7179ec-5097-454b-9ca7-b002e01796fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ca8149-ab30-4911-bdc3-a6c6c1653f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import argparse\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1e42b0-12e4-4414-a259-98ebf2715098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageDataset(Dataset):\n",
    "#     \"\"\"Simple dataset for loading images\"\"\"\n",
    "    \n",
    "#     def __init__(self, image_dir, image_list, labels=None, resolution=224):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             image_dir: Directory containing images\n",
    "#             image_list: List of image filenames\n",
    "#             labels: List of labels (optional, for train/val)\n",
    "#             resolution: Image resolution (96 for competition, 224 for DINO baseline)\n",
    "#         \"\"\"\n",
    "#         self.image_dir = Path(image_dir)\n",
    "#         self.image_list = image_list\n",
    "#         self.labels = labels\n",
    "#         self.resolution = resolution\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.image_list)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         img_name = self.image_list[idx]\n",
    "#         img_path = self.image_dir / img_name\n",
    "        \n",
    "#         # Load and resize image\n",
    "#         image = Image.open(img_path).convert('RGB')\n",
    "#         image = image.resize((self.resolution, self.resolution), Image.BILINEAR)\n",
    "        \n",
    "#         if self.labels is not None:\n",
    "#             return image, self.labels[idx], img_name\n",
    "#         return image, img_name\n",
    "\n",
    "# class ImageDataset(Dataset):\n",
    "#     \"\"\"Simple dataset for loading images with MAE/DINO-style transforms.\"\"\"\n",
    "\n",
    "#     def __init__(self, image_dir, image_list, labels=None, resolution=224, split=\"train\"):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             image_dir: Directory containing images\n",
    "#             image_list: List of filenames\n",
    "#             labels: Optional list of labels\n",
    "#             resolution: Base image size (224 for DINO, 96 for competition)\n",
    "#             split: \"train\" or \"val\" or \"test\"\n",
    "#         \"\"\"\n",
    "#         self.image_dir = Path(image_dir)\n",
    "#         self.image_list = image_list\n",
    "#         self.labels = labels\n",
    "#         self.split = split\n",
    "#         self.resolution = resolution\n",
    "\n",
    "#         # Same ImageNet normalization used in CUBLinearProbeDataset\n",
    "#         imagenet_mean = [0.485, 0.456, 0.406]\n",
    "#         imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "#         if split == \"train\":\n",
    "#             self.transform = v2.Compose([\n",
    "#                 v2.RandomResizedCrop(resolution, scale=(0.8, 1.0)),\n",
    "#                 v2.RandomHorizontalFlip(p=0.5),\n",
    "#                 v2.ColorJitter(\n",
    "#                     brightness=0.4,\n",
    "#                     contrast=0.4,\n",
    "#                     saturation=0.4,\n",
    "#                     hue=0.1\n",
    "#                 ),\n",
    "#                 v2.ToImage(),\n",
    "#                 v2.ToDtype(torch.float32, scale=True),\n",
    "#                 v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "#             ])\n",
    "#         else:\n",
    "#             # val/test preprocessing\n",
    "#             self.transform = v2.Compose([\n",
    "#                 v2.Resize(256),\n",
    "#                 v2.CenterCrop(resolution),\n",
    "#                 v2.ToImage(),\n",
    "#                 v2.ToDtype(torch.float32, scale=True),\n",
    "#                 v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "#             ])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_list)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_name = self.image_list[idx]\n",
    "#         img_path = self.image_dir / img_name\n",
    "\n",
    "#         # Load RGB image\n",
    "#         img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "#         # Apply SAME transforms that CUBLinearProbeDataset uses\n",
    "#         img = self.transform(img)\n",
    "\n",
    "#         if self.labels is not None:\n",
    "#             return img, self.labels[idx], img_name\n",
    "#         return img, img_name\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_list, labels=None,\n",
    "                 resolution=224, split=\"train\", apply_transforms=True):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.image_list = image_list\n",
    "        self.labels = labels\n",
    "        self.split = split\n",
    "        self.resolution = resolution\n",
    "        self.apply_transforms = apply_transforms\n",
    "\n",
    "        imagenet_mean = [0.485, 0.456, 0.406]\n",
    "        imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        if apply_transforms:\n",
    "            if split == \"train\":\n",
    "                self.transform = v2.Compose([\n",
    "                    v2.RandomResizedCrop(resolution, scale=(0.8, 1.0)),\n",
    "                    v2.RandomHorizontalFlip(p=0.5),\n",
    "                    v2.ColorJitter(\n",
    "                        brightness=0.4,\n",
    "                        contrast=0.4,\n",
    "                        saturation=0.4,\n",
    "                        hue=0.1\n",
    "                    ),\n",
    "                    v2.ToImage(),\n",
    "                    v2.ToDtype(torch.float32, scale=True),\n",
    "                    v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = v2.Compose([\n",
    "                    v2.Resize(256),\n",
    "                    v2.CenterCrop(resolution),\n",
    "                    v2.ToImage(),\n",
    "                    v2.ToDtype(torch.float32, scale=True),\n",
    "                    v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = None   # <-- important\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_list[idx]\n",
    "        img_path = self.image_dir / img_name\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)   # -> Tensor (for supervised)\n",
    "        # else: keep img as PIL (for SSL)\n",
    "\n",
    "        if self.labels is not None:\n",
    "            return img, self.labels[idx], img_name\n",
    "        return img, img_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b652cf42-2018-4352-bad0-0820f878dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle PIL images\"\"\"\n",
    "    if len(batch[0]) == 3:  # train/val (image, label, filename)\n",
    "        images = [item[0] for item in batch]\n",
    "        labels = [item[1] for item in batch]\n",
    "        filenames = [item[2] for item in batch]\n",
    "        return images, labels, filenames\n",
    "    else:  # test (image, filename)\n",
    "        images = [item[0] for item in batch]\n",
    "        filenames = [item[1] for item in batch]\n",
    "        return images, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38930ee5-8311-4029-9e83-52512b5bc917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t       test_images.csv\t\t train_labels.csv\n",
      "sample_submission.csv  test_labels_INTERNAL.csv  val\n",
      "test\t\t       train\t\t\t val_labels.csv\n"
     ]
    }
   ],
   "source": [
    "! ls /home/long/code/amogh/data/testset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbebdfb-cd4a-46b4-ba48-5dfaa5f73b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset metadata...\n",
      "  Train: 8232 images\n",
      "  Val:   1727 images\n",
      "  Test:  1829 images\n",
      "  Classes: 200\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters (replace args.*)\n",
    "# ============================================================\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "resolution = 224   # or whatever you want for training\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = Path(\"/home/long/code/amogh/data/testset_1\")\n",
    "\n",
    "print(\"\\nLoading dataset metadata...\")\n",
    "train_df = pd.read_csv(data_dir / 'train_labels.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val_labels.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} images\")\n",
    "print(f\"  Val:   {len(val_df)} images\")\n",
    "print(f\"  Test:  {len(test_df)} images\")\n",
    "print(f\"  Classes: {train_df['class_id'].nunique()}\")\n",
    "\n",
    "train_dataset1 = ImageDataset(\n",
    "    data_dir / 'train',\n",
    "    train_df['filename'].tolist(),\n",
    "    train_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "val_dataset1 = ImageDataset(\n",
    "    data_dir / 'val',\n",
    "    val_df['filename'].tolist(),\n",
    "    val_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "test_dataset1 = ImageDataset(\n",
    "    data_dir / 'test',\n",
    "    test_df['filename'].tolist(),\n",
    "    labels=test_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "train_loader1 = DataLoader(\n",
    "    train_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader1 = DataLoader(\n",
    "    val_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader1 = DataLoader(\n",
    "    test_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88ee4a0-c0d6-43bc-9c3a-c02b9fcf8ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset metadata...\n",
      "  Train: 26880 images\n",
      "  Val:   5760 images\n",
      "  Test:  5760 images\n",
      "  Classes: 64\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters (replace args.*)\n",
    "# ============================================================\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "resolution = 224   # or whatever you want for training\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = Path(\"/home/long/code/amogh/data/testset_2\")\n",
    "\n",
    "print(\"\\nLoading dataset metadata...\")\n",
    "train_df = pd.read_csv(data_dir / 'train_labels.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val_labels.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} images\")\n",
    "print(f\"  Val:   {len(val_df)} images\")\n",
    "print(f\"  Test:  {len(test_df)} images\")\n",
    "print(f\"  Classes: {train_df['class_id'].nunique()}\")\n",
    "\n",
    "train_dataset2 = ImageDataset(\n",
    "    data_dir / 'train',\n",
    "    train_df['filename'].tolist(),\n",
    "    train_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "val_dataset2 = ImageDataset(\n",
    "    data_dir / 'val',\n",
    "    val_df['filename'].tolist(),\n",
    "    val_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "test_dataset2 = ImageDataset(\n",
    "    data_dir / 'test',\n",
    "    test_df['filename'].tolist(),\n",
    "    labels=test_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "train_loader2 = DataLoader(\n",
    "    train_dataset2,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader2 = DataLoader(\n",
    "    val_dataset2,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader2 = DataLoader(\n",
    "    test_dataset2,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa553af5-7b27-4817-b11b-7fa789f90444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset metadata...\n",
      "  Train: 13895 images\n",
      "  Val:   2977 images\n",
      "  Test:  2978 images\n",
      "  Classes: 397\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters (replace args.*)\n",
    "# ============================================================\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "resolution = 224   # or whatever you want for training\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = Path(\"/home/long/code/amogh/data/testset_3\")\n",
    "\n",
    "print(\"\\nLoading dataset metadata...\")\n",
    "train_df = pd.read_csv(data_dir / 'train_labels.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val_labels.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} images\")\n",
    "print(f\"  Val:   {len(val_df)} images\")\n",
    "print(f\"  Test:  {len(test_df)} images\")\n",
    "print(f\"  Classes: {train_df['class_id'].nunique()}\")\n",
    "\n",
    "train_dataset3 = ImageDataset(\n",
    "    data_dir / 'train',\n",
    "    train_df['filename'].tolist(),\n",
    "    train_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "val_dataset3 = ImageDataset(\n",
    "    data_dir / 'val',\n",
    "    val_df['filename'].tolist(),\n",
    "    val_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "test_dataset3 = ImageDataset(\n",
    "    data_dir / 'test',\n",
    "    test_df['filename'].tolist(),\n",
    "    labels=test_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "train_loader3 = DataLoader(\n",
    "    train_dataset3,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader3 = DataLoader(\n",
    "    val_dataset3,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader3 = DataLoader(\n",
    "    test_dataset3,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49fad00f-9e89-428b-9456-e434dbb9102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lightly|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79398049-f622-4014-9603-68116d9c7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9416738c-3205-421d-b0e5-7b9fc1f1b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from timm.models.vision_transformer import vit_base_patch32_224\n",
    "from torch import nn\n",
    "from lightly.models import utils\n",
    "from lightly.models.modules import MAEDecoderTIMM, MaskedVisionTransformerTIMM\n",
    "from lightly.transforms import MAETransform\n",
    "import copy\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.loss import DINOLoss  # only needed if you re-train SSL\n",
    "from lightly.models.utils import deactivate_requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c6497ed-8306-40ca-a5c0-ac9d72db4c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINO(nn.Module):\n",
    "    def __init__(self, backbone, input_dim):\n",
    "        super().__init__()\n",
    "        self.student_backbone = backbone\n",
    "        self.student_head = DINOProjectionHead(\n",
    "            input_dim, 512, 64, 2048, freeze_last_layer=1\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(backbone)\n",
    "        self.teacher_head = DINOProjectionHead(input_dim, 512, 64, 2048)\n",
    "        deactivate_requires_grad(self.teacher_backbone)\n",
    "        deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.student_backbone(x).flatten(start_dim=1)\n",
    "        z = self.student_head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5ac9f55-fa6f-4308-ab9b-65130561f1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_linear_head.pt   submission-Copy1.csv\n",
      "configs\t\t      submission.csv\n",
      "data\t\t      test_loader_all_data1.ipynb\n",
      "dino.ipynb\t      test_loader_all_data2.ipynb\n",
      "dino-res34.ipynb      test_loader_all_data3.ipynb\n",
      "final.pt\t      test_loader_all.ipynb\n",
      "hpc\t\t      test_loader_all_resnet_big.ipynb\n",
      "lejepa_example.ipynb  testset1.ipynb\n",
      "mae.ipynb\t      wandb\n",
      "outputs\t\t      wejepa.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6be9f7-d5dd-4ed8-ac49-b2fc69c5ee29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee649a90-3e50-444f-af4a-e302bb13552b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "# --- Build same backbone as used for DINO pretraining ---\n",
    "resnet = torchvision.models.resnet18()\n",
    "# resnet = torchvision.models.resnet34()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])  # (B, 512, 1, 1)\n",
    "input_dim = 512\n",
    "\n",
    "dino_model = DINO(backbone, input_dim)\n",
    "\n",
    "# --- Load your pre-trained DINO checkpoint ---\n",
    "ckpt = torch.load(\n",
    "    \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed.pt\",\n",
    "    map_location=\"cpu\",\n",
    ")\n",
    "\n",
    "dino_model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8be1dbbc-8e76-46fc-a60a-44c200a1e9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_linear_head.pt   submission-Copy1.csv\n",
      "configs\t\t      submission.csv\n",
      "data\t\t      test_loader_all_data1.ipynb\n",
      "dino.ipynb\t      test_loader_all_data2.ipynb\n",
      "dino-res34.ipynb      test_loader_all_data3.ipynb\n",
      "final.pt\t      test_loader_all.ipynb\n",
      "hpc\t\t      test_loader_all_resnet_big.ipynb\n",
      "lejepa_example.ipynb  testset1.ipynb\n",
      "mae.ipynb\t      wandb\n",
      "outputs\t\t      wejepa.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78b08208-43a3-4927-a09f-a78eb63602f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/long/code/dl_project1/experiments\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42411f41-8095-401a-aea7-02c9b1a729ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract raw subsets from train_ds and val_ds\n",
    "# raw_train_subset = train_raw_ds\n",
    "# raw_val_subset   = val_raw_ds\n",
    "\n",
    "# 2. SSL transform\n",
    "from lightly.transforms import MAETransform\n",
    "# ssl_transform = MAETransform()\n",
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "\n",
    "# ssl_transform = DINOTransform()\n",
    "# ssl_transform = get_cub_transform(split=\"train\", img_size=224)\n",
    "ssl_transform = DINOTransform()\n",
    "\n",
    "# 3. SSL dataset wrapper\n",
    "class CUB_SSL_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     data = self.subset[idx]  # (img, label, path)\n",
    "    #     img = data[0]\n",
    "    #     return self.transform(img)\n",
    "    def __getitem__(self, idx):\n",
    "        img, _, _ = self.subset[idx]\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        return self.transform(img)  # returns list[Tensor] from DINOTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73dbb580-f0e5-4b82-8c85-6c8d915de2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL training images: 9959\n",
      "Dataset found. Total samples: 9959\n",
      "Batch size: 64\n",
      "Number of workers: 4\n"
     ]
    }
   ],
   "source": [
    "# 4. Combine train + val subsets (no labels)\n",
    "from torch.utils.data import ConcatDataset\n",
    "ssl_trainval_raw = ConcatDataset([\n",
    "    train_dataset1, val_dataset1,\n",
    "    # train_dataset2, val_dataset2,\n",
    "    # train_dataset3, val_dataset3,\n",
    "])\n",
    "\n",
    "\n",
    "# 5. Build SSL dataset\n",
    "ssl_trainval_ds = CUB_SSL_Dataset(\n",
    "    subset=ssl_trainval_raw,\n",
    "    transform=ssl_transform,\n",
    ")\n",
    "\n",
    "# 6. Build SSL dataloader\n",
    "ssl_loader = DataLoader(\n",
    "    ssl_trainval_ds,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    # collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(\"SSL training images:\", len(ssl_trainval_ds))\n",
    "\n",
    "if ssl_loader.dataset is not None:\n",
    "    print(f\"Dataset found. Total samples: {len(ssl_loader.dataset)}\")\n",
    "    print(f\"Batch size: {ssl_loader.batch_size}\")\n",
    "    print(f\"Number of workers: {ssl_loader.num_workers}\")\n",
    "else:\n",
    "    print(\"Dataset is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5171e5d6-9b06-4e04-824b-f19875c6efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"===== SSL Train+Val Raw Dataset =====\")\n",
    "# print(\"Type:\", type(ssl_trainval_raw))\n",
    "# print(\"Total length:\", len(ssl_trainval_raw))\n",
    "\n",
    "# print(\"\\n-- Sub-datasets inside ConcatDataset --\")\n",
    "# for i, ds in enumerate(ssl_trainval_raw.datasets):\n",
    "#     print(f\"[{i}] {type(ds)}, length = {len(ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59bbb75d-bd9d-4056-ae93-0d0dec36a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f17650bb-5511-433d-a3e1-646f3a8879b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.loss import DINOLoss\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "from lightly.utils.scheduler import cosine_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c289ae65-63fb-4e7b-99a1-c7a7495106e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SSL Batch Debug ===\n",
      "Batch is a <class 'list'> with length 8\n",
      "\n",
      "--- View 0 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 224, 224])\n",
      "Dtype: torch.float32\n",
      "\n",
      "--- View 1 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 224, 224])\n",
      "Dtype: torch.float32\n",
      "\n",
      "--- View 2 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 96, 96])\n",
      "Dtype: torch.float32\n",
      "\n",
      "--- View 3 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 96, 96])\n",
      "Dtype: torch.float32\n",
      "\n",
      "--- View 4 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 96, 96])\n",
      "Dtype: torch.float32\n",
      "\n",
      "--- View 5 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 96, 96])\n",
      "Dtype: torch.float32\n",
      "\n",
      "--- View 6 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 96, 96])\n",
      "Dtype: torch.float32\n",
      "\n",
      "--- View 7 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 96, 96])\n",
      "Dtype: torch.float32\n",
      "\n",
      "=== End Debug ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- Inspect one batch from SSL dataloader ----\n",
    "\n",
    "batch = next(iter(ssl_loader))\n",
    "\n",
    "print(\"\\n=== SSL Batch Debug ===\")\n",
    "\n",
    "if isinstance(batch, list) or isinstance(batch, tuple):\n",
    "    print(f\"Batch is a {type(batch)} with length {len(batch)}\")\n",
    "\n",
    "# DINOTransform returns a list of views per item, but DataLoader collates it into:\n",
    "# batch = list_of_views, where each element has shape (B, C, H, W)\n",
    "\n",
    "# Example: batch[0] = global crops   shape: (B, 3, 224, 224)\n",
    "#          batch[1] = global crops   shape: (B, 3, 224, 224)\n",
    "#          batch[2] = local crops    shape: (B, 3, 96, 96)\n",
    "# etc.\n",
    "\n",
    "for i, view in enumerate(batch):\n",
    "    print(f\"\\n--- View {i} ---\")\n",
    "    print(f\"Type: {type(view)}\")\n",
    "    try:\n",
    "        print(f\"Shape: {view.shape}\")\n",
    "    except Exception:\n",
    "        print(\"View has no `.shape` attribute\")\n",
    "    print(f\"Dtype: {getattr(view, 'dtype', None)}\")\n",
    "\n",
    "print(\"\\n=== End Debug ===\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ee457d6-f863-4636-bf19-933cbda66fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:3'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab1a8389-884b-4c35-b74b-89a52a8cb037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of batch: <class 'list'>\n",
      "Batch length: 8\n",
      "  item[0] type: <class 'torch.Tensor'>\n",
      "  item[0] shape: torch.Size([64, 3, 224, 224])\n",
      "  item[1] type: <class 'torch.Tensor'>\n",
      "  item[1] shape: torch.Size([64, 3, 224, 224])\n",
      "  item[2] type: <class 'torch.Tensor'>\n",
      "  item[2] shape: torch.Size([64, 3, 96, 96])\n",
      "  item[3] type: <class 'torch.Tensor'>\n",
      "  item[3] shape: torch.Size([64, 3, 96, 96])\n",
      "  item[4] type: <class 'torch.Tensor'>\n",
      "  item[4] shape: torch.Size([64, 3, 96, 96])\n",
      "  item[5] type: <class 'torch.Tensor'>\n",
      "  item[5] shape: torch.Size([64, 3, 96, 96])\n",
      "  item[6] type: <class 'torch.Tensor'>\n",
      "  item[6] shape: torch.Size([64, 3, 96, 96])\n",
      "  item[7] type: <class 'torch.Tensor'>\n",
      "  item[7] shape: torch.Size([64, 3, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(ssl_loader))  # or ssl_loader\n",
    "\n",
    "print(\"Type of batch:\", type(batch))\n",
    "\n",
    "if isinstance(batch, (list, tuple)):\n",
    "    print(\"Batch length:\", len(batch))\n",
    "    for i, x in enumerate(batch):\n",
    "        print(f\"  item[{i}] type: {type(x)}\")\n",
    "        if hasattr(x, \"shape\"):\n",
    "            print(f\"  item[{i}] shape:\", x.shape)\n",
    "else:\n",
    "    print(\"Batch shape:\", batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40d3614e-61f5-4bf6-a055-2838ce5c07b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save directory ready: /home/long/code/amogh/data/models\n",
      "Starting SSL Training (DINO)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | train loss: 2.15083 | time: 65.3s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████████████████████████████████████████████████████████████████████████████| 156/156 [01:08<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss: 2.05434 | time: 68.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train loss: 1.99911 | time: 65.9s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████████████████████████████████████████████████████████████████████████████| 156/156 [01:08<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | train loss: 1.95605 | time: 68.1s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | train loss: 1.91377 | time: 66.2s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | train loss: 1.89051 | time: 66.1s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | train loss: 1.86258 | time: 66.3s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | train loss: 1.82887 | time: 66.2s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | train loss: 1.80813 | time: 65.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | train loss: 1.77933 | time: 65.6s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train loss: 1.76557 | time: 65.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | train loss: 1.74170 | time: 67.0s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | train loss: 1.71617 | time: 66.2s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:07<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | train loss: 1.69515 | time: 67.6s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | train loss: 1.67408 | time: 66.1s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | train loss: 1.65993 | time: 65.6s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | train loss: 1.63422 | time: 65.7s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | train loss: 1.62251 | time: 65.8s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | train loss: 1.60817 | time: 65.9s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | train loss: 1.58981 | time: 65.5s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | train loss: 1.55695 | time: 65.7s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | train loss: 1.54786 | time: 65.6s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | train loss: 1.52606 | time: 65.7s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | train loss: 1.50183 | time: 65.7s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | train loss: 1.49181 | time: 65.1s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | train loss: 1.47288 | time: 66.6s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 | train loss: 1.45822 | time: 66.7s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | train loss: 1.44252 | time: 66.1s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 | train loss: 1.42510 | time: 65.6s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 | train loss: 1.41509 | time: 65.5s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:07<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 | train loss: 1.40313 | time: 67.1s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 | train loss: 1.37497 | time: 66.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 | train loss: 1.35769 | time: 65.7s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 | train loss: 1.35482 | time: 66.2s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 | train loss: 1.33314 | time: 65.7s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:07<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 | train loss: 1.31977 | time: 67.1s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 | train loss: 1.31238 | time: 66.5s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 | train loss: 1.28266 | time: 66.9s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 | train loss: 1.27591 | time: 65.6s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 | train loss: 1.25736 | time: 65.2s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 | train loss: 1.25373 | time: 64.9s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 | train loss: 1.24012 | time: 65.2s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 | train loss: 1.22321 | time: 65.3s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:09<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 | train loss: 1.20234 | time: 69.7s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 | train loss: 1.20129 | time: 66.7s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 | train loss: 1.19516 | time: 66.2s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 | train loss: 1.17578 | time: 65.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 | train loss: 1.15461 | time: 65.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:07<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 | train loss: 1.15115 | time: 67.2s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:12<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 | train loss: 1.13588 | time: 72.8s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 | train loss: 1.13657 | time: 66.6s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 | train loss: 1.11907 | time: 66.6s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 | train loss: 1.10284 | time: 66.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 | train loss: 1.08805 | time: 66.9s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 | train loss: 1.08250 | time: 66.1s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 | train loss: 1.08652 | time: 65.1s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:07<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 | train loss: 1.06490 | time: 67.7s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 | train loss: 1.05651 | time: 66.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 | train loss: 1.04206 | time: 65.1s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:03<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 | train loss: 1.03858 | time: 63.9s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 | train loss: 1.02115 | time: 64.3s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:03<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 | train loss: 1.02331 | time: 63.8s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:03<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 | train loss: 1.00898 | time: 63.7s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:03<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 | train loss: 1.00771 | time: 63.6s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:07<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 | train loss: 0.99864 | time: 67.5s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 | train loss: 0.98312 | time: 65.0s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 | train loss: 0.97855 | time: 64.7s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 | train loss: 0.96692 | time: 64.0s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:03<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 | train loss: 0.96997 | time: 63.6s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:03<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 | train loss: 0.95522 | time: 63.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:09<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 | train loss: 0.94228 | time: 69.5s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:07<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 | train loss: 0.95345 | time: 67.2s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 | train loss: 0.94161 | time: 64.5s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 | train loss: 0.93891 | time: 64.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 | train loss: 0.91639 | time: 64.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 | train loss: 0.90515 | time: 64.3s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:03<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 | train loss: 0.91785 | time: 64.0s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:03<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 | train loss: 0.88870 | time: 63.5s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:11<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 | train loss: 0.89260 | time: 71.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 | train loss: 0.89221 | time: 65.5s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 | train loss: 0.88243 | time: 64.5s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 | train loss: 0.87175 | time: 64.8s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 | train loss: 0.88525 | time: 64.2s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 | train loss: 0.86635 | time: 66.6s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:07<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 | train loss: 0.86668 | time: 67.5s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 | train loss: 0.85470 | time: 64.9s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 | train loss: 0.84159 | time: 64.2s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 | train loss: 0.85117 | time: 64.5s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:03<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 | train loss: 0.82589 | time: 63.6s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:03<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 | train loss: 0.82883 | time: 63.6s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 | train loss: 0.81852 | time: 64.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:06<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 | train loss: 0.82351 | time: 66.8s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:12<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 | train loss: 0.83094 | time: 72.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 | train loss: 0.82113 | time: 64.7s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 | train loss: 0.81875 | time: 64.7s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 | train loss: 0.80682 | time: 64.8s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 | train loss: 0.80468 | time: 64.9s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:04<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 | train loss: 0.81513 | time: 64.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 156/156 [01:03<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 | train loss: 0.79856 | time: 63.8s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/100: 100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [01:03<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 | train loss: 0.79374 | time: 63.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\n"
     ]
    }
   ],
   "source": [
    "# ---------- Project / save dir ----------\n",
    "PROJECT_NAME = \"dino-v1\"  # folder name\n",
    "\n",
    "save_dir = \"/home/long/code/amogh/data/models/\"\n",
    "save_dir = Path(save_dir)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Save directory ready: {save_dir}\")\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "dino_model.to(device)\n",
    "\n",
    "ckpt_path = save_dir / f\"{PROJECT_NAME}_full_finetuned_18_test2fixed_data1_data1againidk.pt\"\n",
    "start_epoch = 0\n",
    "\n",
    "# # --- Optimizer: only student parameters ---\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     list(dino_model.student_backbone.parameters()) +\n",
    "#     list(dino_model.student_head.parameters()),\n",
    "#     lr=1.5e-4,\n",
    "#     weight_decay=1e-4,\n",
    "# )\n",
    "\n",
    "optimizer = torch.optim.Adam(dino_model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = DINOLoss(\n",
    "    output_dim=2048,\n",
    "    warmup_teacher_temp_epochs=5,\n",
    ")\n",
    "# move loss to correct device because it also contains parameters\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# ---------- Training loop ----------\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 100\n",
    "print(\"Starting SSL Training (DINO)\")\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    dino_model.train()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # EMA momentum for teacher this epoch\n",
    "    momentum_val = cosine_schedule(epoch, num_epochs, 0.996, 1.0)\n",
    "\n",
    "    for batch in tqdm(ssl_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "\n",
    "        # ssl_loader should give a list of crops (views)\n",
    "        # Handle both \"views\" and \"(views, _)\" cases\n",
    "        if isinstance(batch, (list, tuple)) and isinstance(batch[0], torch.Tensor):\n",
    "            views = batch\n",
    "        elif isinstance(batch, (list, tuple)):\n",
    "            views = batch[0]\n",
    "        else:\n",
    "            views = batch\n",
    "\n",
    "        # move all crops to GPU\n",
    "        views = [v.to(device, non_blocking=True) for v in views]\n",
    "\n",
    "        # ---- EMA update for teacher ----\n",
    "        update_momentum(dino_model.student_backbone, dino_model.teacher_backbone, m=momentum_val)\n",
    "        update_momentum(dino_model.student_head,     dino_model.teacher_head,     m=momentum_val)\n",
    "\n",
    "        # first two are global crops for teacher\n",
    "        global_views = views[:2]\n",
    "\n",
    "        # teacher on global crops (no grad)\n",
    "        with torch.no_grad():\n",
    "            teacher_out = [dino_model.forward_teacher(v) for v in global_views]\n",
    "\n",
    "        # student on all crops (global + local)\n",
    "        student_out = [dino_model(v) for v in views]\n",
    "\n",
    "        # ---- DINO loss ----\n",
    "        loss = criterion(teacher_out, student_out, epoch=epoch)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        dino_model.student_head.cancel_last_layer_gradients(current_epoch=epoch)\n",
    "        optimizer.step()\n",
    "\n",
    "        bsz = views[0].size(0)\n",
    "        total_loss += loss.item() * bsz\n",
    "        n_samples += bsz\n",
    "        global_step += 1\n",
    "\n",
    "    avg_loss = total_loss / max(n_samples, 1)\n",
    "    elapsed = time.time() - epoch_start\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train loss: {avg_loss:.5f} | time: {elapsed:.1f}s\")\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        # ---- Save checkpoint (always same filename) ----\n",
    "        ckpt = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": dino_model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"avg_loss\": avg_loss,\n",
    "        }\n",
    "        torch.save(ckpt, ckpt_path)\n",
    "        print(f\"✓ Saved checkpoint: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72799a15-a19e-404e-8015-a5845c8f374e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wejepa",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
