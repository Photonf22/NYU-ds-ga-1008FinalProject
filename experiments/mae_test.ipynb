{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "source": [
    "This example requires the following dependencies to be installed:\n",
    "pip install lightly[timm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rS-dY-5jnG8T",
   "metadata": {
    "id": "rS-dY-5jnG8T"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1",
   "metadata": {
    "id": "1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: lightly[timm]\n"
     ]
    }
   ],
   "source": [
    "#!pip install lightly[timm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "g5IhAntGsbP2",
   "metadata": {
    "id": "g5IhAntGsbP2"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "class RawImageDataset(Dataset):\n",
    "    \"\"\"Dataset that loads images directly from raw files.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, image_extensions=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "        if image_extensions is None:\n",
    "            image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.JPEG', '*.JPG', '*.PNG']\n",
    "\n",
    "        # Find all image files\n",
    "        self.image_paths = []\n",
    "        print(f\"Searching for images in: {self.root_dir}\")\n",
    "\n",
    "        for pattern in image_extensions:\n",
    "            found = glob.glob(str(self.root_dir / '**' / pattern), recursive=True)\n",
    "            self.image_paths.extend(found)\n",
    "            if found:\n",
    "                print(f\"  Found {len(found)} {pattern} files\")\n",
    "\n",
    "        self.image_paths.sort()\n",
    "        print(f\"Total images found: {len(self.image_paths)}\")\n",
    "\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"\\nWarning: No images found. Directory structure (first 20 items):\")\n",
    "            for item in sorted(self.root_dir.rglob('*'))[:20]:\n",
    "                print(f\"  {item}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        img_path = self.image_paths[idx]\n",
    "\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            img = Image.new('RGB', (96, 96), color='black')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            # import pdb; pdb.set_trace()\n",
    "            # print(img.shape,\"old image shape\")\n",
    "            # img = img[0]\n",
    "            # print(img.shape,\"new image shape\")\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "def download_and_extract_dataset(repo_id, cache_dir=None, max_workers=4):\n",
    "    \"\"\"Download and extract dataset from HuggingFace.\"\"\"\n",
    "\n",
    "    print(f\"Downloading dataset from {repo_id}...\")\n",
    "\n",
    "    try:\n",
    "        local_dir = snapshot_download(\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\",\n",
    "            cache_dir=cache_dir,\n",
    "            max_workers=max_workers,\n",
    "            resume_download=True,\n",
    "        )\n",
    "        print(f\"Dataset downloaded to: {local_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "        print(\"Retrying with single worker...\")\n",
    "        local_dir = snapshot_download(\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\",\n",
    "            cache_dir=cache_dir,\n",
    "            max_workers=1,\n",
    "            resume_download=True,\n",
    "        )\n",
    "        print(f\"Dataset downloaded to: {local_dir}\")\n",
    "\n",
    "    # Extract zip files if present\n",
    "    local_path = Path(local_dir)\n",
    "    zip_files = list(local_path.glob('*.zip'))\n",
    "\n",
    "    if zip_files:\n",
    "        print(f\"\\nFound {len(zip_files)} zip files. Extracting...\")\n",
    "        extract_dir = local_path / 'extracted'\n",
    "        extract_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # for zip_file in zip_files:\n",
    "        #     print(f\"  Extracting {zip_file.name}...\")\n",
    "        #     try:\n",
    "        #         with zipfile.ZipFile(zip_file, 'r') as zf:\n",
    "        #             zf.extractall(extract_dir)\n",
    "        #         print(\"    ✓ Extracted successfully\")\n",
    "        #     except Exception as e:\n",
    "        #         print(f\"    ✗ Error: {e}\")\n",
    "\n",
    "        return extract_dir\n",
    "    else:\n",
    "        print(\"No zip files found, using directory as-is\")\n",
    "        return local_path\n",
    "\n",
    "\n",
    "def get_mae_transform():\n",
    "    \"\"\"Get MAE-compatible transform.\"\"\"\n",
    "    from lightly.transforms import MAETransform\n",
    "    return MAETransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "crOS747Gs5Ds",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "68c2ae9e7b694d06b54f4d024728eaa2",
      "4a5bee40ea824c3ab07c5db6c38b1ac5",
      "e65b5ab1013a40ec9a51a9f634756a6f",
      "0d9aeeb9448f4dbca32099a74dc50d87",
      "be98e9fee6c34964a9ade441c8cff257",
      "903e45db6299410c8f2c23786fc30423",
      "c01b079103944ccd8e40309494385418",
      "29808084c82149379bd4e300c7e5b266",
      "ca69d9cae0064a15b2b887740ecf591a",
      "e4aa1dba7ce54730a302875831cfe11f",
      "f733db29b4214296a9311a76ffb24ea8"
     ]
    },
    "id": "crOS747Gs5Ds",
    "outputId": "c3b038a2-1fee-4d14-ab97-43d1732c171f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for images in: data/devel\n",
      "  Found 790473 *.jpg files\n",
      "  Found 45000 *.png files\n",
      "Total images found: 835473\n",
      "\n",
      "Dataset ready with 835473 images\n"
     ]
    }
   ],
   "source": [
    "# Download and extract dataset\n",
    "# data_dir = download_and_extract_dataset(\n",
    "#     repo_id=\"tsbpp/fall2025_deeplearning\",\n",
    "#     cache_dir=None,\n",
    "#     max_workers=4\n",
    "# )\n",
    "data_dir = Path('./data/devel')\n",
    "\n",
    "# Create transform\n",
    "transform = get_mae_transform()\n",
    "\n",
    "# Create dataset\n",
    "dataset = RawImageDataset(data_dir, transform=transform)\n",
    "print(f\"\\nDataset ready with {len(dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2",
   "metadata": {
    "id": "2"
   },
   "outputs": [],
   "source": [
    "# Note: The model and training settings do not follow the reference settings\n",
    "# from the paper. The settings are chosen such that the example can easily be\n",
    "# run on a small dataset with a single GPU.\n",
    "import torch\n",
    "import torchvision\n",
    "from timm.models.vision_transformer import vit_base_patch32_224\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3",
   "metadata": {
    "id": "3"
   },
   "outputs": [],
   "source": [
    "from lightly.models import utils\n",
    "from lightly.models.modules import MAEDecoderTIMM, MaskedVisionTransformerTIMM\n",
    "from lightly.transforms import MAETransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4",
   "metadata": {
    "id": "4"
   },
   "outputs": [],
   "source": [
    "class MAE(nn.Module):\n",
    "    def __init__(self, vit):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_dim = 512\n",
    "        self.mask_ratio = 0.75\n",
    "        self.patch_size = vit.patch_embed.patch_size[0]\n",
    "\n",
    "        self.backbone = MaskedVisionTransformerTIMM(vit=vit)\n",
    "        self.sequence_length = self.backbone.sequence_length\n",
    "        self.decoder = MAEDecoderTIMM(\n",
    "            num_patches=vit.patch_embed.num_patches,\n",
    "            patch_size=self.patch_size,\n",
    "            embed_dim=vit.embed_dim,\n",
    "            decoder_embed_dim=decoder_dim,\n",
    "            decoder_depth=1,\n",
    "            decoder_num_heads=16,\n",
    "            mlp_ratio=4.0,\n",
    "            proj_drop_rate=0.0,\n",
    "            attn_drop_rate=0.0,\n",
    "        )\n",
    "\n",
    "    def forward_encoder(self, images, idx_keep=None):\n",
    "        return self.backbone.encode(images=images, idx_keep=idx_keep)\n",
    "\n",
    "    def forward_decoder(self, x_encoded, idx_keep, idx_mask):\n",
    "        # build decoder input\n",
    "        batch_size = x_encoded.shape[0]\n",
    "        x_decode = self.decoder.embed(x_encoded)\n",
    "        x_masked = utils.repeat_token(\n",
    "            self.decoder.mask_token, (batch_size, self.sequence_length)\n",
    "        )\n",
    "        x_masked = utils.set_at_index(x_masked, idx_keep, x_decode.type_as(x_masked))\n",
    "\n",
    "        # decoder forward pass\n",
    "        x_decoded = self.decoder.decode(x_masked)\n",
    "\n",
    "        # predict pixel values for masked tokens\n",
    "        x_pred = utils.get_at_index(x_decoded, idx_mask)\n",
    "        x_pred = self.decoder.predict(x_pred)\n",
    "        return x_pred\n",
    "\n",
    "    def forward(self, images):\n",
    "        batch_size = images.shape[0]\n",
    "        idx_keep, idx_mask = utils.random_token_mask(\n",
    "            size=(batch_size, self.sequence_length),\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            device=images.device,\n",
    "        )\n",
    "        x_encoded = self.forward_encoder(images=images, idx_keep=idx_keep)\n",
    "        x_pred = self.forward_decoder(\n",
    "            x_encoded=x_encoded, idx_keep=idx_keep, idx_mask=idx_mask\n",
    "        )\n",
    "\n",
    "        # get image patches for masked tokens\n",
    "        patches = utils.patchify(images, self.patch_size)\n",
    "        # must adjust idx_mask for missing class token\n",
    "        target = utils.get_at_index(patches, idx_mask - 1)\n",
    "        return x_pred, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5",
   "metadata": {
    "id": "5"
   },
   "outputs": [],
   "source": [
    "vit = vit_base_patch32_224()\n",
    "model = MAE(vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6",
    "outputId": "24a16192-12db-4103-d463-d610715b0f4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE(\n",
       "  (backbone): MaskedVisionTransformerTIMM(\n",
       "    (vit): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): MAEDecoderTIMM(\n",
       "    (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (decoder_blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "    (decoder_pred): Linear(in_features=512, out_features=3072, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eEn8ZnZmuCGm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEn8ZnZmuCGm",
    "outputId": "2c9c8df2-524d-4c76-9be2-fbc18c5e971e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 93,374,184\n",
      "Trainable parameters: 93,310,184\n"
     ]
    }
   ],
   "source": [
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7",
   "metadata": {
    "id": "7",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "transform = MAETransform()\n",
    "# we ignore object detection annotations by setting target_transform to return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8",
   "metadata": {
    "id": "8"
   },
   "outputs": [],
   "source": [
    "def target_transform(t):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9",
   "metadata": {
    "id": "9",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dataset = torchvision.datasets.VOCDetection(\n",
    "#     \"datasets/pascal_voc\",\n",
    "#     download=True,\n",
    "#     transform=transform,\n",
    "#     target_transform=target_transform,\n",
    "# )\n",
    "# or create a dataset from a folder containing images or videos:\n",
    "# dataset = LightlyDataset(\"path/to/folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10",
   "metadata": {
    "id": "10"
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8,\n",
    "    # transform=transform,\n",
    "    # target_transform=target_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11",
   "metadata": {
    "id": "11"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1.5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "12",
   "metadata": {
    "id": "12",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Starting Training\")\n",
    "# for epoch in range(10):\n",
    "#     total_loss = 0\n",
    "#     for batch in dataloader:\n",
    "#         views = batch[0]\n",
    "#         images = views[0].to(device)  # views contains only a single view\n",
    "#         predictions, targets = model(images)\n",
    "#         loss = criterion(predictions, targets)\n",
    "#         total_loss += loss.detach()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "k0HA-gnWtz1O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k0HA-gnWtz1O",
    "outputId": "a9079324-255a-4e2a-ed75-f274379667e8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "\n",
    "# # ---------- Drive setup ----------\n",
    "# try:\n",
    "#     from google.colab import drive\n",
    "#     drive.mount('/content/drive')\n",
    "#     DRIVE_ROOT = Path(\"/content/drive/MyDrive\")\n",
    "#     IS_COLAB = True\n",
    "#     print(\"✓ Running on Colab, Drive mounted.\")\n",
    "# except Exception:\n",
    "#     DRIVE_ROOT = Path(\"./saved_models\")\n",
    "#     IS_COLAB = False\n",
    "#     print(\"⚠️ Not on Colab, using local folder ./saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bPKUNjfDtyLJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bPKUNjfDtyLJ",
    "outputId": "93ba726c-6fa7-4e94-9636-93a9b169408f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save directory ready: outputs/mae\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mae-run-1</strong> at: <a href='https://wandb.ai/lquan9/mae/runs/bjgxb9co' target=\"_blank\">https://wandb.ai/lquan9/mae/runs/bjgxb9co</a><br> View project at: <a href='https://wandb.ai/lquan9/mae' target=\"_blank\">https://wandb.ai/lquan9/mae</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251129_203754-bjgxb9co/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/long/code/dl_project1/experiments/wandb/run-20251129_203809-2xfjkkbn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lquan9/mae/runs/2xfjkkbn' target=\"_blank\">mae-run-1</a></strong> to <a href='https://wandb.ai/lquan9/mae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lquan9/mae' target=\"_blank\">https://wandb.ai/lquan9/mae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lquan9/mae/runs/2xfjkkbn' target=\"_blank\">https://wandb.ai/lquan9/mae/runs/2xfjkkbn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|                                                       | 0/3263 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2356717.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2356717.jpg'\n",
      "Error loading data/devel/visual_genome/vg2_2350190.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2350190.jpg'\n",
      "Error loading data/devel/visual_genome/vg2_2370307.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2370307.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   1%|▍                                             | 27/3263 [00:08<14:04,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2388284.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2388284.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   1%|▌                                             | 37/3263 [00:11<14:01,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2400944.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2400944.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   2%|▉                                             | 65/3263 [00:18<13:51,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2346745.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2346745.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   2%|▉                                             | 66/3263 [00:18<13:50,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2323362.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2323362.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   2%|█                                             | 74/3263 [00:20<13:44,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2405004.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2405004.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   4%|█▉                                           | 145/3263 [00:39<13:37,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2361256.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2361256.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   5%|██▏                                          | 155/3263 [00:41<13:37,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2395307.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2395307.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   5%|██▍                                          | 176/3263 [00:47<13:34,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2390058.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2390058.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   6%|██▌                                          | 183/3263 [00:49<13:34,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2369711.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2369711.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   6%|██▌                                          | 186/3263 [00:50<13:31,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2407333.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2407333.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   7%|███▎                                         | 239/3263 [01:04<13:20,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2346773.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2346773.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   8%|███▍                                         | 253/3263 [01:07<13:21,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2339625.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2339625.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   8%|███▌                                         | 254/3263 [01:08<13:22,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2412350.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2412350.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   8%|███▌                                         | 261/3263 [01:10<13:11,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2415989.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2415989.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   8%|███▋                                         | 271/3263 [01:12<13:21,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2383685.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2383685.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  11%|████▋                                        | 343/3263 [01:31<13:06,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2351122.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2351122.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  11%|████▊                                        | 352/3263 [01:34<13:09,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2344844.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2344844.jpg'\n",
      "Error loading data/devel/visual_genome/vg2_2402944.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2402944.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  11%|████▉                                        | 354/3263 [01:34<13:04,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2337155.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2337155.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  11%|█████                                        | 363/3263 [01:37<13:01,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2335839.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2335839.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  11%|█████                                        | 364/3263 [01:37<13:02,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2338403.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2338403.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  12%|█████▏                                       | 378/3263 [01:41<12:59,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2370333.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2370333.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  13%|█████▋                                       | 412/3263 [01:50<12:49,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2320630.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2320630.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  13%|█████▊                                       | 421/3263 [01:53<12:47,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2393276.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2393276.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  13%|█████▉                                       | 432/3263 [01:55<12:52,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2379468.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2379468.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  14%|██████▎                                      | 462/3263 [02:04<12:37,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2344765.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2344765.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  15%|██████▋                                      | 483/3263 [02:09<12:42,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2319311.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2319311.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  15%|██████▋                                      | 484/3263 [02:10<12:39,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2316472.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2316472.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  16%|███████▍                                     | 536/3263 [02:24<12:18,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2370544.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2370544.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  17%|███████▌                                     | 550/3263 [02:28<12:22,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2369516.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2369516.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  17%|███████▊                                     | 568/3263 [02:32<12:13,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2354293.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2354293.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  18%|████████                                     | 585/3263 [02:37<12:03,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2336771.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2336771.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  18%|████████▏                                    | 591/3263 [02:39<12:11,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2415090.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2415090.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  18%|████████▎                                    | 600/3263 [02:41<12:08,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2394940.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2394940.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  19%|████████▍                                    | 616/3263 [02:45<12:01,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2395669.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2395669.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  19%|████████▌                                    | 624/3263 [02:48<11:54,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2408234.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2408234.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  19%|████████▋                                    | 627/3263 [02:48<11:55,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2396395.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2396395.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  20%|█████████                                    | 656/3263 [02:56<11:54,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2359672.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2359672.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  21%|█████████▎                                   | 673/3263 [03:01<11:49,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2321051.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2321051.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  22%|█████████▊                                   | 709/3263 [03:11<11:32,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2323189.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2323189.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  23%|██████████▍                                  | 757/3263 [03:24<11:21,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2402595.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2402595.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  24%|██████████▋                                  | 778/3263 [03:30<11:12,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2414491.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2414491.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  24%|██████████▉                                  | 796/3263 [03:34<11:12,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2329481.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2329481.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  27%|███████████▉                                 | 868/3263 [03:54<10:45,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2384303.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2384303.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  28%|████████████▌                                | 910/3263 [04:05<10:40,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2407337.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2407337.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  29%|████████████▉                                | 936/3263 [04:13<10:34,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2340698.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2340698.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  29%|█████████████                                | 944/3263 [04:15<10:28,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2357595.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2357595.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  29%|█████████████                                | 950/3263 [04:16<10:29,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2350687.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2350687.jpg'\n",
      "Error loading data/devel/visual_genome/vg2_2400248.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2400248.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  29%|█████████████                                | 951/3263 [04:17<10:28,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2379927.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2379927.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  29%|█████████████▏                               | 953/3263 [04:17<10:25,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2345238.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2345238.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  29%|█████████████▏                               | 957/3263 [04:18<10:24,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2324980.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2324980.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  30%|█████████████▎                               | 963/3263 [04:20<10:21,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2414917.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2414917.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  30%|█████████████▋                               | 995/3263 [04:29<10:16,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2340300.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2340300.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  31%|█████████████▊                              | 1026/3263 [04:37<10:07,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2356194.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2356194.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  33%|██████████████▌                             | 1081/3263 [04:52<09:51,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2383215.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2383215.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  34%|██████████████▉                             | 1104/3263 [04:58<09:52,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2385366.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2385366.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  34%|███████████████                             | 1116/3263 [05:02<09:43,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2407937.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2407937.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  35%|███████████████▎                            | 1138/3263 [05:08<09:40,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2316003.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2316003.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  36%|███████████████▋                            | 1168/3263 [05:16<09:27,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2408825.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2408825.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  36%|███████████████▊                            | 1170/3263 [05:16<09:26,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2355831.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2355831.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  38%|████████████████▋                           | 1234/3263 [05:34<09:12,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2391568.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2391568.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  38%|████████████████▋                           | 1240/3263 [05:35<09:10,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data/devel/visual_genome/vg2_2336131.jpg: cannot identify image file 'data/devel/visual_genome/vg2_2336131.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  39%|█████████████████▏                          | 1271/3263 [05:44<09:06,  3.65it/s]"
     ]
    }
   ],
   "source": [
    "# ---------- Project / save dir ----------\n",
    "PROJECT_NAME = \"mae\"  # wandb project AND folder name\n",
    "DRIVE_ROOT = \"outputs\"\n",
    "save_dir = Path(DRIVE_ROOT) / Path(PROJECT_NAME)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Save directory ready: {save_dir}\")\n",
    "\n",
    "# ---------- wandb init ----------\n",
    "\n",
    "wandb.init(\n",
    "    entity=\"lquan9\",\n",
    "    project=PROJECT_NAME,\n",
    "    name=\"mae-run-1\",      # change run name if you like\n",
    ")\n",
    "\n",
    "# ---------- Training loop ----------\n",
    "num_epochs = 100\n",
    "print(\"Starting Training\")\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "global_step = 0\n",
    "step_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        views = batch[0]\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # images = views[0].to(device)\n",
    "        images = views.to(device)\n",
    "\n",
    "        # Forward\n",
    "        predictions, targets = model(images)\n",
    "        loss = criterion(predictions, targets)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # ---- wandb STEP LOGGING ----\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"loss/step\": loss.item(),\n",
    "                \"time/step_sec\": time.time() - step_start,\n",
    "                \"step\": global_step,\n",
    "                \"epoch\": epoch,\n",
    "            },\n",
    "            step=global_step,\n",
    "        )\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")\n",
    "\n",
    "    # ---- wandb logging ----\n",
    "    wandb.log({\n",
    "        \"loss/train\": avg_loss,\n",
    "        \"epoch\": epoch,\n",
    "    })\n",
    "\n",
    "    # ---- Save checkpoint to Drive/mae/ (always same filename) ----\n",
    "    ckpt_path = save_dir / f\"{PROJECT_NAME}_latest.pt\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"avg_loss\": avg_loss,\n",
    "        },\n",
    "        ckpt_path,\n",
    "    )\n",
    "    print(f\"✓ Saved checkpoint: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q4HGSbeYujt0",
   "metadata": {
    "id": "q4HGSbeYujt0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d9aeeb9448f4dbca32099a74dc50d87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4aa1dba7ce54730a302875831cfe11f",
      "placeholder": "​",
      "style": "IPY_MODEL_f733db29b4214296a9311a76ffb24ea8",
      "value": " 6/6 [00:00&lt;00:00, 145.19it/s]"
     }
    },
    "29808084c82149379bd4e300c7e5b266": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a5bee40ea824c3ab07c5db6c38b1ac5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_903e45db6299410c8f2c23786fc30423",
      "placeholder": "​",
      "style": "IPY_MODEL_c01b079103944ccd8e40309494385418",
      "value": "Fetching 6 files: 100%"
     }
    },
    "68c2ae9e7b694d06b54f4d024728eaa2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4a5bee40ea824c3ab07c5db6c38b1ac5",
       "IPY_MODEL_e65b5ab1013a40ec9a51a9f634756a6f",
       "IPY_MODEL_0d9aeeb9448f4dbca32099a74dc50d87"
      ],
      "layout": "IPY_MODEL_be98e9fee6c34964a9ade441c8cff257"
     }
    },
    "903e45db6299410c8f2c23786fc30423": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be98e9fee6c34964a9ade441c8cff257": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c01b079103944ccd8e40309494385418": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca69d9cae0064a15b2b887740ecf591a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e4aa1dba7ce54730a302875831cfe11f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e65b5ab1013a40ec9a51a9f634756a6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29808084c82149379bd4e300c7e5b266",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca69d9cae0064a15b2b887740ecf591a",
      "value": 6
     }
    },
    "f733db29b4214296a9311a76ffb24ea8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
