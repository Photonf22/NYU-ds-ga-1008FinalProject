{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc761ad1",
   "metadata": {},
   "source": [
    "# Example based on our README.md\n",
    "1. Dataset download\n",
    "2. Pre-training\n",
    "3. Fine-tuning\n",
    "4. Different backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de0bef",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e8ab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 'train' available under /home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\n",
      "Split 'train' available under /home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\n",
      "Split 'test' available under /home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\n",
      "Downloading CUB-200-2011 from https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz...\n",
      "Download complete!\n",
      "Extracting CUB-200-2011 dataset...\n",
      "Extraction complete!\n",
      "Split 'train,test' available under /home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\n"
     ]
    }
   ],
   "source": [
    "# using the cli\n",
    "\n",
    "# download class dataset\n",
    "!python -m wejepa.datasets.download --dataset-root ./data --dataset-name tsbpp/fall2025_deeplearning --splits train\n",
    "\n",
    "# for development, download a small subset\n",
    "# !python -m wejepa.datasets.download --dataset-root ./data --dataset-name tsbpp/fall2025_deeplearning --splits 'train[:10]'\n",
    "\n",
    "# download cifar100 dataset\n",
    "!python -m wejepa.datasets.download --dataset-root ./data --dataset-name cifar100\n",
    "\n",
    "# download cub200 dataset\n",
    "!python -m wejepa.datasets.download --dataset-root ./data --dataset-name cub200 --splits train,test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21635921",
   "metadata": {},
   "source": [
    "### 2. Pre-training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90f233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "Model has 4,449,408 trainable parameters.\n",
      "Training:  25%|████████                        | 49/195 [00:10<00:27,  5.37it/s]Epoch 1 Iter 50/195 | Loss 0.2530 | 1244.2 img/s\n",
      "Training:  51%|████████████████▏               | 99/195 [00:19<00:16,  5.74it/s]Epoch 1 Iter 100/195 | Loss 0.2100 | 1310.0 img/s\n",
      "Training:  76%|███████████████████████▋       | 149/195 [00:28<00:08,  5.20it/s]Epoch 1 Iter 150/195 | Loss 0.1971 | 1319.0 img/s\n",
      "Training: 100%|███████████████████████████████| 195/195 [00:37<00:00,  5.19it/s]\n",
      "Epoch completed.\n",
      "Saved checkpoint to /home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa/ijepa_epoch_0001.pt\n",
      "Epoch 1/5 | loss=0.1895\n",
      "Training:  25%|████████                        | 49/195 [00:09<00:27,  5.23it/s]Epoch 2 Iter 50/195 | Loss 0.1672 | 1355.3 img/s\n",
      "Training:  51%|████████████████▏               | 99/195 [00:18<00:14,  6.59it/s]Epoch 2 Iter 100/195 | Loss 0.1604 | 1358.7 img/s\n",
      "Training:  76%|███████████████████████▋       | 149/195 [00:28<00:09,  5.06it/s]Epoch 2 Iter 150/195 | Loss 0.1599 | 1352.3 img/s\n",
      "Training: 100%|███████████████████████████████| 195/195 [00:37<00:00,  5.26it/s]\n",
      "Epoch completed.\n",
      "Saved checkpoint to /home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa/ijepa_epoch_0002.pt\n",
      "Epoch 2/5 | loss=0.1563\n",
      "Training:  25%|████████                        | 49/195 [00:09<00:26,  5.42it/s]Epoch 3 Iter 50/195 | Loss 0.1292 | 1362.9 img/s\n",
      "Training:  51%|████████████████▏               | 99/195 [00:18<00:17,  5.48it/s]Epoch 3 Iter 100/195 | Loss 0.1242 | 1355.4 img/s\n",
      "Training:  76%|███████████████████████▋       | 149/195 [00:27<00:08,  5.23it/s]Epoch 3 Iter 150/195 | Loss 0.1198 | 1373.4 img/s\n",
      "Training: 100%|███████████████████████████████| 195/195 [00:36<00:00,  5.37it/s]\n",
      "Epoch completed.\n",
      "Saved checkpoint to /home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa/ijepa_epoch_0003.pt\n",
      "Epoch 3/5 | loss=0.1156\n",
      "Training:  25%|████████                        | 49/195 [00:09<00:31,  4.69it/s]Epoch 4 Iter 50/195 | Loss 0.0940 | 1319.2 img/s\n",
      "Training:  51%|████████████████▏               | 99/195 [00:19<00:21,  4.51it/s]Epoch 4 Iter 100/195 | Loss 0.0927 | 1322.2 img/s\n",
      "Training:  76%|███████████████████████▋       | 149/195 [00:29<00:09,  4.90it/s]Epoch 4 Iter 150/195 | Loss 0.0917 | 1311.7 img/s\n",
      "Training: 100%|███████████████████████████████| 195/195 [00:38<00:00,  5.11it/s]\n",
      "Epoch completed.\n",
      "Saved checkpoint to /home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa/ijepa_epoch_0004.pt\n",
      "Epoch 4/5 | loss=0.0898\n",
      "Training:  25%|████████                        | 49/195 [00:09<00:33,  4.42it/s]Epoch 5 Iter 50/195 | Loss 0.0841 | 1326.7 img/s\n",
      "Training:  51%|████████████████▏               | 99/195 [00:19<00:17,  5.47it/s]Epoch 5 Iter 100/195 | Loss 0.0835 | 1322.1 img/s\n",
      "Training:  76%|███████████████████████▋       | 149/195 [00:29<00:10,  4.56it/s]Epoch 5 Iter 150/195 | Loss 0.0819 | 1315.9 img/s\n",
      "Training: 100%|███████████████████████████████| 195/195 [00:37<00:00,  5.21it/s]\n",
      "Epoch completed.\n",
      "Saved checkpoint to /home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa/ijepa_epoch_0005.pt\n",
      "Epoch 5/5 | loss=0.0826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the cli\n",
    "\n",
    "# Clear\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "# Train using default cifar100 config + custom ViT backbone\n",
    "# !python -m wejepa.train.pretrain --print-config     # print only\n",
    "# !python -m wejepa.train.pretrain                    # train\n",
    "_\n",
    "# FIXME: bug when using .arrow files, the file path is not correctly set, workaround is to rename the arrow file\n",
    "#   cp fall2025_deeplearning-train.arrow tsbpp___fall2025_deeplearning-train.arrow\n",
    "\n",
    "# print where --config searches for config files\n",
    "!python -m wejepa.train.pretrain --config hf224_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# programmatically\n",
    "from wejepa import default_config, launch_pretraining\n",
    "cfg = default_config()\n",
    "launch_pretraining(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffda647",
   "metadata": {},
   "source": [
    "### 3. Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2913c702",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.finetune' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.finetune'; this may result in unpredictable behaviour\n",
      "[Linear probe] Epoch 1/10 | loss=4.5246 | train_acc=0.027 | val_acc=0.042\n",
      "[Linear probe] Epoch 2/10 | loss=4.4346 | train_acc=0.040 | val_acc=0.049\n",
      "[Linear probe] Epoch 3/10 | loss=4.3911 | train_acc=0.042 | val_acc=0.052\n",
      "[Linear probe] Epoch 4/10 | loss=4.3613 | train_acc=0.046 | val_acc=0.056\n",
      "[Linear probe] Epoch 5/10 | loss=4.3389 | train_acc=0.047 | val_acc=0.063\n",
      "[Linear probe] Epoch 6/10 | loss=4.3260 | train_acc=0.049 | val_acc=0.064\n",
      "[Linear probe] Epoch 7/10 | loss=4.3143 | train_acc=0.051 | val_acc=0.063\n",
      "[Linear probe] Epoch 8/10 | loss=4.3070 | train_acc=0.051 | val_acc=0.069\n",
      "[Linear probe] Epoch 9/10 | loss=4.2993 | train_acc=0.054 | val_acc=0.069\n",
      "[Linear probe] Epoch 10/10 | loss=4.2907 | train_acc=0.055 | val_acc=0.072\n"
     ]
    }
   ],
   "source": [
    "# using the cli\n",
    "!python -m wejepa.train.finetune \\\n",
    "    --checkpoint outputs/ijepa/ijepa_epoch_0005.pt \\\n",
    "    --epochs 10 \\\n",
    "    --batch-size 256 \\\n",
    "    --lr 3e-4 \\\n",
    "    --num-classes 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f283f657",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear probe] Epoch 1/5 | loss=4.4194 | train_acc=0.037 | val_acc=0.055\n",
      "[Linear probe] Epoch 2/5 | loss=4.3261 | train_acc=0.046 | val_acc=0.062\n",
      "[Linear probe] Epoch 3/5 | loss=4.2992 | train_acc=0.051 | val_acc=0.065\n",
      "[Linear probe] Epoch 4/5 | loss=4.2842 | train_acc=0.053 | val_acc=0.073\n",
      "[Linear probe] Epoch 5/5 | loss=4.2742 | train_acc=0.055 | val_acc=0.070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearProbe(\n",
       "  (backbone): IJEPA_base(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (conv): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "    )\n",
       "    (post_emb_norm): Identity()\n",
       "    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    (student_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (teacher_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (predictor): Predictor(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=192, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# programmatically\n",
    "from wejepa.train import FinetuneConfig, train_linear_probe\n",
    "\n",
    "ft_cfg = FinetuneConfig(\n",
    "    checkpoint_path=\"outputs/ijepa/ijepa_epoch_0005.pt\",\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "train_linear_probe(ft_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e75d3",
   "metadata": {},
   "source": [
    "### 4. Different Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a884a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered backbones: \n",
      "- convnext_small\n",
      "- convnext_tiny\n",
      "- swin_s\n",
      "- swin_t\n",
      "- vit_b_16\n",
      "\n",
      "Pretraining with backbone: vit_b_16\n",
      "\n",
      "Pretraining with backbone: swin_t\n",
      "\n",
      "Pretraining with backbone: convnext_tiny\n",
      "\n",
      "Backbone: convnext_small\n",
      "Image size: 224 | Patch size: 32\n",
      "Total trainable params: 121.15M\n",
      "Student + predictor params: 70.88M\n",
      "Pred shape: (4, 1, 9, 768) | Target shape: (4, 1, 9, 768)\n",
      "{\n",
      "  \"data\": {\n",
      "    \"dataset_root\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\",\n",
      "    \"dataset_name\": \"cifar100\",\n",
      "    \"image_size\": 224,\n",
      "    \"train_batch_size\": 256,\n",
      "    \"eval_batch_size\": 512,\n",
      "    \"num_workers\": 4,\n",
      "    \"pin_memory\": true,\n",
      "    \"persistent_workers\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"crop_scale\": [\n",
      "      0.6,\n",
      "      1.0\n",
      "    ],\n",
      "    \"color_jitter\": 0.5,\n",
      "    \"use_color_distortion\": true,\n",
      "    \"use_horizontal_flip\": true,\n",
      "    \"normalization_mean\": [\n",
      "      0.5071,\n",
      "      0.4867,\n",
      "      0.4408\n",
      "    ],\n",
      "    \"normalization_std\": [\n",
      "      0.2675,\n",
      "      0.2565,\n",
      "      0.2761\n",
      "    ],\n",
      "    \"use_fake_data\": false,\n",
      "    \"fake_data_size\": 512\n",
      "  },\n",
      "  \"mask\": {\n",
      "    \"target_aspect_ratio\": [\n",
      "      0.75,\n",
      "      1.5\n",
      "    ],\n",
      "    \"target_scale\": [\n",
      "      0.15,\n",
      "      0.2\n",
      "    ],\n",
      "    \"context_aspect_ratio\": 1.0,\n",
      "    \"context_scale\": [\n",
      "      0.85,\n",
      "      1.0\n",
      "    ],\n",
      "    \"num_target_blocks\": 4\n",
      "  },\n",
      "  \"model\": {\n",
      "    \"img_size\": 224,\n",
      "    \"patch_size\": 32,\n",
      "    \"in_chans\": 3,\n",
      "    \"embed_dim\": 384,\n",
      "    \"enc_depth\": 6,\n",
      "    \"pred_depth\": 4,\n",
      "    \"num_heads\": 6,\n",
      "    \"post_emb_norm\": false,\n",
      "    \"layer_dropout\": 0.0,\n",
      "    \"classification_backbone\": \"convnext_small\",\n",
      "    \"classification_num_classes\": 100,\n",
      "    \"classification_pretrained\": false\n",
      "  },\n",
      "  \"optimizer\": {\n",
      "    \"epochs\": 5,\n",
      "    \"warmup_epochs\": 1,\n",
      "    \"base_learning_rate\": 0.001,\n",
      "    \"start_learning_rate\": 0.0001,\n",
      "    \"final_learning_rate\": 1e-05,\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"final_weight_decay\": 0.2,\n",
      "    \"betas\": [\n",
      "      0.9,\n",
      "      0.95\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"grad_clip_norm\": 1.0,\n",
      "    \"momentum_teacher\": 0.996,\n",
      "    \"momentum_teacher_final\": 1.0\n",
      "  },\n",
      "  \"hardware\": {\n",
      "    \"seed\": 42,\n",
      "    \"world_size\": null,\n",
      "    \"mixed_precision\": true,\n",
      "    \"compile_model\": false,\n",
      "    \"log_every\": 50,\n",
      "    \"checkpoint_every\": 1,\n",
      "    \"output_dir\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa\"\n",
      "  }\n",
      "}\n",
      "Saved config for convnext_small at configs/pretrain_convnext_small.json\n",
      "\n",
      "Backbone: convnext_tiny\n",
      "Image size: 224 | Patch size: 32\n",
      "Total trainable params: 99.51M\n",
      "Student + predictor params: 70.88M\n",
      "Pred shape: (4, 1, 9, 768) | Target shape: (4, 1, 9, 768)\n",
      "{\n",
      "  \"data\": {\n",
      "    \"dataset_root\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\",\n",
      "    \"dataset_name\": \"cifar100\",\n",
      "    \"image_size\": 224,\n",
      "    \"train_batch_size\": 256,\n",
      "    \"eval_batch_size\": 512,\n",
      "    \"num_workers\": 4,\n",
      "    \"pin_memory\": true,\n",
      "    \"persistent_workers\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"crop_scale\": [\n",
      "      0.6,\n",
      "      1.0\n",
      "    ],\n",
      "    \"color_jitter\": 0.5,\n",
      "    \"use_color_distortion\": true,\n",
      "    \"use_horizontal_flip\": true,\n",
      "    \"normalization_mean\": [\n",
      "      0.5071,\n",
      "      0.4867,\n",
      "      0.4408\n",
      "    ],\n",
      "    \"normalization_std\": [\n",
      "      0.2675,\n",
      "      0.2565,\n",
      "      0.2761\n",
      "    ],\n",
      "    \"use_fake_data\": false,\n",
      "    \"fake_data_size\": 512\n",
      "  },\n",
      "  \"mask\": {\n",
      "    \"target_aspect_ratio\": [\n",
      "      0.75,\n",
      "      1.5\n",
      "    ],\n",
      "    \"target_scale\": [\n",
      "      0.15,\n",
      "      0.2\n",
      "    ],\n",
      "    \"context_aspect_ratio\": 1.0,\n",
      "    \"context_scale\": [\n",
      "      0.85,\n",
      "      1.0\n",
      "    ],\n",
      "    \"num_target_blocks\": 4\n",
      "  },\n",
      "  \"model\": {\n",
      "    \"img_size\": 224,\n",
      "    \"patch_size\": 32,\n",
      "    \"in_chans\": 3,\n",
      "    \"embed_dim\": 768,\n",
      "    \"enc_depth\": 6,\n",
      "    \"pred_depth\": 4,\n",
      "    \"num_heads\": 12,\n",
      "    \"post_emb_norm\": false,\n",
      "    \"layer_dropout\": 0.0,\n",
      "    \"classification_backbone\": \"convnext_tiny\",\n",
      "    \"classification_num_classes\": 100,\n",
      "    \"classification_pretrained\": false\n",
      "  },\n",
      "  \"optimizer\": {\n",
      "    \"epochs\": 5,\n",
      "    \"warmup_epochs\": 1,\n",
      "    \"base_learning_rate\": 0.001,\n",
      "    \"start_learning_rate\": 0.0001,\n",
      "    \"final_learning_rate\": 1e-05,\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"final_weight_decay\": 0.2,\n",
      "    \"betas\": [\n",
      "      0.9,\n",
      "      0.95\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"grad_clip_norm\": 1.0,\n",
      "    \"momentum_teacher\": 0.996,\n",
      "    \"momentum_teacher_final\": 1.0\n",
      "  },\n",
      "  \"hardware\": {\n",
      "    \"seed\": 42,\n",
      "    \"world_size\": null,\n",
      "    \"mixed_precision\": true,\n",
      "    \"compile_model\": false,\n",
      "    \"log_every\": 50,\n",
      "    \"checkpoint_every\": 1,\n",
      "    \"output_dir\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa\"\n",
      "  }\n",
      "}\n",
      "Saved config for convnext_tiny at configs/pretrain_convnext_tiny.json\n",
      "\n",
      "Backbone: swin_s\n",
      "Image size: 224 | Patch size: 4\n",
      "Total trainable params: 49.65M\n",
      "Student + predictor params: 0.01M\n",
      "Pred shape: (4, 1, 231, 7) | Target shape: (4, 1, 231, 7)\n",
      "{\n",
      "  \"data\": {\n",
      "    \"dataset_root\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\",\n",
      "    \"dataset_name\": \"cifar100\",\n",
      "    \"image_size\": 224,\n",
      "    \"train_batch_size\": 256,\n",
      "    \"eval_batch_size\": 512,\n",
      "    \"num_workers\": 4,\n",
      "    \"pin_memory\": true,\n",
      "    \"persistent_workers\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"crop_scale\": [\n",
      "      0.6,\n",
      "      1.0\n",
      "    ],\n",
      "    \"color_jitter\": 0.5,\n",
      "    \"use_color_distortion\": true,\n",
      "    \"use_horizontal_flip\": true,\n",
      "    \"normalization_mean\": [\n",
      "      0.5071,\n",
      "      0.4867,\n",
      "      0.4408\n",
      "    ],\n",
      "    \"normalization_std\": [\n",
      "      0.2675,\n",
      "      0.2565,\n",
      "      0.2761\n",
      "    ],\n",
      "    \"use_fake_data\": false,\n",
      "    \"fake_data_size\": 512\n",
      "  },\n",
      "  \"mask\": {\n",
      "    \"target_aspect_ratio\": [\n",
      "      0.75,\n",
      "      1.5\n",
      "    ],\n",
      "    \"target_scale\": [\n",
      "      0.15,\n",
      "      0.2\n",
      "    ],\n",
      "    \"context_aspect_ratio\": 1.0,\n",
      "    \"context_scale\": [\n",
      "      0.85,\n",
      "      1.0\n",
      "    ],\n",
      "    \"num_target_blocks\": 4\n",
      "  },\n",
      "  \"model\": {\n",
      "    \"img_size\": 224,\n",
      "    \"patch_size\": 4,\n",
      "    \"in_chans\": 3,\n",
      "    \"embed_dim\": 768,\n",
      "    \"enc_depth\": 6,\n",
      "    \"pred_depth\": 4,\n",
      "    \"num_heads\": 12,\n",
      "    \"post_emb_norm\": false,\n",
      "    \"layer_dropout\": 0.0,\n",
      "    \"classification_backbone\": \"swin_s\",\n",
      "    \"classification_num_classes\": 100,\n",
      "    \"classification_pretrained\": false\n",
      "  },\n",
      "  \"optimizer\": {\n",
      "    \"epochs\": 5,\n",
      "    \"warmup_epochs\": 1,\n",
      "    \"base_learning_rate\": 0.001,\n",
      "    \"start_learning_rate\": 0.0001,\n",
      "    \"final_learning_rate\": 1e-05,\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"final_weight_decay\": 0.2,\n",
      "    \"betas\": [\n",
      "      0.9,\n",
      "      0.95\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"grad_clip_norm\": 1.0,\n",
      "    \"momentum_teacher\": 0.996,\n",
      "    \"momentum_teacher_final\": 1.0\n",
      "  },\n",
      "  \"hardware\": {\n",
      "    \"seed\": 42,\n",
      "    \"world_size\": null,\n",
      "    \"mixed_precision\": true,\n",
      "    \"compile_model\": false,\n",
      "    \"log_every\": 50,\n",
      "    \"checkpoint_every\": 1,\n",
      "    \"output_dir\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa\"\n",
      "  }\n",
      "}\n",
      "Saved config for swin_s at configs/pretrain_swin_s.json\n",
      "\n",
      "Backbone: swin_t\n",
      "Image size: 224 | Patch size: 4\n",
      "Total trainable params: 28.33M\n",
      "Student + predictor params: 0.01M\n",
      "Pred shape: (4, 1, 231, 7) | Target shape: (4, 1, 231, 7)\n",
      "{\n",
      "  \"data\": {\n",
      "    \"dataset_root\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\",\n",
      "    \"dataset_name\": \"cifar100\",\n",
      "    \"image_size\": 224,\n",
      "    \"train_batch_size\": 256,\n",
      "    \"eval_batch_size\": 512,\n",
      "    \"num_workers\": 4,\n",
      "    \"pin_memory\": true,\n",
      "    \"persistent_workers\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"crop_scale\": [\n",
      "      0.6,\n",
      "      1.0\n",
      "    ],\n",
      "    \"color_jitter\": 0.5,\n",
      "    \"use_color_distortion\": true,\n",
      "    \"use_horizontal_flip\": true,\n",
      "    \"normalization_mean\": [\n",
      "      0.5071,\n",
      "      0.4867,\n",
      "      0.4408\n",
      "    ],\n",
      "    \"normalization_std\": [\n",
      "      0.2675,\n",
      "      0.2565,\n",
      "      0.2761\n",
      "    ],\n",
      "    \"use_fake_data\": false,\n",
      "    \"fake_data_size\": 512\n",
      "  },\n",
      "  \"mask\": {\n",
      "    \"target_aspect_ratio\": [\n",
      "      0.75,\n",
      "      1.5\n",
      "    ],\n",
      "    \"target_scale\": [\n",
      "      0.15,\n",
      "      0.2\n",
      "    ],\n",
      "    \"context_aspect_ratio\": 1.0,\n",
      "    \"context_scale\": [\n",
      "      0.85,\n",
      "      1.0\n",
      "    ],\n",
      "    \"num_target_blocks\": 4\n",
      "  },\n",
      "  \"model\": {\n",
      "    \"img_size\": 224,\n",
      "    \"patch_size\": 4,\n",
      "    \"in_chans\": 3,\n",
      "    \"embed_dim\": 768,\n",
      "    \"enc_depth\": 6,\n",
      "    \"pred_depth\": 4,\n",
      "    \"num_heads\": 12,\n",
      "    \"post_emb_norm\": false,\n",
      "    \"layer_dropout\": 0.0,\n",
      "    \"classification_backbone\": \"swin_t\",\n",
      "    \"classification_num_classes\": 100,\n",
      "    \"classification_pretrained\": false\n",
      "  },\n",
      "  \"optimizer\": {\n",
      "    \"epochs\": 5,\n",
      "    \"warmup_epochs\": 1,\n",
      "    \"base_learning_rate\": 0.001,\n",
      "    \"start_learning_rate\": 0.0001,\n",
      "    \"final_learning_rate\": 1e-05,\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"final_weight_decay\": 0.2,\n",
      "    \"betas\": [\n",
      "      0.9,\n",
      "      0.95\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"grad_clip_norm\": 1.0,\n",
      "    \"momentum_teacher\": 0.996,\n",
      "    \"momentum_teacher_final\": 1.0\n",
      "  },\n",
      "  \"hardware\": {\n",
      "    \"seed\": 42,\n",
      "    \"world_size\": null,\n",
      "    \"mixed_precision\": true,\n",
      "    \"compile_model\": false,\n",
      "    \"log_every\": 50,\n",
      "    \"checkpoint_every\": 1,\n",
      "    \"output_dir\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa\"\n",
      "  }\n",
      "}\n",
      "Saved config for swin_t at configs/pretrain_swin_t.json\n",
      "\n",
      "Backbone: vit_b_16\n",
      "Image size: 224 | Patch size: 16\n",
      "Total trainable params: 157.60M\n",
      "Student + predictor params: 70.88M\n",
      "Pred shape: (4, 1, 36, 768) | Target shape: (4, 1, 36, 768)\n",
      "{\n",
      "  \"data\": {\n",
      "    \"dataset_root\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\",\n",
      "    \"dataset_name\": \"cifar100\",\n",
      "    \"image_size\": 224,\n",
      "    \"train_batch_size\": 256,\n",
      "    \"eval_batch_size\": 512,\n",
      "    \"num_workers\": 4,\n",
      "    \"pin_memory\": true,\n",
      "    \"persistent_workers\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"crop_scale\": [\n",
      "      0.6,\n",
      "      1.0\n",
      "    ],\n",
      "    \"color_jitter\": 0.5,\n",
      "    \"use_color_distortion\": true,\n",
      "    \"use_horizontal_flip\": true,\n",
      "    \"normalization_mean\": [\n",
      "      0.5071,\n",
      "      0.4867,\n",
      "      0.4408\n",
      "    ],\n",
      "    \"normalization_std\": [\n",
      "      0.2675,\n",
      "      0.2565,\n",
      "      0.2761\n",
      "    ],\n",
      "    \"use_fake_data\": false,\n",
      "    \"fake_data_size\": 512\n",
      "  },\n",
      "  \"mask\": {\n",
      "    \"target_aspect_ratio\": [\n",
      "      0.75,\n",
      "      1.5\n",
      "    ],\n",
      "    \"target_scale\": [\n",
      "      0.15,\n",
      "      0.2\n",
      "    ],\n",
      "    \"context_aspect_ratio\": 1.0,\n",
      "    \"context_scale\": [\n",
      "      0.85,\n",
      "      1.0\n",
      "    ],\n",
      "    \"num_target_blocks\": 4\n",
      "  },\n",
      "  \"model\": {\n",
      "    \"img_size\": 224,\n",
      "    \"patch_size\": 16,\n",
      "    \"in_chans\": 3,\n",
      "    \"embed_dim\": 768,\n",
      "    \"enc_depth\": 6,\n",
      "    \"pred_depth\": 4,\n",
      "    \"num_heads\": 12,\n",
      "    \"post_emb_norm\": false,\n",
      "    \"layer_dropout\": 0.0,\n",
      "    \"classification_backbone\": \"vit_b_16\",\n",
      "    \"classification_num_classes\": 100,\n",
      "    \"classification_pretrained\": false\n",
      "  },\n",
      "  \"optimizer\": {\n",
      "    \"epochs\": 5,\n",
      "    \"warmup_epochs\": 1,\n",
      "    \"base_learning_rate\": 0.001,\n",
      "    \"start_learning_rate\": 0.0001,\n",
      "    \"final_learning_rate\": 1e-05,\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"final_weight_decay\": 0.2,\n",
      "    \"betas\": [\n",
      "      0.9,\n",
      "      0.95\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"grad_clip_norm\": 1.0,\n",
      "    \"momentum_teacher\": 0.996,\n",
      "    \"momentum_teacher_final\": 1.0\n",
      "  },\n",
      "  \"hardware\": {\n",
      "    \"seed\": 42,\n",
      "    \"world_size\": null,\n",
      "    \"mixed_precision\": true,\n",
      "    \"compile_model\": false,\n",
      "    \"log_every\": 50,\n",
      "    \"checkpoint_every\": 1,\n",
      "    \"output_dir\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa\"\n",
      "  }\n",
      "}\n",
      "Saved config for vit_b_16 at configs/pretrain_vit_b_16.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "from wejepa.backbones import adapt_config_for_backbone, available_backbones\n",
    "from wejepa.config import IJepaConfig\n",
    "from wejepa import default_config, launch_pretraining, IJEPA_base\n",
    "\n",
    "print(\"Registered backbones: \")\n",
    "for backbone in available_backbones():\n",
    "    print(f\"- {backbone}\")\n",
    "\n",
    "candidates = [\"vit_b_16\", \"swin_t\", \"convnext_tiny\"]\n",
    "for backbone in candidates:\n",
    "    print(f\"\\nPretraining with backbone: {backbone}\")\n",
    "\n",
    "for backbone in available_backbones():\n",
    "    cfg = adapt_config_for_backbone(default_config(), backbone)\n",
    "    print(f\"\\nBackbone: {backbone}\")\n",
    "    print(f\"Image size: {cfg.model.img_size} | Patch size: {cfg.model.patch_size}\")\n",
    "\n",
    "    model = IJEPA_base(\n",
    "        img_size=cfg.model.img_size,\n",
    "        patch_size=cfg.model.patch_size,\n",
    "        in_chans=cfg.model.in_chans,\n",
    "        embed_dim=cfg.model.embed_dim,\n",
    "        enc_depth=cfg.model.enc_depth,\n",
    "        pred_depth=cfg.model.pred_depth,\n",
    "        num_heads=cfg.model.num_heads,\n",
    "        backbone=cfg.model.classification_backbone,\n",
    "        pretrained=cfg.model.classification_pretrained,\n",
    "    )\n",
    "\n",
    "    print(f\"Total trainable params: {model.count_trainable_parameters() / 1e6:.2f}M\")\n",
    "    print(f\"Student + predictor params: {model.count_parameters() / 1e6:.2f}M\")\n",
    "\n",
    "    dummy = torch.randn(1, cfg.model.in_chans, cfg.model.img_size, cfg.model.img_size)\n",
    "    preds, targets = model(dummy)\n",
    "    print(f\"Pred shape: {tuple(preds.shape)} | Target shape: {tuple(targets.shape)}\")\n",
    "    print(json.dumps(cfg.to_dict(), indent=2))\n",
    "\n",
    "    cfg.hardware.output_dir = f\"./outputs/ijepa/{backbone}\"\n",
    "    cfg_path = Path(f\"configs/pretrain_{backbone}.json\")\n",
    "    cfg_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cfg_path.write_text(json.dumps(cfg.to_dict(), indent=2))\n",
    "    print(f\"Saved config for {backbone} at {cfg_path}\")\n",
    "\n",
    "    # launch_pretraining(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4a93a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ijepa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
