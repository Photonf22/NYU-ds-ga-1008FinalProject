{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc761ad1",
   "metadata": {},
   "source": [
    "# Example based on our README.md\n",
    "1. Dataset download\n",
    "2. Pre-training\n",
    "3. Fine-tuning\n",
    "4. Inference\n",
    "5. Different backbones\n",
    "6. Visualize\n",
    "7. Extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de0bef",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e8ab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 'train[:10]' available under /home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\n",
      "Split 'train' available under /home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\n",
      "Split 'test' available under /home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\n"
     ]
    }
   ],
   "source": [
    "# using the cli\n",
    "\n",
    "# download class dataset\n",
    "# !python -m wejepa.datasets.download --dataset-root ./data --dataset-name tsbpp/fall2025_deeplearning --splits train\n",
    "\n",
    "# for development, download a small subset\n",
    "!python -m wejepa.datasets.download --dataset-root ./data --dataset-name tsbpp/fall2025_deeplearning --splits 'train[:10]'\n",
    "\n",
    "# download cifar100 dataset\n",
    "!python -m wejepa.datasets.download --dataset-root ./data --dataset-name cifar100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21635921",
   "metadata": {},
   "source": [
    "### 2. Pre-training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the cli\n",
    "\n",
    "# Clear\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "# Train using default cifar100 config + custom ViT backbone\n",
    "# !python -m wejepa.train.pretrain --print-config     # print only\n",
    "# !python -m wejepa.train.pretrain                    # train\n",
    "_\n",
    "# FIXME: bug when using .arrow files, the file path is not correctly set, workaround is to rename the arrow file\n",
    "#   cp fall2025_deeplearning-train.arrow tsbpp___fall2025_deeplearning-train.arrow\n",
    "\n",
    "# print where --config searches for config files\n",
    "!python -m wejepa.train.pretrain --config hf224_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# programmatically\n",
    "from wejepa import default_config, launch_pretraining\n",
    "cfg = default_config()\n",
    "launch_pretraining(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffda647",
   "metadata": {},
   "source": [
    "### 3. Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the cli\n",
    "!python -m wejepa.train.finetune \\\n",
    "    --checkpoint outputs/ijepa/ijepa_epoch_0005.pt \\\n",
    "    --epochs 10 \\\n",
    "    --batch-size 256 \\\n",
    "    --lr 3e-4 \\\n",
    "    --num-classes 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# programmatically\n",
    "from wejepa.train import FinetuneConfig, train_linear_probe\n",
    "\n",
    "ft_cfg = FinetuneConfig(\n",
    "    checkpoint_path=\"outputs/ijepa/ijepa_epoch_0005.pt\",\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "train_linear_probe(ft_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4317f0",
   "metadata": {},
   "source": [
    "### 4. Running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from wejepa.train import load_backbone_from_checkpoint\n",
    "from wejepa import default_config\n",
    "\n",
    "cfg = default_config()\n",
    "backbone = load_backbone_from_checkpoint(\"outputs/ijepa/ijepa_epoch_0005.pt\", cfg)\n",
    "backbone.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(cfg.data.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cfg.data.normalization_mean, cfg.data.normalization_std),\n",
    "])\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"./data/tsbpp___fall2025_deeplearning\",\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "label_feature = ds.features[\"label\"] if hasattr(ds, \"features\") else None\n",
    "label_names = label_feature.names if label_feature is not None else None\n",
    "num_classes = len(label_names) if label_names is not None else 100 # default to 100 classes\n",
    "\n",
    "decoder = LinearProbe(backbone, num_classes)\n",
    "decoder.load_state_dict(torch.load(\"outputs/ijepa/linear_probe.pt\", map_location=\"cpu\"))\n",
    "decoder.eval()\n",
    "\n",
    "# grab an image from the dataset\n",
    "image = transform(ds[0][\"image\"]).unsqueeze(0)\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = decoder(image)\n",
    "    probs = torch.softmax(logits,dim=1)\n",
    "    pred_ind = int(probs.argmax(dim=1).item())\n",
    "\n",
    "pred_label = label_names[pred_ind] if label_names is not None else str(pred_ind)\n",
    "top5_inds = probs.topk(5).indices.squeeze(0).tolist()\n",
    "top5_labels = [label_names[i] if label_names is not None else str(i) for i in top5_inds]\n",
    "print(f\"Predicted label: {pred_label}\")\n",
    "print(f\"Top-5 predicted labels: {top5_labels}\")\n",
    "\n",
    "# remove batch dimension and convert to numpy\n",
    "img_np = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# undo normalization for display\n",
    "mean = np.array(cfg.data.normalization_mean)\n",
    "std = np.array(cfg.data.normalization_std)\n",
    "img_np = (img_np * std) + mean\n",
    "img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "plt.imshow(img_np)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = backbone(image)\n",
    "    pooled = tokens.mean(dim=1)  # embeddings for downstream heads\n",
    "\n",
    "# TODO: use the embeddings `pooled` for downstream tasks like classification \n",
    "print(f\"Extracted embeddings shape: {pooled.shape}\")\n",
    "\n",
    "num_classes = 100  # adjust based on your dataset\n",
    "classifier = torch.nn.Linear(pooled.size(1), num_classes)\n",
    "logits = classifier(pooled)\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "# display the classified scores\n",
    "print(f\"Classified scores: {logits}\")\n",
    "\n",
    "# assign predicted class\n",
    "predicted_class = torch.argmax(logits, dim=1)\n",
    "print(f\"Predicted class: {predicted_class.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e75d3",
   "metadata": {},
   "source": [
    "### 5. Different Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a884a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backbone: convnext_small\n",
      "Image size: 224 | Patch size: 32\n",
      "Trainable params: 121.15M\n",
      "Student + Predictor params: 70.88M\n",
      "Pred shape: (4, 1, 9, 768) | Target shape: (4, 1, 9, 768)\n",
      "{\n",
      "  \"data\": {\n",
      "    \"dataset_root\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\",\n",
      "    \"dataset_name\": \"cifar100\",\n",
      "    \"image_size\": 224,\n",
      "    \"train_batch_size\": 256,\n",
      "    \"eval_batch_size\": 512,\n",
      "    \"num_workers\": 4,\n",
      "    \"pin_memory\": true,\n",
      "    \"persistent_workers\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"crop_scale\": [\n",
      "      0.6,\n",
      "      1.0\n",
      "    ],\n",
      "    \"color_jitter\": 0.5,\n",
      "    \"use_color_distortion\": true,\n",
      "    \"use_horizontal_flip\": true,\n",
      "    \"normalization_mean\": [\n",
      "      0.5071,\n",
      "      0.4867,\n",
      "      0.4408\n",
      "    ],\n",
      "    \"normalization_std\": [\n",
      "      0.2675,\n",
      "      0.2565,\n",
      "      0.2761\n",
      "    ],\n",
      "    \"use_fake_data\": false,\n",
      "    \"fake_data_size\": 512\n",
      "  },\n",
      "  \"mask\": {\n",
      "    \"target_aspect_ratio\": [\n",
      "      0.75,\n",
      "      1.5\n",
      "    ],\n",
      "    \"target_scale\": [\n",
      "      0.15,\n",
      "      0.2\n",
      "    ],\n",
      "    \"context_aspect_ratio\": 1.0,\n",
      "    \"context_scale\": [\n",
      "      0.85,\n",
      "      1.0\n",
      "    ],\n",
      "    \"num_target_blocks\": 4\n",
      "  },\n",
      "  \"model\": {\n",
      "    \"img_size\": 224,\n",
      "    \"patch_size\": 32,\n",
      "    \"in_chans\": 3,\n",
      "    \"embed_dim\": 384,\n",
      "    \"enc_depth\": 6,\n",
      "    \"pred_depth\": 4,\n",
      "    \"num_heads\": 6,\n",
      "    \"post_emb_norm\": false,\n",
      "    \"layer_dropout\": 0.0,\n",
      "    \"classification_backbone\": \"convnext_small\",\n",
      "    \"classification_num_classes\": 100,\n",
      "    \"classification_pretrained\": true\n",
      "  },\n",
      "  \"optimizer\": {\n",
      "    \"epochs\": 5,\n",
      "    \"warmup_epochs\": 1,\n",
      "    \"base_learning_rate\": 0.001,\n",
      "    \"start_learning_rate\": 0.0001,\n",
      "    \"final_learning_rate\": 1e-05,\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"final_weight_decay\": 0.2,\n",
      "    \"betas\": [\n",
      "      0.9,\n",
      "      0.95\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"grad_clip_norm\": 1.0,\n",
      "    \"momentum_teacher\": 0.996,\n",
      "    \"momentum_teacher_final\": 1.0\n",
      "  },\n",
      "  \"hardware\": {\n",
      "    \"seed\": 42,\n",
      "    \"world_size\": null,\n",
      "    \"mixed_precision\": true,\n",
      "    \"compile_model\": false,\n",
      "    \"log_every\": 50,\n",
      "    \"checkpoint_every\": 1,\n",
      "    \"output_dir\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Backbone: convnext_tiny\n",
      "Image size: 224 | Patch size: 32\n",
      "Trainable params: 99.51M\n",
      "Student + Predictor params: 70.88M\n",
      "Pred shape: (4, 1, 9, 768) | Target shape: (4, 1, 9, 768)\n",
      "{\n",
      "  \"data\": {\n",
      "    \"dataset_root\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\",\n",
      "    \"dataset_name\": \"cifar100\",\n",
      "    \"image_size\": 224,\n",
      "    \"train_batch_size\": 256,\n",
      "    \"eval_batch_size\": 512,\n",
      "    \"num_workers\": 4,\n",
      "    \"pin_memory\": true,\n",
      "    \"persistent_workers\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"crop_scale\": [\n",
      "      0.6,\n",
      "      1.0\n",
      "    ],\n",
      "    \"color_jitter\": 0.5,\n",
      "    \"use_color_distortion\": true,\n",
      "    \"use_horizontal_flip\": true,\n",
      "    \"normalization_mean\": [\n",
      "      0.5071,\n",
      "      0.4867,\n",
      "      0.4408\n",
      "    ],\n",
      "    \"normalization_std\": [\n",
      "      0.2675,\n",
      "      0.2565,\n",
      "      0.2761\n",
      "    ],\n",
      "    \"use_fake_data\": false,\n",
      "    \"fake_data_size\": 512\n",
      "  },\n",
      "  \"mask\": {\n",
      "    \"target_aspect_ratio\": [\n",
      "      0.75,\n",
      "      1.5\n",
      "    ],\n",
      "    \"target_scale\": [\n",
      "      0.15,\n",
      "      0.2\n",
      "    ],\n",
      "    \"context_aspect_ratio\": 1.0,\n",
      "    \"context_scale\": [\n",
      "      0.85,\n",
      "      1.0\n",
      "    ],\n",
      "    \"num_target_blocks\": 4\n",
      "  },\n",
      "  \"model\": {\n",
      "    \"img_size\": 224,\n",
      "    \"patch_size\": 32,\n",
      "    \"in_chans\": 3,\n",
      "    \"embed_dim\": 768,\n",
      "    \"enc_depth\": 6,\n",
      "    \"pred_depth\": 4,\n",
      "    \"num_heads\": 12,\n",
      "    \"post_emb_norm\": false,\n",
      "    \"layer_dropout\": 0.0,\n",
      "    \"classification_backbone\": \"convnext_tiny\",\n",
      "    \"classification_num_classes\": 100,\n",
      "    \"classification_pretrained\": true\n",
      "  },\n",
      "  \"optimizer\": {\n",
      "    \"epochs\": 5,\n",
      "    \"warmup_epochs\": 1,\n",
      "    \"base_learning_rate\": 0.001,\n",
      "    \"start_learning_rate\": 0.0001,\n",
      "    \"final_learning_rate\": 1e-05,\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"final_weight_decay\": 0.2,\n",
      "    \"betas\": [\n",
      "      0.9,\n",
      "      0.95\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"grad_clip_norm\": 1.0,\n",
      "    \"momentum_teacher\": 0.996,\n",
      "    \"momentum_teacher_final\": 1.0\n",
      "  },\n",
      "  \"hardware\": {\n",
      "    \"seed\": 42,\n",
      "    \"world_size\": null,\n",
      "    \"mixed_precision\": true,\n",
      "    \"compile_model\": false,\n",
      "    \"log_every\": 50,\n",
      "    \"checkpoint_every\": 1,\n",
      "    \"output_dir\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Backbone: swin_s\n",
      "Image size: 224 | Patch size: 4\n",
      "Trainable params: 49.65M\n",
      "Student + Predictor params: 0.01M\n",
      "Pred shape: (4, 1, 231, 7) | Target shape: (4, 1, 231, 7)\n",
      "{\n",
      "  \"data\": {\n",
      "    \"dataset_root\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\",\n",
      "    \"dataset_name\": \"cifar100\",\n",
      "    \"image_size\": 224,\n",
      "    \"train_batch_size\": 256,\n",
      "    \"eval_batch_size\": 512,\n",
      "    \"num_workers\": 4,\n",
      "    \"pin_memory\": true,\n",
      "    \"persistent_workers\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"crop_scale\": [\n",
      "      0.6,\n",
      "      1.0\n",
      "    ],\n",
      "    \"color_jitter\": 0.5,\n",
      "    \"use_color_distortion\": true,\n",
      "    \"use_horizontal_flip\": true,\n",
      "    \"normalization_mean\": [\n",
      "      0.5071,\n",
      "      0.4867,\n",
      "      0.4408\n",
      "    ],\n",
      "    \"normalization_std\": [\n",
      "      0.2675,\n",
      "      0.2565,\n",
      "      0.2761\n",
      "    ],\n",
      "    \"use_fake_data\": false,\n",
      "    \"fake_data_size\": 512\n",
      "  },\n",
      "  \"mask\": {\n",
      "    \"target_aspect_ratio\": [\n",
      "      0.75,\n",
      "      1.5\n",
      "    ],\n",
      "    \"target_scale\": [\n",
      "      0.15,\n",
      "      0.2\n",
      "    ],\n",
      "    \"context_aspect_ratio\": 1.0,\n",
      "    \"context_scale\": [\n",
      "      0.85,\n",
      "      1.0\n",
      "    ],\n",
      "    \"num_target_blocks\": 4\n",
      "  },\n",
      "  \"model\": {\n",
      "    \"img_size\": 224,\n",
      "    \"patch_size\": 4,\n",
      "    \"in_chans\": 3,\n",
      "    \"embed_dim\": 768,\n",
      "    \"enc_depth\": 6,\n",
      "    \"pred_depth\": 4,\n",
      "    \"num_heads\": 12,\n",
      "    \"post_emb_norm\": false,\n",
      "    \"layer_dropout\": 0.0,\n",
      "    \"classification_backbone\": \"swin_s\",\n",
      "    \"classification_num_classes\": 100,\n",
      "    \"classification_pretrained\": true\n",
      "  },\n",
      "  \"optimizer\": {\n",
      "    \"epochs\": 5,\n",
      "    \"warmup_epochs\": 1,\n",
      "    \"base_learning_rate\": 0.001,\n",
      "    \"start_learning_rate\": 0.0001,\n",
      "    \"final_learning_rate\": 1e-05,\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"final_weight_decay\": 0.2,\n",
      "    \"betas\": [\n",
      "      0.9,\n",
      "      0.95\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"grad_clip_norm\": 1.0,\n",
      "    \"momentum_teacher\": 0.996,\n",
      "    \"momentum_teacher_final\": 1.0\n",
      "  },\n",
      "  \"hardware\": {\n",
      "    \"seed\": 42,\n",
      "    \"world_size\": null,\n",
      "    \"mixed_precision\": true,\n",
      "    \"compile_model\": false,\n",
      "    \"log_every\": 50,\n",
      "    \"checkpoint_every\": 1,\n",
      "    \"output_dir\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Backbone: swin_t\n",
      "Image size: 224 | Patch size: 4\n",
      "Trainable params: 28.33M\n",
      "Student + Predictor params: 0.01M\n",
      "Pred shape: (4, 1, 231, 7) | Target shape: (4, 1, 231, 7)\n",
      "{\n",
      "  \"data\": {\n",
      "    \"dataset_root\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\",\n",
      "    \"dataset_name\": \"cifar100\",\n",
      "    \"image_size\": 224,\n",
      "    \"train_batch_size\": 256,\n",
      "    \"eval_batch_size\": 512,\n",
      "    \"num_workers\": 4,\n",
      "    \"pin_memory\": true,\n",
      "    \"persistent_workers\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"crop_scale\": [\n",
      "      0.6,\n",
      "      1.0\n",
      "    ],\n",
      "    \"color_jitter\": 0.5,\n",
      "    \"use_color_distortion\": true,\n",
      "    \"use_horizontal_flip\": true,\n",
      "    \"normalization_mean\": [\n",
      "      0.5071,\n",
      "      0.4867,\n",
      "      0.4408\n",
      "    ],\n",
      "    \"normalization_std\": [\n",
      "      0.2675,\n",
      "      0.2565,\n",
      "      0.2761\n",
      "    ],\n",
      "    \"use_fake_data\": false,\n",
      "    \"fake_data_size\": 512\n",
      "  },\n",
      "  \"mask\": {\n",
      "    \"target_aspect_ratio\": [\n",
      "      0.75,\n",
      "      1.5\n",
      "    ],\n",
      "    \"target_scale\": [\n",
      "      0.15,\n",
      "      0.2\n",
      "    ],\n",
      "    \"context_aspect_ratio\": 1.0,\n",
      "    \"context_scale\": [\n",
      "      0.85,\n",
      "      1.0\n",
      "    ],\n",
      "    \"num_target_blocks\": 4\n",
      "  },\n",
      "  \"model\": {\n",
      "    \"img_size\": 224,\n",
      "    \"patch_size\": 4,\n",
      "    \"in_chans\": 3,\n",
      "    \"embed_dim\": 768,\n",
      "    \"enc_depth\": 6,\n",
      "    \"pred_depth\": 4,\n",
      "    \"num_heads\": 12,\n",
      "    \"post_emb_norm\": false,\n",
      "    \"layer_dropout\": 0.0,\n",
      "    \"classification_backbone\": \"swin_t\",\n",
      "    \"classification_num_classes\": 100,\n",
      "    \"classification_pretrained\": true\n",
      "  },\n",
      "  \"optimizer\": {\n",
      "    \"epochs\": 5,\n",
      "    \"warmup_epochs\": 1,\n",
      "    \"base_learning_rate\": 0.001,\n",
      "    \"start_learning_rate\": 0.0001,\n",
      "    \"final_learning_rate\": 1e-05,\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"final_weight_decay\": 0.2,\n",
      "    \"betas\": [\n",
      "      0.9,\n",
      "      0.95\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"grad_clip_norm\": 1.0,\n",
      "    \"momentum_teacher\": 0.996,\n",
      "    \"momentum_teacher_final\": 1.0\n",
      "  },\n",
      "  \"hardware\": {\n",
      "    \"seed\": 42,\n",
      "    \"world_size\": null,\n",
      "    \"mixed_precision\": true,\n",
      "    \"compile_model\": false,\n",
      "    \"log_every\": 50,\n",
      "    \"checkpoint_every\": 1,\n",
      "    \"output_dir\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Backbone: vit_b_16\n",
      "Image size: 224 | Patch size: 16\n",
      "Trainable params: 157.60M\n",
      "Student + Predictor params: 70.88M\n",
      "Pred shape: (4, 1, 36, 768) | Target shape: (4, 1, 36, 768)\n",
      "{\n",
      "  \"data\": {\n",
      "    \"dataset_root\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/data\",\n",
      "    \"dataset_name\": \"cifar100\",\n",
      "    \"image_size\": 224,\n",
      "    \"train_batch_size\": 256,\n",
      "    \"eval_batch_size\": 512,\n",
      "    \"num_workers\": 4,\n",
      "    \"pin_memory\": true,\n",
      "    \"persistent_workers\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"crop_scale\": [\n",
      "      0.6,\n",
      "      1.0\n",
      "    ],\n",
      "    \"color_jitter\": 0.5,\n",
      "    \"use_color_distortion\": true,\n",
      "    \"use_horizontal_flip\": true,\n",
      "    \"normalization_mean\": [\n",
      "      0.5071,\n",
      "      0.4867,\n",
      "      0.4408\n",
      "    ],\n",
      "    \"normalization_std\": [\n",
      "      0.2675,\n",
      "      0.2565,\n",
      "      0.2761\n",
      "    ],\n",
      "    \"use_fake_data\": false,\n",
      "    \"fake_data_size\": 512\n",
      "  },\n",
      "  \"mask\": {\n",
      "    \"target_aspect_ratio\": [\n",
      "      0.75,\n",
      "      1.5\n",
      "    ],\n",
      "    \"target_scale\": [\n",
      "      0.15,\n",
      "      0.2\n",
      "    ],\n",
      "    \"context_aspect_ratio\": 1.0,\n",
      "    \"context_scale\": [\n",
      "      0.85,\n",
      "      1.0\n",
      "    ],\n",
      "    \"num_target_blocks\": 4\n",
      "  },\n",
      "  \"model\": {\n",
      "    \"img_size\": 224,\n",
      "    \"patch_size\": 16,\n",
      "    \"in_chans\": 3,\n",
      "    \"embed_dim\": 768,\n",
      "    \"enc_depth\": 6,\n",
      "    \"pred_depth\": 4,\n",
      "    \"num_heads\": 12,\n",
      "    \"post_emb_norm\": false,\n",
      "    \"layer_dropout\": 0.0,\n",
      "    \"classification_backbone\": \"vit_b_16\",\n",
      "    \"classification_num_classes\": 100,\n",
      "    \"classification_pretrained\": true\n",
      "  },\n",
      "  \"optimizer\": {\n",
      "    \"epochs\": 5,\n",
      "    \"warmup_epochs\": 1,\n",
      "    \"base_learning_rate\": 0.001,\n",
      "    \"start_learning_rate\": 0.0001,\n",
      "    \"final_learning_rate\": 1e-05,\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"final_weight_decay\": 0.2,\n",
      "    \"betas\": [\n",
      "      0.9,\n",
      "      0.95\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"grad_clip_norm\": 1.0,\n",
      "    \"momentum_teacher\": 0.996,\n",
      "    \"momentum_teacher_final\": 1.0\n",
      "  },\n",
      "  \"hardware\": {\n",
      "    \"seed\": 42,\n",
      "    \"world_size\": null,\n",
      "    \"mixed_precision\": true,\n",
      "    \"compile_model\": false,\n",
      "    \"log_every\": 50,\n",
      "    \"checkpoint_every\": 1,\n",
      "    \"output_dir\": \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/experiments/outputs/ijepa\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "from wejepa.backbones import adapt_config_for_backbone, available_backbones\n",
    "from wejepa import IJEPA_base, default_config\n",
    "\n",
    "for backbone in available_backbones():\n",
    "    cfg = adapt_config_for_backbone(default_config(), backbone)\n",
    "    print(f\"\\nBackbone: {backbone}\")\n",
    "    print(f\"Image size: {cfg.model.img_size} | Patch size: {cfg.model.patch_size}\")\n",
    "\n",
    "    model = IJEPA_base(\n",
    "        img_size=cfg.model.img_size,\n",
    "        patch_size=cfg.model.patch_size,\n",
    "        in_chans=cfg.model.in_chans,\n",
    "        embed_dim=cfg.model.embed_dim,\n",
    "        enc_depth=cfg.model.enc_depth,\n",
    "        pred_depth=cfg.model.pred_depth,\n",
    "        num_heads=cfg.model.num_heads,\n",
    "        backbone=cfg.model.classification_backbone,\n",
    "        pretrained=cfg.model.classification_pretrained,\n",
    "    )\n",
    "\n",
    "    print(f\"Trainable params: {model.count_trainable_parameters() / 1e6:.2f}M\")\n",
    "    print(f\"Student + Predictor params: {model.count_parameters() / 1e6:.2f}M\")\n",
    "\n",
    "    dummy = torch.randn(1, cfg.model.in_chans, cfg.model.img_size, cfg.model.img_size)\n",
    "    preds, targets = model(dummy)\n",
    "    print(f\"Pred shape: {tuple(preds.shape)} | Target shape: {tuple(targets.shape)}\")\n",
    "    print(json.dumps(cfg.to_dict(), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wejepa.backbones import available_backbones\n",
    "from wejepa.config import IJepaConfig\n",
    "from wejepa import default_config, launch_pretraining, IJEPA_base\n",
    "from pathlib import Path\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "print(\"Registered backbones: \")\n",
    "for backbone in available_backbones():\n",
    "    print(f\"- {backbone}\")\n",
    "\n",
    "candidates = [\"vit_b_16\", \"swin_t\", \"convnext_tiny\"]\n",
    "for backbone in candidates:\n",
    "    print(f\"\\nPretraining with backbone: {backbone}\")\n",
    "\n",
    "    with open(\"hf224_config.json\", \"r\") as f:\n",
    "        cfg_dict = json.load(f)\n",
    "    cfg = IJepaConfig.from_dict(cfg_dict)\n",
    "\n",
    "    cfg.model.classification_backbone = backbone\n",
    "    cfg.model.classification_pretrained = True\n",
    "    cfg.hardware.output_dir = f\"./outputs/ijepa/{backbone}\"\n",
    "    cfg_path = Path(f\"configs/pretrain_{backbone}.json\")\n",
    "    cfg_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cfg_path.write_text(json.dumps(cfg.to_dict(), indent=2))\n",
    "    print(f\"Saved config for {backbone} at {cfg_path}\")\n",
    "\n",
    "    launch_pretraining(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74790eb",
   "metadata": {},
   "source": [
    "### 6. Visualizing Backbone Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "from wejepa.backbones import resolve_preprocess_transforms\n",
    "\n",
    "class HFImageDataset(Dataset):\n",
    "    def __init__(self, backbone_name: str, split: str = \"train[:256]\", label_field: str = \"label\"):\n",
    "        self.dataset = load_dataset(\"./data/tsbpp___fall2025_deeplearning\", split=split)\n",
    "        self.transform = resolve_preprocess_transforms(backbone_name)\n",
    "        # Some datasets may not provide labels; fall back to a single class for visualization.\n",
    "        self.label_field = label_field if label_field in self.dataset.features else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        image = self.transform(sample[\"image\"])\n",
    "        label = sample[self.label_field] if self.label_field else 0\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def build_dataloader(backbone_name: str, batch_size: int = 24, split: str = \"train[:192]\") -> DataLoader:\n",
    "    dataset = HFImageDataset(backbone_name=backbone_name, split=split)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59791046",
   "metadata": {},
   "source": [
    "### 7. Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7406f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wejepa.analysis.visualization import (\n",
    "    extract_backbone_features,\n",
    "    plot_tsne_embeddings,\n",
    "    run_tsne_projection,\n",
    ")\n",
    "from wejepa.backbones import build_backbone\n",
    "\n",
    "backbone_names = [\"vit_b_16\", \"swin_t\", \"convnext_tiny\"]\n",
    "tsne_results = {}\n",
    "\n",
    "for backbone_name in backbone_names:\n",
    "    print(f\"Projecting embeddings for {backbone_name} ...\")\n",
    "    backbone, feature_dim = build_backbone(backbone_name, pretrained=True, freeze_backbone=True)\n",
    "\n",
    "    # Use a small slice of the dataset to keep visualization quick.\n",
    "    dataloader = build_dataloader(backbone_name, batch_size=24, split=\"train[:192]\")\n",
    "    features, labels = extract_backbone_features(backbone, dataloader, max_batches=4)\n",
    "\n",
    "    embedding = run_tsne_projection(features, perplexity=20.0, random_state=42)\n",
    "    fig = plot_tsne_embeddings(embedding, labels)\n",
    "    fig.suptitle(f\"{backbone_name} TSNE\", y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    tsne_results[backbone_name] = embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ijepa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
