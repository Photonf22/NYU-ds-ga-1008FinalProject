{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc761ad1",
   "metadata": {},
   "source": [
    "# Example based on our README.md\n",
    "1. Dataset download\n",
    "2. Prepare dataset\n",
    "3. Pre-training\n",
    "4. Fine-tuning\n",
    "5. Different backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de0bef",
   "metadata": {},
   "source": [
    "\n",
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e8ab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Preparing download for dataset=tsbpp/fall2025_deeplearning splits=('train',) root=/home/lquang/Code/wejepa/experiments/data\n",
      "[DEBUG] Loading HuggingFace dataset 'tsbpp/fall2025_deeplearning' split='train' cache_dir=/home/lquang/Code/wejepa/experiments/data\n",
      "Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n",
      "Downloading (incomplete total...): 2.46kB [00:00, 14.2kB/s] 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 6 files:  17%|████▌                      | 1/6 [00:00<00:00,  6.13it/s]\u001b[A\n",
      "Fetching 6 files: 100%|███████████████████████████| 6/6 [00:03<00:00,  1.76it/s]\u001b[A\n",
      "Download complete: : 2.46kB [00:03, 14.2kB/s]              [DEBUG] Using snapshot_download for dataset 'tsbpp/fall2025_deeplearning' split='train'\n",
      "[DEBUG] Downloaded dataset to /home/lquang/Code/wejepa/experiments/data/tsbpp_fall2025_deeplearning, extracting archives if any.\n",
      "\n",
      "Download complete: : 2.46kB [00:03, 686B/s]               | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Extracted /home/lquang/Code/wejepa/experiments/data/tsbpp_fall2025_deeplearning/cc3m_96px_part4.zip\n",
      "\n",
      "Extracting ZIP files:  20%|████▌                  | 1/5 [00:08<00:35,  8.94s/it]\u001b[AExtracted /home/lquang/Code/wejepa/experiments/data/tsbpp_fall2025_deeplearning/cc3m_96px_part5.zip\n",
      "\n",
      "Extracting ZIP files:  40%|█████████▏             | 2/5 [00:17<00:26,  8.75s/it]\u001b[AExtracted /home/lquang/Code/wejepa/experiments/data/tsbpp_fall2025_deeplearning/cc3m_96px_part2.zip\n",
      "\n",
      "Extracting ZIP files:  60%|█████████████▊         | 3/5 [00:26<00:17,  8.78s/it]\u001b[AExtracted /home/lquang/Code/wejepa/experiments/data/tsbpp_fall2025_deeplearning/cc3m_96px_part1.zip\n",
      "\n",
      "Extracting ZIP files:  80%|██████████████████▍    | 4/5 [00:35<00:08,  8.84s/it]\u001b[AExtracted /home/lquang/Code/wejepa/experiments/data/tsbpp_fall2025_deeplearning/cc3m_96px_part3.zip\n",
      "\n",
      "Extracting ZIP files: 100%|███████████████████████| 5/5 [00:44<00:00,  8.83s/it]\u001b[A\n",
      "Extracting TAR files: 0it [00:00, ?it/s]\n",
      "Split 'train' available under /home/lquang/Code/wejepa/experiments/data/tsbpp_fall2025_deeplearning\n"
     ]
    }
   ],
   "source": [
    "# using the cli\n",
    "\n",
    "# download class dataset\n",
    "#!python -m wejepa.datasets.download --dataset-root ./data --dataset-name tsbpp/fall2025_deeplearning --splits train\n",
    "\n",
    "# for development, download a small subset\n",
    "#!python -m wejepa.datasets.download --dataset-root ./data --dataset-name tsbpp/fall2025_deeplearning --splits 'train[:10]'\n",
    "\n",
    "# download the class pretrain dataset raw data\n",
    "!python -m wejepa.datasets.download --dataset-root ./data --dataset-name tsbpp/fall2025_deeplearning --snapshot-download --splits train --debug\n",
    "\n",
    "# download cifar100 dataset\n",
    "#!python -m wejepa.datasets.download --dataset-root ./data --dataset-name cifar100\n",
    "\n",
    "# download cub200 dataset\n",
    "#!python -m wejepa.datasets.download --dataset-root ./data --dataset-name cub200 --splits train,test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793c8d98",
   "metadata": {},
   "source": [
    "### Inspect and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2987b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare raw images into huggingface dataset format (not needed with the current dataset structure)\n",
    "# !python -m wejepa.datasets.prepare_images --input-dir ./data/tsbpp_fall2025_deeplearning --output-dir ./data/tsbpp___fall2025_deeplearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21635921",
   "metadata": {},
   "source": [
    "### Pre-training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e90f233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "Model has 4,449,408 trainable parameters.\n",
      "Loading imagefolder dataset from directory: ./data/tsbpp_fall2025_deeplearning\n",
      "Loading imagefolder dataset from directory: ./data/tsbpp_fall2025_deeplearning\n",
      "Loading imagefolder dataset from directory: ./data/tsbpp_fall2025_deeplearning\n",
      "Loading imagefolder dataset from directory: ./data/tsbpp_fall2025_deeplearning\n",
      "Resolving data files: 100%|█████████| 500000/500000 [00:01<00:00, 399303.05it/s]\n",
      "Resolving data files: 100%|█████████| 500000/500000 [00:02<00:00, 211517.01it/s]\n",
      "Resolving data files: 100%|█████████| 500000/500000 [00:01<00:00, 491503.60it/s]\n",
      "Resolving data files: 100%|█████████| 500000/500000 [00:00<00:00, 518048.88it/s]\n",
      "\n",
      "Downloading data:   0%|                           | 0/500000 [00:00<?, ?files/s]\u001b[A\n",
      "Downloading data:   7%|▊          | 36532/500000 [00:00<00:01, 362904.62files/s]\u001b[A\n",
      "Downloading data:  15%|█▌         | 72823/500000 [00:00<00:02, 209750.39files/s]\u001b[A\n",
      "Downloading data:  25%|██▍       | 123456/500000 [00:00<00:01, 306522.00files/s]\u001b[A\n",
      "Downloading data:  35%|███▍      | 173556/500000 [00:00<00:00, 368346.04files/s]\u001b[A\n",
      "Downloading data:  46%|████▌     | 227621/500000 [00:00<00:00, 422155.39files/s]\u001b[A\n",
      "Downloading data:  55%|█████▍    | 273850/500000 [00:00<00:00, 432014.52files/s]\u001b[A\n",
      "Downloading data:  65%|██████▍   | 324954/500000 [00:00<00:00, 456086.27files/s]\u001b[A\n",
      "Downloading data:  75%|███████▍  | 374293/500000 [00:00<00:00, 467390.31files/s]\u001b[A\n",
      "Downloading data:  85%|████████▌ | 426445/500000 [00:01<00:00, 483743.86files/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 500000/500000 [00:01<00:00, 427440.99files/s]\u001b[A\n",
      "Generating train split: 500000 examples [00:13, 37893.76 examples/s]\n",
      "/home/lquang/Code/wejepa/wejepa/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "Training:   0%|                                        | 0/1953 [00:00<?, ?it/s]<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "Training:   3%|▊                              | 49/1953 [01:14<06:54,  4.59it/s]Epoch 1 Iter 50/1953 | Loss 0.4288 | 171.3 img/s\n",
      "Training:   5%|█▌                             | 99/1953 [01:25<06:14,  4.95it/s]Epoch 1 Iter 100/1953 | Loss 0.4145 | 300.2 img/s\n",
      "Training:   8%|██▎                           | 149/1953 [01:35<06:19,  4.76it/s]Epoch 1 Iter 150/1953 | Loss 0.4215 | 399.5 img/s\n",
      "Training:  10%|███                           | 199/1953 [01:46<06:42,  4.36it/s]Epoch 1 Iter 200/1953 | Loss 0.4278 | 478.1 img/s\n",
      "Training:  13%|███▊                          | 249/1953 [01:57<06:11,  4.59it/s]Epoch 1 Iter 250/1953 | Loss 0.4323 | 542.7 img/s\n",
      "Training:  15%|████▌                         | 299/1953 [02:08<06:03,  4.56it/s]Epoch 1 Iter 300/1953 | Loss 0.4314 | 595.5 img/s\n",
      "Training:  18%|█████▎                        | 349/1953 [02:19<06:25,  4.16it/s]Epoch 1 Iter 350/1953 | Loss 0.4260 | 639.4 img/s\n",
      "Training:  20%|██████▏                       | 399/1953 [02:30<05:50,  4.43it/s]Epoch 1 Iter 400/1953 | Loss 0.4194 | 677.4 img/s\n",
      "Training:  23%|██████▉                       | 449/1953 [02:42<05:45,  4.35it/s]Epoch 1 Iter 450/1953 | Loss 0.4132 | 709.0 img/s\n",
      "Training:  26%|███████▋                      | 499/1953 [02:53<05:32,  4.37it/s]Epoch 1 Iter 500/1953 | Loss 0.4069 | 736.2 img/s\n",
      "Training:  28%|████████▍                     | 549/1953 [03:04<04:50,  4.83it/s]Epoch 1 Iter 550/1953 | Loss 0.4025 | 760.9 img/s\n",
      "Training:  31%|█████████▏                    | 599/1953 [03:15<05:05,  4.42it/s]Epoch 1 Iter 600/1953 | Loss 0.3991 | 783.4 img/s\n",
      "Training:  33%|█████████▉                    | 649/1953 [03:26<04:41,  4.63it/s]Epoch 1 Iter 650/1953 | Loss 0.3962 | 802.9 img/s\n",
      "Training:  36%|██████████▋                   | 699/1953 [03:38<05:01,  4.17it/s]Epoch 1 Iter 700/1953 | Loss 0.3941 | 819.4 img/s\n",
      "Training:  38%|███████████▌                  | 749/1953 [03:49<04:29,  4.46it/s]Epoch 1 Iter 750/1953 | Loss 0.3931 | 834.7 img/s\n",
      "Training:  41%|████████████▎                 | 799/1953 [04:01<04:34,  4.21it/s]Epoch 1 Iter 800/1953 | Loss 0.3926 | 847.6 img/s\n",
      "Training:  43%|█████████████                 | 849/1953 [04:12<04:26,  4.15it/s]Epoch 1 Iter 850/1953 | Loss 0.3929 | 860.4 img/s\n",
      "Training:  46%|█████████████▊                | 899/1953 [04:24<04:06,  4.27it/s]Epoch 1 Iter 900/1953 | Loss 0.3936 | 871.1 img/s\n",
      "Training:  49%|██████████████▌               | 949/1953 [04:35<03:49,  4.38it/s]Epoch 1 Iter 950/1953 | Loss 0.3938 | 882.5 img/s\n",
      "Training:  51%|███████████████▎              | 999/1953 [04:46<03:45,  4.24it/s]Epoch 1 Iter 1000/1953 | Loss 0.3954 | 891.6 img/s\n",
      "Training:  54%|███████████████▌             | 1049/1953 [04:58<03:30,  4.30it/s]Epoch 1 Iter 1050/1953 | Loss 0.3947 | 900.1 img/s\n",
      "Training:  56%|████████████████▎            | 1099/1953 [05:09<03:14,  4.39it/s]Epoch 1 Iter 1100/1953 | Loss 0.3942 | 908.2 img/s\n",
      "Training:  59%|█████████████████            | 1149/1953 [05:20<02:53,  4.64it/s]Epoch 1 Iter 1150/1953 | Loss 0.3934 | 916.6 img/s\n",
      "Training:  61%|█████████████████▊           | 1199/1953 [05:32<03:01,  4.16it/s]Epoch 1 Iter 1200/1953 | Loss 0.3931 | 923.9 img/s\n",
      "Training:  64%|██████████████████▌          | 1249/1953 [05:43<02:35,  4.52it/s]Epoch 1 Iter 1250/1953 | Loss 0.3928 | 930.8 img/s\n",
      "Training:  67%|███████████████████▎         | 1299/1953 [05:54<02:28,  4.40it/s]Epoch 1 Iter 1300/1953 | Loss 0.3919 | 937.2 img/s\n",
      "Training:  69%|████████████████████         | 1349/1953 [06:06<02:15,  4.45it/s]Epoch 1 Iter 1350/1953 | Loss 0.3914 | 943.7 img/s\n",
      "Training:  72%|████████████████████▊        | 1399/1953 [06:17<02:03,  4.48it/s]Epoch 1 Iter 1400/1953 | Loss 0.3913 | 948.6 img/s\n",
      "Training:  74%|█████████████████████▌       | 1449/1953 [06:28<01:52,  4.50it/s]Epoch 1 Iter 1450/1953 | Loss 0.3914 | 953.8 img/s\n",
      "Training:  77%|██████████████████████▎      | 1499/1953 [06:40<01:47,  4.20it/s]Epoch 1 Iter 1500/1953 | Loss 0.3907 | 958.8 img/s\n",
      "Training:  79%|███████████████████████      | 1549/1953 [06:51<01:26,  4.65it/s]Epoch 1 Iter 1550/1953 | Loss 0.3903 | 963.6 img/s\n",
      "Training:  82%|███████████████████████▋     | 1599/1953 [07:02<01:18,  4.53it/s]Epoch 1 Iter 1600/1953 | Loss 0.3896 | 968.0 img/s\n",
      "Training:  84%|████████████████████████▍    | 1649/1953 [07:14<01:12,  4.21it/s]Epoch 1 Iter 1650/1953 | Loss 0.3886 | 972.2 img/s\n",
      "Training:  87%|█████████████████████████▏   | 1699/1953 [07:25<00:54,  4.70it/s]Epoch 1 Iter 1700/1953 | Loss 0.3880 | 976.2 img/s\n",
      "Training:  90%|█████████████████████████▉   | 1749/1953 [07:36<00:45,  4.48it/s]Epoch 1 Iter 1750/1953 | Loss 0.3877 | 979.9 img/s\n",
      "Training:  92%|██████████████████████████▋  | 1799/1953 [07:48<00:35,  4.30it/s]Epoch 1 Iter 1800/1953 | Loss 0.3870 | 983.2 img/s\n",
      "Training:  95%|███████████████████████████▍ | 1849/1953 [07:59<00:24,  4.31it/s]Epoch 1 Iter 1850/1953 | Loss 0.3863 | 986.7 img/s\n",
      "Training:  97%|████████████████████████████▏| 1899/1953 [08:10<00:12,  4.46it/s]Epoch 1 Iter 1900/1953 | Loss 0.3855 | 990.2 img/s\n",
      "Training: 100%|████████████████████████████▉| 1949/1953 [08:22<00:00,  4.48it/s]Epoch 1 Iter 1950/1953 | Loss 0.3848 | 993.7 img/s\n",
      "Training: 100%|█████████████████████████████| 1953/1953 [08:23<00:00,  3.88it/s]\n",
      "\n",
      "\n",
      "Training: 100%|█████████████████████████████| 1953/1953 [08:23<00:00,  3.88it/s]\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Training:   0%|                                        | 0/1953 [00:00<?, ?it/s]Saved checkpoint to outputs/ijepa/ijepa_epoch_0001.pt\n",
      "Epoch 1/5 | loss=0.3847\n",
      "Training:   3%|▊                              | 49/1953 [00:12<07:10,  4.42it/s]Epoch 2 Iter 50/1953 | Loss 0.3443 | 1052.9 img/s\n",
      "Training:   5%|█▌                             | 99/1953 [00:23<06:35,  4.69it/s]Epoch 2 Iter 100/1953 | Loss 0.3425 | 1085.0 img/s\n",
      "Training:   8%|██▎                           | 149/1953 [00:35<07:05,  4.24it/s]Epoch 2 Iter 150/1953 | Loss 0.3383 | 1093.4 img/s\n",
      "Training:  10%|███                           | 199/1953 [00:46<06:34,  4.44it/s]Epoch 2 Iter 200/1953 | Loss 0.3383 | 1104.5 img/s\n",
      "Training:  13%|███▊                          | 249/1953 [00:57<06:21,  4.47it/s]Epoch 2 Iter 250/1953 | Loss 0.3359 | 1107.7 img/s\n",
      "Training:  15%|████▌                         | 299/1953 [01:09<06:09,  4.47it/s]Epoch 2 Iter 300/1953 | Loss 0.3336 | 1112.2 img/s\n",
      "Training:  18%|█████▎                        | 349/1953 [01:20<06:12,  4.31it/s]Epoch 2 Iter 350/1953 | Loss 0.3308 | 1113.1 img/s\n",
      "Training:  20%|██████▏                       | 399/1953 [01:31<06:09,  4.21it/s]Epoch 2 Iter 400/1953 | Loss 0.3297 | 1113.6 img/s\n",
      "Training:  23%|██████▉                       | 449/1953 [01:43<05:45,  4.35it/s]Epoch 2 Iter 450/1953 | Loss 0.3270 | 1114.3 img/s\n",
      "Training:  26%|███████▋                      | 499/1953 [01:54<05:17,  4.57it/s]Epoch 2 Iter 500/1953 | Loss 0.3246 | 1116.2 img/s\n",
      "Training:  28%|████████▍                     | 549/1953 [02:06<05:29,  4.26it/s]Epoch 2 Iter 550/1953 | Loss 0.3215 | 1114.7 img/s\n",
      "Training:  31%|█████████▏                    | 599/1953 [02:17<05:10,  4.36it/s]Epoch 2 Iter 600/1953 | Loss 0.3186 | 1115.8 img/s\n",
      "Training:  33%|█████████▉                    | 649/1953 [02:28<04:55,  4.41it/s]Epoch 2 Iter 650/1953 | Loss 0.3162 | 1116.4 img/s\n",
      "Training:  36%|██████████▋                   | 699/1953 [02:40<04:38,  4.50it/s]Epoch 2 Iter 700/1953 | Loss 0.3135 | 1119.1 img/s\n",
      "Training:  38%|███████████▌                  | 749/1953 [02:51<04:21,  4.60it/s]Epoch 2 Iter 750/1953 | Loss 0.3117 | 1120.6 img/s\n",
      "Training:  41%|████████████▎                 | 799/1953 [03:02<04:17,  4.49it/s]Epoch 2 Iter 800/1953 | Loss 0.3095 | 1121.0 img/s\n",
      "Training:  43%|█████████████                 | 849/1953 [03:14<04:08,  4.44it/s]Epoch 2 Iter 850/1953 | Loss 0.3068 | 1121.1 img/s\n",
      "Training:  46%|█████████████▊                | 899/1953 [03:25<04:04,  4.31it/s]Epoch 2 Iter 900/1953 | Loss 0.3047 | 1121.8 img/s\n",
      "Training:  49%|██████████████▌               | 949/1953 [03:36<03:52,  4.31it/s]Epoch 2 Iter 950/1953 | Loss 0.3031 | 1122.0 img/s\n",
      "Training:  51%|███████████████▎              | 999/1953 [03:48<03:36,  4.41it/s]Epoch 2 Iter 1000/1953 | Loss 0.3011 | 1122.0 img/s\n",
      "Training:  54%|███████████████▌             | 1049/1953 [03:59<03:22,  4.45it/s]Epoch 2 Iter 1050/1953 | Loss 0.2989 | 1122.1 img/s\n",
      "Training:  56%|████████████████▎            | 1099/1953 [04:10<02:59,  4.76it/s]Epoch 2 Iter 1100/1953 | Loss 0.2971 | 1122.6 img/s\n",
      "Training:  59%|█████████████████            | 1149/1953 [04:21<02:56,  4.54it/s]Epoch 2 Iter 1150/1953 | Loss 0.2950 | 1123.4 img/s\n",
      "Training:  61%|█████████████████▊           | 1199/1953 [04:33<02:53,  4.34it/s]Epoch 2 Iter 1200/1953 | Loss 0.2936 | 1123.6 img/s\n",
      "Training:  64%|██████████████████▌          | 1249/1953 [04:44<02:27,  4.78it/s]Epoch 2 Iter 1250/1953 | Loss 0.2920 | 1124.3 img/s\n",
      "Training:  67%|███████████████████▎         | 1299/1953 [04:55<02:29,  4.39it/s]Epoch 2 Iter 1300/1953 | Loss 0.2906 | 1124.4 img/s\n",
      "Training:  69%|████████████████████         | 1349/1953 [05:06<02:10,  4.64it/s]Epoch 2 Iter 1350/1953 | Loss 0.2893 | 1125.6 img/s\n",
      "Training:  72%|████████████████████▊        | 1399/1953 [05:18<02:06,  4.37it/s]Epoch 2 Iter 1400/1953 | Loss 0.2877 | 1125.7 img/s\n",
      "Training:  74%|█████████████████████▌       | 1449/1953 [05:29<01:55,  4.36it/s]Epoch 2 Iter 1450/1953 | Loss 0.2863 | 1125.4 img/s\n",
      "Training:  77%|██████████████████████▎      | 1499/1953 [05:41<01:43,  4.39it/s]Epoch 2 Iter 1500/1953 | Loss 0.2850 | 1124.8 img/s\n",
      "Training:  79%|███████████████████████      | 1549/1953 [05:52<01:33,  4.31it/s]Epoch 2 Iter 1550/1953 | Loss 0.2838 | 1124.5 img/s\n",
      "Training:  82%|███████████████████████▋     | 1599/1953 [06:04<01:23,  4.26it/s]Epoch 2 Iter 1600/1953 | Loss 0.2825 | 1124.9 img/s\n",
      "Training:  84%|████████████████████████▍    | 1649/1953 [06:15<01:09,  4.40it/s]Epoch 2 Iter 1650/1953 | Loss 0.2815 | 1124.8 img/s\n",
      "Training:  87%|█████████████████████████▏   | 1699/1953 [06:26<00:56,  4.48it/s]Epoch 2 Iter 1700/1953 | Loss 0.2799 | 1124.7 img/s\n",
      "Training:  90%|█████████████████████████▉   | 1749/1953 [06:38<00:46,  4.38it/s]Epoch 2 Iter 1750/1953 | Loss 0.2789 | 1125.3 img/s\n",
      "Training:  92%|██████████████████████████▋  | 1799/1953 [06:49<00:35,  4.36it/s]Epoch 2 Iter 1800/1953 | Loss 0.2777 | 1125.1 img/s\n",
      "Training:  95%|███████████████████████████▍ | 1849/1953 [07:00<00:23,  4.39it/s]Epoch 2 Iter 1850/1953 | Loss 0.2767 | 1125.6 img/s\n",
      "Training:  97%|████████████████████████████▏| 1899/1953 [07:11<00:12,  4.38it/s]Epoch 2 Iter 1900/1953 | Loss 0.2757 | 1126.1 img/s\n",
      "Training: 100%|████████████████████████████▉| 1949/1953 [07:23<00:00,  4.39it/s]Epoch 2 Iter 1950/1953 | Loss 0.2749 | 1125.7 img/s\n",
      "Training: 100%|█████████████████████████████| 1953/1953 [07:24<00:00,  4.40it/s]\n",
      "\n",
      "Training: 100%|█████████████████████████████| 1953/1953 [07:24<00:00,  4.40it/s]\n",
      "\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Training:   0%|                                        | 0/1953 [00:00<?, ?it/s]Saved checkpoint to outputs/ijepa/ijepa_epoch_0002.pt\n",
      "Epoch 2/5 | loss=0.2748\n",
      "Training:   3%|▊                              | 49/1953 [00:12<07:08,  4.44it/s]Epoch 3 Iter 50/1953 | Loss 0.2317 | 1039.2 img/s\n",
      "Training:   5%|█▌                             | 99/1953 [00:23<06:57,  4.44it/s]Epoch 3 Iter 100/1953 | Loss 0.2335 | 1082.6 img/s\n",
      "Training:   8%|██▎                           | 149/1953 [00:35<06:48,  4.41it/s]Epoch 3 Iter 150/1953 | Loss 0.2328 | 1092.4 img/s\n",
      "Training:  10%|███                           | 199/1953 [00:46<06:48,  4.30it/s]Epoch 3 Iter 200/1953 | Loss 0.2332 | 1100.4 img/s\n",
      "Training:  13%|███▊                          | 249/1953 [00:57<06:20,  4.48it/s]Epoch 3 Iter 250/1953 | Loss 0.2335 | 1103.4 img/s\n",
      "Training:  15%|████▌                         | 299/1953 [01:09<06:21,  4.34it/s]Epoch 3 Iter 300/1953 | Loss 0.2334 | 1108.4 img/s\n",
      "Training:  18%|█████▎                        | 349/1953 [01:20<06:01,  4.44it/s]Epoch 3 Iter 350/1953 | Loss 0.2327 | 1110.3 img/s\n",
      "Training:  20%|██████▏                       | 399/1953 [01:32<05:50,  4.43it/s]Epoch 3 Iter 400/1953 | Loss 0.2310 | 1110.4 img/s\n",
      "Training:  23%|██████▉                       | 449/1953 [01:43<05:36,  4.47it/s]Epoch 3 Iter 450/1953 | Loss 0.2311 | 1113.7 img/s\n",
      "Training:  26%|███████▋                      | 499/1953 [01:54<05:24,  4.48it/s]Epoch 3 Iter 500/1953 | Loss 0.2317 | 1114.7 img/s\n",
      "Training:  28%|████████▍                     | 549/1953 [02:06<05:29,  4.26it/s]Epoch 3 Iter 550/1953 | Loss 0.2308 | 1113.9 img/s\n",
      "Training:  31%|█████████▏                    | 599/1953 [02:17<04:59,  4.52it/s]Epoch 3 Iter 600/1953 | Loss 0.2299 | 1116.5 img/s\n",
      "Training:  33%|█████████▉                    | 649/1953 [02:28<04:49,  4.50it/s]Epoch 3 Iter 650/1953 | Loss 0.2289 | 1117.0 img/s\n",
      "Training:  36%|██████████▋                   | 699/1953 [02:40<04:29,  4.65it/s]Epoch 3 Iter 700/1953 | Loss 0.2277 | 1118.9 img/s\n",
      "Training:  38%|███████████▌                  | 749/1953 [02:51<04:25,  4.53it/s]Epoch 3 Iter 750/1953 | Loss 0.2269 | 1118.6 img/s\n",
      "Training:  41%|████████████▎                 | 799/1953 [03:02<04:09,  4.63it/s]Epoch 3 Iter 800/1953 | Loss 0.2260 | 1120.3 img/s\n",
      "Training:  43%|█████████████                 | 849/1953 [03:14<04:21,  4.22it/s]Epoch 3 Iter 850/1953 | Loss 0.2253 | 1120.4 img/s\n",
      "Training:  46%|█████████████▊                | 899/1953 [03:25<03:50,  4.57it/s]Epoch 3 Iter 900/1953 | Loss 0.2243 | 1120.2 img/s\n",
      "Training:  49%|██████████████▌               | 949/1953 [03:37<03:54,  4.29it/s]Epoch 3 Iter 950/1953 | Loss 0.2241 | 1119.5 img/s\n",
      "Training:  51%|███████████████▎              | 999/1953 [03:48<03:37,  4.39it/s]Epoch 3 Iter 1000/1953 | Loss 0.2236 | 1120.1 img/s\n",
      "Training:  54%|███████████████▌             | 1049/1953 [03:59<03:29,  4.31it/s]Epoch 3 Iter 1050/1953 | Loss 0.2229 | 1120.6 img/s\n",
      "Training:  56%|████████████████▎            | 1099/1953 [04:11<03:16,  4.35it/s]Epoch 3 Iter 1100/1953 | Loss 0.2218 | 1120.6 img/s\n",
      "Training:  59%|█████████████████            | 1149/1953 [04:22<03:10,  4.22it/s]Epoch 3 Iter 1150/1953 | Loss 0.2211 | 1120.4 img/s\n",
      "Training:  61%|█████████████████▊           | 1199/1953 [04:33<02:57,  4.24it/s]Epoch 3 Iter 1200/1953 | Loss 0.2203 | 1120.9 img/s\n",
      "Training:  64%|██████████████████▌          | 1249/1953 [04:45<02:37,  4.46it/s]Epoch 3 Iter 1250/1953 | Loss 0.2196 | 1120.3 img/s\n",
      "Training:  67%|███████████████████▎         | 1299/1953 [04:56<02:23,  4.55it/s]Epoch 3 Iter 1300/1953 | Loss 0.2191 | 1120.5 img/s\n",
      "Training:  69%|████████████████████         | 1349/1953 [05:08<02:23,  4.20it/s]Epoch 3 Iter 1350/1953 | Loss 0.2183 | 1120.3 img/s\n",
      "Training:  72%|████████████████████▊        | 1399/1953 [05:19<02:01,  4.58it/s]Epoch 3 Iter 1400/1953 | Loss 0.2177 | 1120.4 img/s\n",
      "Training:  74%|█████████████████████▌       | 1449/1953 [05:31<01:55,  4.35it/s]Epoch 3 Iter 1450/1953 | Loss 0.2171 | 1120.1 img/s\n",
      "Training:  77%|██████████████████████▎      | 1499/1953 [05:42<01:51,  4.07it/s]Epoch 3 Iter 1500/1953 | Loss 0.2165 | 1120.3 img/s\n",
      "Training:  79%|███████████████████████      | 1549/1953 [05:53<01:26,  4.69it/s]Epoch 3 Iter 1550/1953 | Loss 0.2157 | 1120.3 img/s\n",
      "Training:  82%|███████████████████████▋     | 1599/1953 [06:05<01:20,  4.39it/s]Epoch 3 Iter 1600/1953 | Loss 0.2151 | 1120.6 img/s\n",
      "Training:  84%|████████████████████████▍    | 1649/1953 [06:17<01:06,  4.60it/s]Epoch 3 Iter 1650/1953 | Loss 0.2147 | 1120.3 img/s\n",
      "Training:  87%|█████████████████████████▏   | 1699/1953 [06:28<00:58,  4.31it/s]Epoch 3 Iter 1700/1953 | Loss 0.2142 | 1119.9 img/s\n",
      "Training:  90%|█████████████████████████▉   | 1749/1953 [06:39<00:45,  4.48it/s]Epoch 3 Iter 1750/1953 | Loss 0.2136 | 1120.1 img/s\n",
      "Training:  92%|██████████████████████████▋  | 1799/1953 [06:51<00:37,  4.13it/s]Epoch 3 Iter 1800/1953 | Loss 0.2129 | 1120.2 img/s\n",
      "Training:  95%|███████████████████████████▍ | 1849/1953 [07:02<00:24,  4.26it/s]Epoch 3 Iter 1850/1953 | Loss 0.2123 | 1120.2 img/s\n",
      "Training:  97%|████████████████████████████▏| 1899/1953 [07:13<00:11,  4.52it/s]Epoch 3 Iter 1900/1953 | Loss 0.2117 | 1120.9 img/s\n",
      "Training: 100%|████████████████████████████▉| 1949/1953 [07:25<00:00,  4.26it/s]Epoch 3 Iter 1950/1953 | Loss 0.2111 | 1121.3 img/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 1953/1953 [07:25<00:00,  4.38it/s]\n",
      "\n",
      "Training: 100%|█████████████████████████████| 1953/1953 [07:26<00:00,  4.38it/s]\n",
      "Training: 100%|█████████████████████████████| 1953/1953 [07:26<00:00,  4.38it/s]\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Training:   0%|                                        | 0/1953 [00:00<?, ?it/s]Saved checkpoint to outputs/ijepa/ijepa_epoch_0003.pt\n",
      "Epoch 3/5 | loss=0.2111\n",
      "Training:   3%|▊                              | 49/1953 [00:12<07:19,  4.33it/s]Epoch 4 Iter 50/1953 | Loss 0.1876 | 1039.2 img/s\n",
      "Training:   5%|█▌                             | 99/1953 [00:23<06:50,  4.51it/s]Epoch 4 Iter 100/1953 | Loss 0.1854 | 1086.9 img/s\n",
      "Training:   8%|██▎                           | 149/1953 [00:34<07:11,  4.18it/s]Epoch 4 Iter 150/1953 | Loss 0.1855 | 1099.4 img/s\n",
      "Training:  10%|███                           | 199/1953 [00:46<06:49,  4.28it/s]Epoch 4 Iter 200/1953 | Loss 0.1870 | 1108.0 img/s\n",
      "Training:  13%|███▊                          | 249/1953 [00:57<06:22,  4.46it/s]Epoch 4 Iter 250/1953 | Loss 0.1862 | 1115.0 img/s\n",
      "Training:  15%|████▌                         | 299/1953 [01:08<06:12,  4.44it/s]Epoch 4 Iter 300/1953 | Loss 0.1857 | 1116.3 img/s\n",
      "Training:  18%|█████▎                        | 349/1953 [01:20<06:04,  4.40it/s]Epoch 4 Iter 350/1953 | Loss 0.1855 | 1117.3 img/s\n",
      "Training:  20%|██████▏                       | 399/1953 [01:31<05:44,  4.51it/s]Epoch 4 Iter 400/1953 | Loss 0.1848 | 1120.3 img/s\n",
      "Training:  23%|██████▉                       | 449/1953 [01:42<05:32,  4.53it/s]Epoch 4 Iter 450/1953 | Loss 0.1845 | 1123.1 img/s\n",
      "Training:  26%|███████▋                      | 499/1953 [01:53<05:27,  4.43it/s]Epoch 4 Iter 500/1953 | Loss 0.1836 | 1125.2 img/s\n",
      "Training:  28%|████████▍                     | 549/1953 [02:04<05:04,  4.61it/s]Epoch 4 Iter 550/1953 | Loss 0.1833 | 1125.2 img/s\n",
      "Training:  31%|█████████▏                    | 599/1953 [02:16<04:51,  4.64it/s]Epoch 4 Iter 600/1953 | Loss 0.1821 | 1126.5 img/s\n",
      "Training:  33%|█████████▉                    | 649/1953 [02:27<04:46,  4.55it/s]Epoch 4 Iter 650/1953 | Loss 0.1813 | 1125.2 img/s\n",
      "Training:  36%|██████████▋                   | 699/1953 [02:39<04:53,  4.27it/s]Epoch 4 Iter 700/1953 | Loss 0.1806 | 1125.6 img/s\n",
      "Training:  38%|███████████▌                  | 749/1953 [02:50<04:42,  4.26it/s]Epoch 4 Iter 750/1953 | Loss 0.1800 | 1126.0 img/s\n",
      "Training:  41%|████████████▎                 | 799/1953 [03:01<04:21,  4.41it/s]Epoch 4 Iter 800/1953 | Loss 0.1792 | 1125.4 img/s\n",
      "Training:  43%|█████████████                 | 849/1953 [03:13<03:56,  4.67it/s]Epoch 4 Iter 850/1953 | Loss 0.1784 | 1125.5 img/s\n",
      "Training:  46%|█████████████▊                | 899/1953 [03:24<04:01,  4.36it/s]Epoch 4 Iter 900/1953 | Loss 0.1778 | 1126.9 img/s\n",
      "Training:  49%|██████████████▌               | 949/1953 [03:35<04:00,  4.17it/s]Epoch 4 Iter 950/1953 | Loss 0.1768 | 1126.4 img/s\n",
      "Training:  51%|███████████████▎              | 999/1953 [03:47<03:37,  4.38it/s]Epoch 4 Iter 1000/1953 | Loss 0.1762 | 1127.5 img/s\n",
      "Training:  54%|███████████████▌             | 1049/1953 [03:58<03:25,  4.39it/s]Epoch 4 Iter 1050/1953 | Loss 0.1755 | 1126.9 img/s\n",
      "Training:  56%|████████████████▎            | 1099/1953 [04:09<03:26,  4.13it/s]Epoch 4 Iter 1100/1953 | Loss 0.1747 | 1125.7 img/s\n",
      "Training:  59%|█████████████████            | 1149/1953 [04:21<03:07,  4.29it/s]Epoch 4 Iter 1150/1953 | Loss 0.1739 | 1125.3 img/s\n",
      "Training:  61%|█████████████████▊           | 1199/1953 [04:32<02:52,  4.38it/s]Epoch 4 Iter 1200/1953 | Loss 0.1732 | 1125.3 img/s\n",
      "Training:  64%|██████████████████▌          | 1249/1953 [04:44<02:36,  4.49it/s]Epoch 4 Iter 1250/1953 | Loss 0.1726 | 1126.2 img/s\n",
      "Training:  67%|███████████████████▎         | 1299/1953 [04:55<02:22,  4.59it/s]Epoch 4 Iter 1300/1953 | Loss 0.1719 | 1126.3 img/s\n",
      "Training:  69%|████████████████████         | 1349/1953 [05:06<02:16,  4.41it/s]Epoch 4 Iter 1350/1953 | Loss 0.1714 | 1126.8 img/s\n",
      "Training:  72%|████████████████████▊        | 1399/1953 [05:18<02:09,  4.29it/s]Epoch 4 Iter 1400/1953 | Loss 0.1710 | 1126.3 img/s\n",
      "Training:  74%|█████████████████████▌       | 1449/1953 [05:29<01:55,  4.35it/s]Epoch 4 Iter 1450/1953 | Loss 0.1703 | 1125.9 img/s\n",
      "Training:  77%|██████████████████████▎      | 1499/1953 [05:40<01:46,  4.28it/s]Epoch 4 Iter 1500/1953 | Loss 0.1697 | 1125.5 img/s\n",
      "Training:  79%|███████████████████████      | 1549/1953 [05:52<01:31,  4.44it/s]Epoch 4 Iter 1550/1953 | Loss 0.1692 | 1125.3 img/s\n",
      "Training:  82%|███████████████████████▋     | 1599/1953 [06:03<01:16,  4.61it/s]Epoch 4 Iter 1600/1953 | Loss 0.1684 | 1125.4 img/s\n",
      "Training:  84%|████████████████████████▍    | 1649/1953 [06:15<01:07,  4.47it/s]Epoch 4 Iter 1650/1953 | Loss 0.1679 | 1125.6 img/s\n",
      "Training:  87%|█████████████████████████▏   | 1699/1953 [06:26<00:59,  4.30it/s]Epoch 4 Iter 1700/1953 | Loss 0.1673 | 1125.2 img/s\n",
      "Training:  90%|█████████████████████████▉   | 1749/1953 [06:37<00:46,  4.40it/s]Epoch 4 Iter 1750/1953 | Loss 0.1669 | 1125.3 img/s\n",
      "Training:  92%|██████████████████████████▋  | 1799/1953 [06:49<00:35,  4.37it/s]Epoch 4 Iter 1800/1953 | Loss 0.1663 | 1125.6 img/s\n",
      "Training:  95%|███████████████████████████▍ | 1849/1953 [07:00<00:22,  4.59it/s]Epoch 4 Iter 1850/1953 | Loss 0.1658 | 1125.9 img/s\n",
      "Training:  97%|████████████████████████████▏| 1899/1953 [07:11<00:12,  4.33it/s]Epoch 4 Iter 1900/1953 | Loss 0.1654 | 1126.1 img/s\n",
      "Training: 100%|████████████████████████████▉| 1949/1953 [07:23<00:00,  4.29it/s]Epoch 4 Iter 1950/1953 | Loss 0.1647 | 1126.4 img/s\n",
      "Training: 100%|█████████████████████████████| 1953/1953 [07:24<00:00,  4.40it/s]\n",
      "\n",
      "Training: 100%|█████████████████████████████| 1953/1953 [07:23<00:00,  4.40it/s]\n",
      "\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Training:   0%|                                        | 0/1953 [00:00<?, ?it/s]Saved checkpoint to outputs/ijepa/ijepa_epoch_0004.pt\n",
      "Epoch 4/5 | loss=0.1647\n",
      "Training:   3%|▊                              | 49/1953 [00:12<07:04,  4.48it/s]Epoch 5 Iter 50/1953 | Loss 0.1424 | 1028.3 img/s\n",
      "Training:   5%|█▌                             | 99/1953 [00:23<06:47,  4.55it/s]Epoch 5 Iter 100/1953 | Loss 0.1406 | 1080.7 img/s\n",
      "Training:   8%|██▎                           | 149/1953 [00:34<07:02,  4.27it/s]Epoch 5 Iter 150/1953 | Loss 0.1409 | 1098.0 img/s\n",
      "Training:  10%|███                           | 199/1953 [00:46<06:27,  4.53it/s]Epoch 5 Iter 200/1953 | Loss 0.1417 | 1109.5 img/s\n",
      "Training:  13%|███▊                          | 249/1953 [00:57<06:24,  4.43it/s]Epoch 5 Iter 250/1953 | Loss 0.1410 | 1116.5 img/s\n",
      "Training:  15%|████▌                         | 299/1953 [01:08<06:20,  4.35it/s]Epoch 5 Iter 300/1953 | Loss 0.1408 | 1121.7 img/s\n",
      "Training:  18%|█████▎                        | 349/1953 [01:19<06:00,  4.45it/s]Epoch 5 Iter 350/1953 | Loss 0.1405 | 1121.6 img/s\n",
      "Training:  20%|██████▏                       | 399/1953 [01:31<06:11,  4.18it/s]Epoch 5 Iter 400/1953 | Loss 0.1395 | 1122.0 img/s\n",
      "Training:  23%|██████▉                       | 449/1953 [01:42<05:32,  4.53it/s]Epoch 5 Iter 450/1953 | Loss 0.1395 | 1125.2 img/s\n",
      "Training:  26%|███████▋                      | 499/1953 [01:53<05:28,  4.42it/s]Epoch 5 Iter 500/1953 | Loss 0.1383 | 1124.8 img/s\n",
      "Training:  28%|████████▍                     | 549/1953 [02:04<05:20,  4.38it/s]Epoch 5 Iter 550/1953 | Loss 0.1375 | 1125.3 img/s\n",
      "Training:  31%|█████████▏                    | 599/1953 [02:16<05:10,  4.35it/s]Epoch 5 Iter 600/1953 | Loss 0.1369 | 1126.5 img/s\n",
      "Training:  33%|█████████▉                    | 649/1953 [02:27<05:04,  4.28it/s]Epoch 5 Iter 650/1953 | Loss 0.1364 | 1125.3 img/s\n",
      "Training:  36%|██████████▋                   | 699/1953 [02:39<04:46,  4.38it/s]Epoch 5 Iter 700/1953 | Loss 0.1356 | 1126.1 img/s\n",
      "Training:  38%|███████████▌                  | 749/1953 [02:50<04:33,  4.40it/s]Epoch 5 Iter 750/1953 | Loss 0.1347 | 1125.8 img/s\n",
      "Training:  41%|████████████▎                 | 799/1953 [03:01<04:08,  4.64it/s]Epoch 5 Iter 800/1953 | Loss 0.1341 | 1127.1 img/s\n",
      "Training:  43%|█████████████                 | 849/1953 [03:12<04:01,  4.56it/s]Epoch 5 Iter 850/1953 | Loss 0.1333 | 1128.5 img/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  46%|█████████████▊                | 899/1953 [03:24<04:16,  4.11it/s]Epoch 5 Iter 900/1953 | Loss 0.1326 | 1128.6 img/s\n",
      "Training:  49%|██████████████▌               | 949/1953 [03:35<03:52,  4.32it/s]Epoch 5 Iter 950/1953 | Loss 0.1320 | 1128.2 img/s\n",
      "Training:  51%|███████████████▎              | 999/1953 [03:46<03:18,  4.80it/s]Epoch 5 Iter 1000/1953 | Loss 0.1317 | 1128.3 img/s\n",
      "Training:  54%|███████████████▌             | 1049/1953 [03:57<03:29,  4.31it/s]Epoch 5 Iter 1050/1953 | Loss 0.1312 | 1129.3 img/s\n",
      "Training:  56%|████████████████▎            | 1099/1953 [04:09<03:07,  4.56it/s]Epoch 5 Iter 1100/1953 | Loss 0.1308 | 1129.4 img/s\n",
      "Training:  59%|█████████████████            | 1149/1953 [04:20<03:05,  4.32it/s]Epoch 5 Iter 1150/1953 | Loss 0.1302 | 1129.6 img/s\n",
      "Training:  61%|█████████████████▊           | 1199/1953 [04:31<02:50,  4.42it/s]Epoch 5 Iter 1200/1953 | Loss 0.1296 | 1129.8 img/s\n",
      "Training:  64%|██████████████████▌          | 1249/1953 [04:43<02:38,  4.44it/s]Epoch 5 Iter 1250/1953 | Loss 0.1290 | 1129.8 img/s\n",
      "Training:  67%|███████████████████▎         | 1299/1953 [04:54<02:27,  4.45it/s]Epoch 5 Iter 1300/1953 | Loss 0.1285 | 1129.5 img/s\n",
      "Training:  69%|████████████████████         | 1349/1953 [05:05<02:22,  4.23it/s]Epoch 5 Iter 1350/1953 | Loss 0.1282 | 1129.6 img/s\n",
      "Training:  72%|████████████████████▊        | 1399/1953 [05:17<02:02,  4.53it/s]Epoch 5 Iter 1400/1953 | Loss 0.1278 | 1129.5 img/s\n",
      "Training:  74%|█████████████████████▌       | 1449/1953 [05:28<01:57,  4.30it/s]Epoch 5 Iter 1450/1953 | Loss 0.1273 | 1128.5 img/s\n",
      "Training:  77%|██████████████████████▎      | 1499/1953 [05:40<01:49,  4.15it/s]Epoch 5 Iter 1500/1953 | Loss 0.1269 | 1128.1 img/s\n",
      "Training:  79%|███████████████████████      | 1549/1953 [05:51<01:31,  4.43it/s]Epoch 5 Iter 1550/1953 | Loss 0.1265 | 1128.6 img/s\n",
      "Training:  82%|███████████████████████▋     | 1599/1953 [06:02<01:24,  4.21it/s]Epoch 5 Iter 1600/1953 | Loss 0.1261 | 1129.2 img/s\n",
      "Training:  84%|████████████████████████▍    | 1649/1953 [06:14<01:15,  4.04it/s]Epoch 5 Iter 1650/1953 | Loss 0.1257 | 1128.4 img/s\n",
      "Training:  87%|█████████████████████████▏   | 1699/1953 [06:25<00:59,  4.28it/s]Epoch 5 Iter 1700/1953 | Loss 0.1253 | 1129.0 img/s\n",
      "Training:  90%|█████████████████████████▉   | 1749/1953 [06:36<00:45,  4.47it/s]Epoch 5 Iter 1750/1953 | Loss 0.1250 | 1128.8 img/s\n",
      "Training:  92%|██████████████████████████▋  | 1799/1953 [06:48<00:35,  4.36it/s]Epoch 5 Iter 1800/1953 | Loss 0.1247 | 1128.9 img/s\n",
      "Training:  95%|███████████████████████████▍ | 1849/1953 [06:59<00:24,  4.28it/s]Epoch 5 Iter 1850/1953 | Loss 0.1243 | 1128.6 img/s\n",
      "Training:  97%|████████████████████████████▏| 1899/1953 [07:10<00:12,  4.43it/s]Epoch 5 Iter 1900/1953 | Loss 0.1240 | 1128.5 img/s\n",
      "Training: 100%|████████████████████████████▉| 1949/1953 [07:22<00:00,  4.51it/s]Epoch 5 Iter 1950/1953 | Loss 0.1237 | 1128.5 img/s\n",
      "Training: 100%|█████████████████████████████| 1953/1953 [07:23<00:00,  4.41it/s]\n",
      "\n",
      "Training: 100%|█████████████████████████████| 1953/1953 [07:23<00:00,  4.41it/s]\n",
      "\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Saved checkpoint to outputs/ijepa/ijepa_epoch_0005.pt\n",
      "Epoch 5/5 | loss=0.1237\n"
     ]
    }
   ],
   "source": [
    "# Using the cli\n",
    "\n",
    "# Clear\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "# Train using default cifar100 config + custom ViT backbone\n",
    "# !python -m wejepa.train.pretrain --print-config     # print only\n",
    "# !python -m wejepa.train.pretrain                    # train\n",
    "\n",
    "# FIXME: bug when using .arrow files, the file path is not correctly set, workaround is to rename the arrow file\n",
    "#   cp fall2025_deeplearning-train.arrow tsbpp___fall2025_deeplearning-train.arrow\n",
    "\n",
    "# print where --config searches for config files\n",
    "# !python -m wejepa.train.pretrain --config hf224_config.json\n",
    "\n",
    "!PYTHONWARNINGS=\"ignore::RuntimeWarning\" python -m wejepa.train.pretrain --config configs/pretrain_devel_tsbpp_224.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# programmatically\n",
    "from wejepa import default_config, launch_pretraining\n",
    "cfg = default_config()\n",
    "launch_pretraining(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffda647",
   "metadata": {},
   "source": [
    "### Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2913c702",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear probe] Epoch 1/10 | loss=4.7092 | train_acc=0.021 | val_acc=0.038\n",
      "[Linear probe] Epoch 2/10 | loss=4.5012 | train_acc=0.036 | val_acc=0.049\n",
      "[Linear probe] Epoch 3/10 | loss=4.4424 | train_acc=0.045 | val_acc=0.056\n",
      "[Linear probe] Epoch 4/10 | loss=4.4062 | train_acc=0.046 | val_acc=0.059\n",
      "[Linear probe] Epoch 5/10 | loss=4.3753 | train_acc=0.050 | val_acc=0.063\n",
      "[Linear probe] Epoch 6/10 | loss=4.3507 | train_acc=0.054 | val_acc=0.069\n",
      "[Linear probe] Epoch 7/10 | loss=4.3309 | train_acc=0.055 | val_acc=0.067\n",
      "[Linear probe] Epoch 8/10 | loss=4.3173 | train_acc=0.056 | val_acc=0.071\n",
      "[Linear probe] Epoch 9/10 | loss=4.3029 | train_acc=0.057 | val_acc=0.070\n",
      "[Linear probe] Epoch 10/10 | loss=4.2934 | train_acc=0.060 | val_acc=0.070\n"
     ]
    }
   ],
   "source": [
    "# using the cli\n",
    "!python -m wejepa.train.finetune \\\n",
    "    --checkpoint outputs/ijepa/ijepa_epoch_0005.pt \\\n",
    "    --epochs 10 \\\n",
    "    --batch-size 256 \\\n",
    "    --lr 3e-4 \\\n",
    "    --num-classes 200 \\\n",
    "    --config configs/finetune_devel_cub200_224.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f283f657",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wejepa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# programmatically\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwejepa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfinetune\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FinetuneConfig, train_linear_probe\n\u001b[1;32m      4\u001b[0m ft_cfg \u001b[38;5;241m=\u001b[39m FinetuneConfig(\n\u001b[1;32m      5\u001b[0m     checkpoint_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs/ijepa/ijepa_epoch_0005.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      7\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m      8\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m train_linear_probe(ft_cfg)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wejepa'"
     ]
    }
   ],
   "source": [
    "# programmatically\n",
    "from wejepa.train import FinetuneConfig, train_linear_probe\n",
    "\n",
    "ft_cfg = FinetuneConfig(\n",
    "    checkpoint_path=\"outputs/ijepa/ijepa_epoch_0005.pt\",\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "train_linear_probe(ft_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e75d3",
   "metadata": {},
   "source": [
    "### Different Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a884a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "from wejepa.backbones import adapt_config_for_backbone, available_backbones\n",
    "from wejepa.config import IJepaConfig\n",
    "from wejepa import default_config, launch_pretraining, IJEPA_base\n",
    "\n",
    "print(\"Registered backbones: \")\n",
    "for backbone in available_backbones():\n",
    "    print(f\"- {backbone}\")\n",
    "\n",
    "candidates = [\"vit_b_16\", \"swin_t\", \"convnext_tiny\"]\n",
    "for backbone in candidates:\n",
    "    print(f\"\\nPretraining with backbone: {backbone}\")\n",
    "\n",
    "for backbone in available_backbones():\n",
    "    cfg = adapt_config_for_backbone(default_config(), backbone)\n",
    "    print(f\"\\nBackbone: {backbone}\")\n",
    "    print(f\"Image size: {cfg.model.img_size} | Patch size: {cfg.model.patch_size}\")\n",
    "\n",
    "    model = IJEPA_base(\n",
    "        img_size=cfg.model.img_size,\n",
    "        patch_size=cfg.model.patch_size,\n",
    "        in_chans=cfg.model.in_chans,\n",
    "        embed_dim=cfg.model.embed_dim,\n",
    "        enc_depth=cfg.model.enc_depth,\n",
    "        pred_depth=cfg.model.pred_depth,\n",
    "        num_heads=cfg.model.num_heads,\n",
    "        backbone=cfg.model.classification_backbone,\n",
    "        pretrained=cfg.model.classification_pretrained,\n",
    "    )\n",
    "\n",
    "    print(f\"Total trainable params: {model.count_trainable_parameters() / 1e6:.2f}M\")\n",
    "    print(f\"Student + predictor params: {model.count_parameters() / 1e6:.2f}M\")\n",
    "\n",
    "    dummy = torch.randn(1, cfg.model.in_chans, cfg.model.img_size, cfg.model.img_size)\n",
    "    preds, targets = model(dummy)\n",
    "    print(f\"Pred shape: {tuple(preds.shape)} | Target shape: {tuple(targets.shape)}\")\n",
    "    print(json.dumps(cfg.to_dict(), indent=2))\n",
    "\n",
    "    cfg.hardware.output_dir = f\"./outputs/ijepa/{backbone}\"\n",
    "    cfg_path = Path(f\"configs/pretrain_{backbone}.json\")\n",
    "    cfg_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cfg_path.write_text(json.dumps(cfg.to_dict(), indent=2))\n",
    "    print(f\"Saved config for {backbone} at {cfg_path}\")\n",
    "\n",
    "    # launch_pretraining(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4a93a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
