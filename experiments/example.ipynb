{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc761ad1",
   "metadata": {},
   "source": [
    "# Example based on our README.md\n",
    "1. Dataset download\n",
    "2. Pre-training\n",
    "3. Fine-tuning\n",
    "4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de0bef",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e8ab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 'train[:10]' available under /home/long/code/dl_project1/experiments/data\n",
      "Split 'train' available under /home/long/code/dl_project1/experiments/data\n",
      "Split 'test' available under /home/long/code/dl_project1/experiments/data\n"
     ]
    }
   ],
   "source": [
    "# using the cli\n",
    "\n",
    "# download class dataset\n",
    "# !python -m wejepa.datasets.download --dataset-root ./data --dataset-name tsbpp/fall2025_deeplearning --splits train\n",
    "\n",
    "# for development, download a small subset\n",
    "# !python -m wejepa.datasets.download --dataset-root ./data --dataset-name tsbpp/fall2025_deeplearning --splits 'train[:10]'\n",
    "\n",
    "# download cifar100 dataset\n",
    "# !python -m wejepa.datasets.download --dataset-root ./data --dataset-name cifar100\n",
    "\n",
    "# download cub200 dataset\n",
    "# !python -m wejepa.datasets.download --dataset-root ./data --dataset-name cub200 --splits train,test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21635921",
   "metadata": {},
   "source": [
    "### 2. Pre-training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90f233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "Model has 5,061,504 trainable parameters.\n",
      "Using the latest cached version of the dataset since tsbpp___fall2025_deeplearning couldn't be found on the Hugging Face Hub\n",
      "Using the latest cached version of the dataset since tsbpp___fall2025_deeplearning couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at data/tsbpp___fall2025_deeplearning/default/0.0.0/7b14dd4385d982457822e8e96c5081a30da146d8 (last modified on Thu Nov 20 02:42:58 2025).\n",
      "Found the latest cached dataset configuration 'default' at data/tsbpp___fall2025_deeplearning/default/0.0.0/7b14dd4385d982457822e8e96c5081a30da146d8 (last modified on Thu Nov 20 02:42:58 2025).\n",
      "Using the latest cached version of the dataset since tsbpp___fall2025_deeplearning couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at data/tsbpp___fall2025_deeplearning/default/0.0.0/7b14dd4385d982457822e8e96c5081a30da146d8 (last modified on Thu Nov 20 02:42:58 2025).\n",
      "Using the latest cached version of the dataset since tsbpp___fall2025_deeplearning couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at data/tsbpp___fall2025_deeplearning/default/0.0.0/7b14dd4385d982457822e8e96c5081a30da146d8 (last modified on Thu Nov 20 02:42:58 2025).\n",
      "/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "Training:   0%|                                        | 0/3906 [00:00<?, ?it/s]<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "<frozen runpy>:128: RuntimeWarning: 'wejepa.train.pretrain' found in sys.modules after import of package 'wejepa.train', but prior to execution of 'wejepa.train.pretrain'; this may result in unpredictable behaviour\n",
      "Training:   1%|▎                           | 49/3906 [03:30<13:56:29, 13.01s/it]Epoch 1 Iter 50/3906 | Loss 0.3891 | 30.2 img/s\n",
      "Training:   3%|▋                            | 99/3906 [06:07<7:29:17,  7.08s/it]Epoch 1 Iter 100/3906 | Loss 0.3426 | 34.8 img/s\n",
      "Training:   4%|█                           | 149/3906 [08:42<3:56:28,  3.78s/it]Epoch 1 Iter 150/3906 | Loss 0.3212 | 36.7 img/s\n",
      "Training:   5%|█▍                          | 199/3906 [11:20<2:31:47,  2.46s/it]Epoch 1 Iter 200/3906 | Loss 0.3113 | 37.6 img/s\n",
      "Training:   6%|█▊                          | 249/3906 [13:59<1:26:04,  1.41s/it]Epoch 1 Iter 250/3906 | Loss 0.3073 | 38.1 img/s\n",
      "Training:   8%|██▎                           | 299/3906 [16:41<58:41,  1.02it/s]Epoch 1 Iter 300/3906 | Loss 0.3063 | 38.3 img/s\n",
      "Training:   9%|██▋                           | 349/3906 [19:27<48:19,  1.23it/s]Epoch 1 Iter 350/3906 | Loss 0.3066 | 38.4 img/s\n",
      "Training:  10%|███                           | 399/3906 [22:12<37:12,  1.57it/s]Epoch 1 Iter 400/3906 | Loss 0.3081 | 38.4 img/s\n",
      "Training:  11%|███                        | 449/3906 [25:32<10:50:52, 11.30s/it]Epoch 1 Iter 450/3906 | Loss 0.3099 | 37.5 img/s\n",
      "Training:  13%|███▌                        | 499/3906 [28:17<5:31:45,  5.84s/it]Epoch 1 Iter 500/3906 | Loss 0.3122 | 37.7 img/s\n",
      "Training:  14%|███▉                        | 549/3906 [31:15<6:40:04,  7.15s/it]Epoch 1 Iter 550/3906 | Loss 0.3149 | 37.5 img/s\n",
      "Training:  15%|████▎                       | 599/3906 [34:01<3:03:38,  3.33s/it]Epoch 1 Iter 600/3906 | Loss 0.3186 | 37.6 img/s\n",
      "Training:  17%|████▋                       | 649/3906 [36:45<1:32:13,  1.70s/it]Epoch 1 Iter 650/3906 | Loss 0.3219 | 37.7 img/s\n",
      "Training:  18%|█████▎                        | 699/3906 [39:32<59:06,  1.11s/it]Epoch 1 Iter 700/3906 | Loss 0.3256 | 37.8 img/s\n",
      "Training:  19%|█████▊                        | 749/3906 [42:19<43:11,  1.22it/s]Epoch 1 Iter 750/3906 | Loss 0.3292 | 37.8 img/s\n",
      "Training:  20%|██████▏                       | 799/3906 [45:06<34:25,  1.50it/s]Epoch 1 Iter 800/3906 | Loss 0.3325 | 37.8 img/s\n",
      "Training:  22%|██████                      | 850/3906 [48:27<6:06:04,  7.19s/it]Epoch 1 Iter 850/3906 | Loss 0.3356 | 37.4 img/s\n",
      "Training:  23%|██████▍                     | 899/3906 [51:14<4:07:59,  4.95s/it]Epoch 1 Iter 900/3906 | Loss 0.3382 | 37.5 img/s\n",
      "Training:  24%|██████▊                     | 949/3906 [54:23<7:26:22,  9.06s/it]Epoch 1 Iter 950/3906 | Loss 0.3400 | 37.3 img/s\n",
      "Training:  26%|███████▏                    | 999/3906 [57:09<3:36:08,  4.46s/it]Epoch 1 Iter 1000/3906 | Loss 0.3409 | 37.3 img/s\n",
      "Training:  27%|███████▎                   | 1049/3906 [59:55<1:57:40,  2.47s/it]Epoch 1 Iter 1050/3906 | Loss 0.3412 | 37.4 img/s\n",
      "Training:  28%|███████                  | 1099/3906 [1:02:44<1:09:13,  1.48s/it]Epoch 1 Iter 1100/3906 | Loss 0.3410 | 37.4 img/s\n",
      "Training:  29%|███████▉                   | 1149/3906 [1:05:33<48:19,  1.05s/it]Epoch 1 Iter 1150/3906 | Loss 0.3404 | 37.4 img/s\n",
      "Training:  31%|████████▎                  | 1199/3906 [1:08:20<33:18,  1.35it/s]Epoch 1 Iter 1200/3906 | Loss 0.3395 | 37.5 img/s\n",
      "Training:  32%|███████▉                 | 1249/3906 [1:11:28<4:24:08,  5.96s/it]Epoch 1 Iter 1250/3906 | Loss 0.3384 | 37.3 img/s\n",
      "Training:  33%|████████▎                | 1299/3906 [1:14:15<2:11:21,  3.02s/it]Epoch 1 Iter 1300/3906 | Loss 0.3370 | 37.3 img/s\n",
      "Training:  35%|████████▋                | 1349/3906 [1:17:36<8:15:08, 11.62s/it]Epoch 1 Iter 1350/3906 | Loss 0.3356 | 37.1 img/s\n",
      "Training:  36%|████████▉                | 1399/3906 [1:20:20<3:33:41,  5.11s/it]Epoch 1 Iter 1400/3906 | Loss 0.3341 | 37.2 img/s\n",
      "Training:  37%|█████████▎               | 1449/3906 [1:23:13<2:05:45,  3.07s/it]Epoch 1 Iter 1450/3906 | Loss 0.3325 | 37.2 img/s\n",
      "Training:  38%|█████████▌               | 1499/3906 [1:26:00<1:08:58,  1.72s/it]Epoch 1 Iter 1500/3906 | Loss 0.3309 | 37.2 img/s\n",
      "Training:  40%|██████████▋                | 1549/3906 [1:28:48<42:00,  1.07s/it]Epoch 1 Iter 1550/3906 | Loss 0.3293 | 37.2 img/s\n",
      "Training:  41%|███████████                | 1599/3906 [1:31:37<29:54,  1.29it/s]Epoch 1 Iter 1600/3906 | Loss 0.3277 | 37.3 img/s\n",
      "Training:  42%|██████████▌              | 1649/3906 [1:34:39<2:57:31,  4.72s/it]Epoch 1 Iter 1650/3906 | Loss 0.3261 | 37.2 img/s\n",
      "Training:  43%|██████████▊              | 1699/3906 [1:37:28<1:37:48,  2.66s/it]Epoch 1 Iter 1700/3906 | Loss 0.3245 | 37.2 img/s\n",
      "Training:  45%|███████████▏             | 1749/3906 [1:40:52<7:00:58, 11.71s/it]Epoch 1 Iter 1750/3906 | Loss 0.3229 | 37.0 img/s\n",
      "Training:  46%|███████████▌             | 1799/3906 [1:43:40<3:29:23,  5.96s/it]Epoch 1 Iter 1800/3906 | Loss 0.3213 | 37.0 img/s\n",
      "Training:  47%|███████████▊             | 1849/3906 [1:46:29<1:50:07,  3.21s/it]Epoch 1 Iter 1850/3906 | Loss 0.3196 | 37.1 img/s\n",
      "Training:  49%|████████████▏            | 1899/3906 [1:49:17<1:02:52,  1.88s/it]Epoch 1 Iter 1900/3906 | Loss 0.3181 | 37.1 img/s\n",
      "Training:  50%|█████████████▍             | 1949/3906 [1:52:07<39:33,  1.21s/it]Epoch 1 Iter 1950/3906 | Loss 0.3164 | 37.1 img/s\n",
      "Training:  51%|█████████████▊             | 1999/3906 [1:54:58<27:42,  1.15it/s]Epoch 1 Iter 2000/3906 | Loss 0.3148 | 37.1 img/s\n",
      "Training:  52%|██████████████▏            | 2049/3906 [1:57:47<21:18,  1.45it/s]Epoch 1 Iter 2050/3906 | Loss 0.3132 | 37.1 img/s\n",
      "Training:  54%|██████████████▌            | 2099/3906 [2:00:36<24:11,  1.24it/s]Epoch 1 Iter 2100/3906 | Loss 0.3115 | 37.1 img/s\n",
      "Training:  55%|█████████████▊           | 2149/3906 [2:04:08<6:26:53, 13.21s/it]Epoch 1 Iter 2150/3906 | Loss 0.3099 | 36.9 img/s\n",
      "Training:  56%|██████████████           | 2199/3906 [2:06:58<3:18:55,  6.99s/it]Epoch 1 Iter 2200/3906 | Loss 0.3082 | 37.0 img/s\n",
      "Training:  58%|██████████████▍          | 2249/3906 [2:09:46<1:41:41,  3.68s/it]Epoch 1 Iter 2250/3906 | Loss 0.3066 | 37.0 img/s\n",
      "Training:  59%|███████████████▉           | 2299/3906 [2:12:35<55:38,  2.08s/it]Epoch 1 Iter 2300/3906 | Loss 0.3050 | 37.0 img/s\n",
      "Training:  60%|████████████████▏          | 2349/3906 [2:15:20<32:21,  1.25s/it]Epoch 1 Iter 2350/3906 | Loss 0.3034 | 37.0 img/s\n",
      "Training:  61%|████████████████▌          | 2399/3906 [2:18:12<21:41,  1.16it/s]Epoch 1 Iter 2400/3906 | Loss 0.3018 | 37.0 img/s\n",
      "Training:  63%|████████████████▉          | 2449/3906 [2:20:57<16:15,  1.49it/s]Epoch 1 Iter 2450/3906 | Loss 0.3003 | 37.1 img/s\n",
      "Training:  64%|█████████████████▎         | 2499/3906 [2:23:46<14:13,  1.65it/s]Epoch 1 Iter 2500/3906 | Loss 0.2988 | 37.1 img/s\n",
      "Training:  65%|████████████████▎        | 2549/3906 [2:27:23<5:37:11, 14.91s/it]Epoch 1 Iter 2550/3906 | Loss 0.2974 | 36.9 img/s\n",
      "Training:  67%|████████████████▋        | 2599/3906 [2:30:10<2:37:50,  7.25s/it]Epoch 1 Iter 2600/3906 | Loss 0.2960 | 36.9 img/s\n",
      "Training:  68%|████████████████▉        | 2649/3906 [2:32:53<1:17:14,  3.69s/it]Epoch 1 Iter 2650/3906 | Loss 0.2946 | 37.0 img/s\n",
      "Training:  69%|██████████████████▋        | 2699/3906 [2:35:44<43:25,  2.16s/it]Epoch 1 Iter 2700/3906 | Loss 0.2933 | 37.0 img/s\n",
      "Training:  70%|███████████████████        | 2749/3906 [2:38:32<25:11,  1.31s/it]Epoch 1 Iter 2750/3906 | Loss 0.2920 | 37.0 img/s\n",
      "Training:  72%|███████████████████▎       | 2799/3906 [2:41:23<16:48,  1.10it/s]Epoch 1 Iter 2800/3906 | Loss 0.2909 | 37.0 img/s\n",
      "Training:  73%|███████████████████▋       | 2850/3906 [2:44:09<11:00,  1.60it/s]Epoch 1 Iter 2850/3906 | Loss 0.2897 | 37.0 img/s\n",
      "Training:  74%|████████████████████       | 2899/3906 [2:46:52<10:04,  1.67it/s]Epoch 1 Iter 2900/3906 | Loss 0.2886 | 37.1 img/s\n",
      "Training:  75%|██████████████████▊      | 2949/3906 [2:50:31<4:04:55, 15.36s/it]Epoch 1 Iter 2950/3906 | Loss 0.2875 | 36.9 img/s\n",
      "Training:  77%|███████████████████▏     | 2999/3906 [2:53:21<1:56:27,  7.70s/it]Epoch 1 Iter 3000/3906 | Loss 0.2864 | 36.9 img/s\n",
      "Training:  78%|█████████████████████      | 3049/3906 [2:56:07<54:39,  3.83s/it]Epoch 1 Iter 3050/3906 | Loss 0.2855 | 36.9 img/s\n",
      "Training:  79%|█████████████████████▍     | 3099/3906 [2:58:56<29:46,  2.21s/it]Epoch 1 Iter 3100/3906 | Loss 0.2846 | 37.0 img/s\n",
      "Training:  81%|█████████████████████▊     | 3149/3906 [3:01:44<16:36,  1.32s/it]Epoch 1 Iter 3150/3906 | Loss 0.2837 | 37.0 img/s\n",
      "Training:  82%|██████████████████████     | 3199/3906 [3:04:31<10:32,  1.12it/s]Epoch 1 Iter 3200/3906 | Loss 0.2828 | 37.0 img/s\n",
      "Training:  83%|██████████████████████▍    | 3249/3906 [3:07:19<07:43,  1.42it/s]Epoch 1 Iter 3250/3906 | Loss 0.2820 | 37.0 img/s\n",
      "Training:  84%|██████████████████████▊    | 3299/3906 [3:10:07<05:42,  1.77it/s]Epoch 1 Iter 3300/3906 | Loss 0.2813 | 37.0 img/s\n",
      "Training:  86%|█████████████████████▍   | 3349/3906 [3:13:44<2:18:40, 14.94s/it]Epoch 1 Iter 3350/3906 | Loss 0.2806 | 36.9 img/s\n",
      "Training:  87%|█████████████████████▊   | 3399/3906 [3:16:32<1:02:26,  7.39s/it]Epoch 1 Iter 3400/3906 | Loss 0.2800 | 36.9 img/s\n",
      "Training:  88%|███████████████████████▊   | 3449/3906 [3:19:17<28:22,  3.73s/it]Epoch 1 Iter 3450/3906 | Loss 0.2794 | 36.9 img/s\n",
      "Training:  90%|████████████████████████▏  | 3499/3906 [3:22:07<14:37,  2.16s/it]Epoch 1 Iter 3500/3906 | Loss 0.2788 | 36.9 img/s\n",
      "Training:  91%|████████████████████████▌  | 3549/3906 [3:24:54<07:50,  1.32s/it]Epoch 1 Iter 3550/3906 | Loss 0.2783 | 37.0 img/s\n",
      "Training:  92%|████████████████████████▉  | 3599/3906 [3:27:43<04:29,  1.14it/s]Epoch 1 Iter 3600/3906 | Loss 0.2778 | 37.0 img/s\n",
      "Training:  93%|█████████████████████████▏ | 3649/3906 [3:30:30<02:51,  1.50it/s]Epoch 1 Iter 3650/3906 | Loss 0.2773 | 37.0 img/s\n",
      "Training:  95%|█████████████████████████▌ | 3699/3906 [3:33:20<02:01,  1.70it/s]Epoch 1 Iter 3700/3906 | Loss 0.2768 | 37.0 img/s\n",
      "Training:  96%|█████████████████████████▉ | 3749/3906 [3:36:56<39:10, 14.97s/it]Epoch 1 Iter 3750/3906 | Loss 0.2765 | 36.9 img/s\n",
      "Training:  97%|██████████████████████████▎| 3799/3906 [3:39:41<13:09,  7.38s/it]Epoch 1 Iter 3800/3906 | Loss 0.2761 | 36.9 img/s\n",
      "Training:  99%|██████████████████████████▌| 3849/3906 [3:42:32<03:47,  4.00s/it]Epoch 1 Iter 3850/3906 | Loss 0.2758 | 36.9 img/s\n",
      "Training: 100%|██████████████████████████▉| 3899/3906 [3:44:45<00:08,  1.21s/it]Epoch 1 Iter 3900/3906 | Loss 0.2755 | 37.0 img/s\n",
      "Training: 100%|███████████████████████████| 3906/3906 [3:44:48<00:00,  3.45s/it]\n",
      "\n",
      "Training: 100%|███████████████████████████| 3906/3906 [3:44:48<00:00,  3.45s/it]\n",
      "\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Training:   0%|                                        | 0/3906 [00:00<?, ?it/s]Saved checkpoint to outputs/ijepa/ijepa_epoch_0001.pt\n",
      "Epoch 1/5 | loss=0.2754\n",
      "Training:   1%|▎                           | 49/3906 [03:31<14:07:50, 13.19s/it]Epoch 2 Iter 50/3906 | Loss 0.2474 | 29.9 img/s\n",
      "Training:   3%|▋                            | 99/3906 [06:13<7:34:08,  7.16s/it]Epoch 2 Iter 100/3906 | Loss 0.2486 | 34.2 img/s\n",
      "Training:   4%|█                           | 149/3906 [08:58<4:44:24,  4.54s/it]Epoch 2 Iter 150/3906 | Loss 0.2492 | 35.7 img/s\n",
      "Training:   5%|█▍                          | 199/3906 [11:42<2:56:06,  2.85s/it]Epoch 2 Iter 200/3906 | Loss 0.2502 | 36.4 img/s\n",
      "Training:   6%|█▊                          | 250/3906 [14:27<1:22:16,  1.35s/it]Epoch 2 Iter 250/3906 | Loss 0.2505 | 36.9 img/s\n",
      "Training:   8%|██▏                         | 299/3906 [17:11<1:05:21,  1.09s/it]Epoch 2 Iter 300/3906 | Loss 0.2509 | 37.2 img/s\n",
      "Training:   9%|██▋                           | 349/3906 [19:54<44:40,  1.33it/s]Epoch 2 Iter 350/3906 | Loss 0.2513 | 37.5 img/s\n",
      "Training:  10%|███                           | 399/3906 [22:40<36:18,  1.61it/s]Epoch 2 Iter 400/3906 | Loss 0.2521 | 37.6 img/s\n",
      "Training:  11%|███▏                        | 449/3906 [25:50<8:06:33,  8.44s/it]Epoch 2 Iter 450/3906 | Loss 0.2524 | 36.8 img/s\n",
      "Training:  13%|███▌                        | 499/3906 [28:48<6:22:32,  6.74s/it]Epoch 2 Iter 500/3906 | Loss 0.2525 | 37.0 img/s\n",
      "Training:  14%|███▉                        | 549/3906 [31:46<6:23:13,  6.85s/it]Epoch 2 Iter 550/3906 | Loss 0.2525 | 36.9 img/s\n",
      "Training:  15%|████▎                       | 599/3906 [34:34<3:34:30,  3.89s/it]Epoch 2 Iter 600/3906 | Loss 0.2528 | 37.0 img/s\n",
      "Training:  17%|████▋                       | 650/3906 [37:24<1:34:10,  1.74s/it]Epoch 2 Iter 650/3906 | Loss 0.2531 | 37.1 img/s\n",
      "Training:  18%|█████                       | 699/3906 [40:13<1:16:27,  1.43s/it]Epoch 2 Iter 700/3906 | Loss 0.2533 | 37.1 img/s\n",
      "Training:  19%|█████▊                        | 749/3906 [43:03<53:14,  1.01s/it]Epoch 2 Iter 750/3906 | Loss 0.2533 | 37.1 img/s\n",
      "Training:  20%|██████▏                       | 799/3906 [45:53<38:32,  1.34it/s]Epoch 2 Iter 800/3906 | Loss 0.2534 | 37.2 img/s\n",
      "Training:  22%|██████                      | 849/3906 [48:57<4:05:05,  4.81s/it]Epoch 2 Iter 850/3906 | Loss 0.2538 | 37.0 img/s\n",
      "Training:  23%|██████▍                     | 899/3906 [51:48<2:38:49,  3.17s/it]Epoch 2 Iter 900/3906 | Loss 0.2539 | 36.9 img/s\n",
      "Training:  24%|██████▊                     | 950/3906 [55:14<6:47:05,  8.26s/it]Epoch 2 Iter 950/3906 | Loss 0.2544 | 36.7 img/s\n",
      "Training:  26%|███████▏                    | 999/3906 [58:02<4:41:53,  5.82s/it]Epoch 2 Iter 1000/3906 | Loss 0.2545 | 36.8 img/s\n",
      "Training:  27%|██████▋                  | 1049/3906 [1:00:50<2:33:22,  3.22s/it]Epoch 2 Iter 1050/3906 | Loss 0.2548 | 36.8 img/s\n",
      "Training:  28%|███████                  | 1099/3906 [1:03:42<1:32:15,  1.97s/it]Epoch 2 Iter 1100/3906 | Loss 0.2552 | 36.8 img/s\n",
      "Training:  29%|███████▉                   | 1149/3906 [1:06:29<54:42,  1.19s/it]Epoch 2 Iter 1150/3906 | Loss 0.2555 | 36.9 img/s\n",
      "Training:  31%|████████▎                  | 1199/3906 [1:09:22<40:31,  1.11it/s]Epoch 2 Iter 1200/3906 | Loss 0.2559 | 36.9 img/s\n",
      "Training:  32%|████████▋                  | 1249/3906 [1:12:07<29:18,  1.51it/s]Epoch 2 Iter 1250/3906 | Loss 0.2562 | 37.0 img/s\n",
      "Training:  33%|████████▉                  | 1299/3906 [1:14:58<25:41,  1.69it/s]Epoch 2 Iter 1300/3906 | Loss 0.2564 | 36.9 img/s\n",
      "Training:  35%|████████▎               | 1349/3906 [1:18:36<10:27:22, 14.72s/it]Epoch 2 Iter 1350/3906 | Loss 0.2567 | 36.6 img/s\n",
      "Training:  36%|████████▉                | 1399/3906 [1:21:22<5:09:26,  7.41s/it]Epoch 2 Iter 1400/3906 | Loss 0.2570 | 36.7 img/s\n",
      "Training:  37%|█████████▎               | 1449/3906 [1:24:10<2:41:53,  3.95s/it]Epoch 2 Iter 1450/3906 | Loss 0.2574 | 36.8 img/s\n",
      "Training:  38%|█████████▌               | 1499/3906 [1:26:57<1:26:58,  2.17s/it]Epoch 2 Iter 1500/3906 | Loss 0.2577 | 36.8 img/s\n",
      "Training:  40%|██████████▋                | 1549/3906 [1:29:47<50:47,  1.29s/it]Epoch 2 Iter 1550/3906 | Loss 0.2580 | 36.8 img/s\n",
      "Training:  41%|███████████                | 1600/3906 [1:32:35<30:44,  1.25it/s]Epoch 2 Iter 1600/3906 | Loss 0.2583 | 36.9 img/s\n",
      "Training:  42%|███████████▍               | 1649/3906 [1:35:24<26:17,  1.43it/s]Epoch 2 Iter 1650/3906 | Loss 0.2585 | 36.9 img/s\n",
      "Training:  43%|███████████▋               | 1699/3906 [1:38:14<22:17,  1.65it/s]Epoch 2 Iter 1700/3906 | Loss 0.2588 | 36.9 img/s\n",
      "Training:  45%|███████████▏             | 1749/3906 [1:41:53<9:12:45, 15.38s/it]Epoch 2 Iter 1750/3906 | Loss 0.2591 | 36.6 img/s\n",
      "Training:  46%|███████████▌             | 1799/3906 [1:44:41<4:36:10,  7.86s/it]Epoch 2 Iter 1800/3906 | Loss 0.2593 | 36.7 img/s\n",
      "Training:  47%|███████████▊             | 1849/3906 [1:47:26<2:10:07,  3.80s/it]Epoch 2 Iter 1850/3906 | Loss 0.2595 | 36.7 img/s\n",
      "Training:  49%|████████████▏            | 1899/3906 [1:50:13<1:12:53,  2.18s/it]Epoch 2 Iter 1900/3906 | Loss 0.2596 | 36.8 img/s\n",
      "Training:  50%|█████████████▍             | 1949/3906 [1:53:01<44:43,  1.37s/it]Epoch 2 Iter 1950/3906 | Loss 0.2598 | 36.8 img/s\n",
      "Training:  51%|█████████████▊             | 1999/3906 [1:55:48<29:11,  1.09it/s]Epoch 2 Iter 2000/3906 | Loss 0.2600 | 36.8 img/s\n",
      "Training:  52%|██████████████▏            | 2049/3906 [1:58:35<20:06,  1.54it/s]Epoch 2 Iter 2050/3906 | Loss 0.2602 | 36.9 img/s\n",
      "Training:  54%|██████████████▌            | 2099/3906 [2:01:23<18:18,  1.65it/s]Epoch 2 Iter 2100/3906 | Loss 0.2604 | 36.9 img/s\n",
      "Training:  55%|█████████████▊           | 2149/3906 [2:05:02<7:15:00, 14.86s/it]Epoch 2 Iter 2150/3906 | Loss 0.2605 | 36.7 img/s\n",
      "Training:  56%|██████████████           | 2199/3906 [2:07:51<3:32:42,  7.48s/it]Epoch 2 Iter 2200/3906 | Loss 0.2607 | 36.7 img/s\n",
      "Training:  58%|██████████████▍          | 2249/3906 [2:10:40<1:49:27,  3.96s/it]Epoch 2 Iter 2250/3906 | Loss 0.2608 | 36.7 img/s\n",
      "Training:  59%|███████████████▉           | 2299/3906 [2:13:28<58:56,  2.20s/it]Epoch 2 Iter 2300/3906 | Loss 0.2610 | 36.8 img/s\n",
      "Training:  60%|████████████████▏          | 2349/3906 [2:16:15<31:59,  1.23s/it]Epoch 2 Iter 2350/3906 | Loss 0.2611 | 36.8 img/s\n",
      "Training:  61%|████████████████▌          | 2399/3906 [2:19:04<22:48,  1.10it/s]Epoch 2 Iter 2400/3906 | Loss 0.2612 | 36.8 img/s\n",
      "Training:  63%|████████████████▉          | 2450/3906 [2:21:52<14:51,  1.63it/s]Epoch 2 Iter 2450/3906 | Loss 0.2614 | 36.8 img/s\n",
      "Training:  64%|█████████████████▎         | 2500/3906 [2:24:40<12:49,  1.83it/s]Epoch 2 Iter 2500/3906 | Loss 0.2615 | 36.9 img/s\n",
      "Training:  65%|████████████████▎        | 2549/3906 [2:28:16<5:38:32, 14.97s/it]Epoch 2 Iter 2550/3906 | Loss 0.2616 | 36.7 img/s\n",
      "Training:  67%|████████████████▋        | 2599/3906 [2:31:01<2:42:30,  7.46s/it]Epoch 2 Iter 2600/3906 | Loss 0.2616 | 36.7 img/s\n",
      "Training:  68%|████████████████▉        | 2649/3906 [2:33:55<1:24:19,  4.02s/it]Epoch 2 Iter 2650/3906 | Loss 0.2617 | 36.7 img/s\n",
      "Training:  69%|██████████████████▋        | 2699/3906 [2:36:42<43:18,  2.15s/it]Epoch 2 Iter 2700/3906 | Loss 0.2619 | 36.8 img/s\n",
      "Training:  70%|███████████████████        | 2749/3906 [2:39:31<24:52,  1.29s/it]Epoch 2 Iter 2750/3906 | Loss 0.2619 | 36.8 img/s\n",
      "Training:  72%|███████████████████▎       | 2799/3906 [2:42:19<16:49,  1.10it/s]Epoch 2 Iter 2800/3906 | Loss 0.2621 | 36.8 img/s\n",
      "Training:  73%|███████████████████▋       | 2849/3906 [2:45:03<12:01,  1.46it/s]Epoch 2 Iter 2850/3906 | Loss 0.2622 | 36.8 img/s\n",
      "Training:  74%|████████████████████       | 2899/3906 [2:47:56<09:59,  1.68it/s]Epoch 2 Iter 2900/3906 | Loss 0.2623 | 36.8 img/s\n",
      "Training:  75%|██████████████████▊      | 2949/3906 [2:51:33<4:00:51, 15.10s/it]Epoch 2 Iter 2950/3906 | Loss 0.2624 | 36.7 img/s\n",
      "Training:  77%|███████████████████▏     | 2999/3906 [2:54:18<1:49:50,  7.27s/it]Epoch 2 Iter 3000/3906 | Loss 0.2625 | 36.7 img/s\n",
      "Training:  78%|█████████████████████      | 3049/3906 [2:57:07<56:34,  3.96s/it]Epoch 2 Iter 3050/3906 | Loss 0.2626 | 36.7 img/s\n",
      "Training:  79%|█████████████████████▍     | 3099/3906 [2:59:55<29:38,  2.20s/it]Epoch 2 Iter 3100/3906 | Loss 0.2626 | 36.8 img/s\n",
      "Training:  81%|█████████████████████▊     | 3149/3906 [3:02:40<16:29,  1.31s/it]Epoch 2 Iter 3150/3906 | Loss 0.2627 | 36.8 img/s\n",
      "Training:  82%|██████████████████████     | 3199/3906 [3:05:33<10:34,  1.11it/s]Epoch 2 Iter 3200/3906 | Loss 0.2628 | 36.8 img/s\n",
      "Training:  83%|██████████████████████▍    | 3249/3906 [3:08:20<07:31,  1.46it/s]Epoch 2 Iter 3250/3906 | Loss 0.2628 | 36.8 img/s\n",
      "Training:  84%|██████████████████████▊    | 3299/3906 [3:11:06<05:50,  1.73it/s]Epoch 2 Iter 3300/3906 | Loss 0.2628 | 36.8 img/s\n",
      "Training:  86%|█████████████████████▍   | 3349/3906 [3:14:44<2:23:54, 15.50s/it]Epoch 2 Iter 3350/3906 | Loss 0.2628 | 36.7 img/s\n",
      "Training:  87%|█████████████████████▊   | 3399/3906 [3:17:30<1:02:02,  7.34s/it]Epoch 2 Iter 3400/3906 | Loss 0.2628 | 36.7 img/s\n",
      "Training:  88%|███████████████████████▊   | 3449/3906 [3:20:19<29:32,  3.88s/it]Epoch 2 Iter 3450/3906 | Loss 0.2628 | 36.7 img/s\n",
      "Training:  90%|████████████████████████▏  | 3499/3906 [3:23:07<14:27,  2.13s/it]Epoch 2 Iter 3500/3906 | Loss 0.2628 | 36.8 img/s\n",
      "Training:  91%|████████████████████████▌  | 3549/3906 [3:25:57<07:54,  1.33s/it]Epoch 2 Iter 3550/3906 | Loss 0.2628 | 36.8 img/s\n",
      "Training:  92%|████████████████████████▉  | 3599/3906 [3:28:42<04:30,  1.14it/s]Epoch 2 Iter 3600/3906 | Loss 0.2627 | 36.8 img/s\n",
      "Training:  93%|█████████████████████████▏ | 3649/3906 [3:31:31<02:46,  1.54it/s]Epoch 2 Iter 3650/3906 | Loss 0.2627 | 36.8 img/s\n",
      "Training:  95%|█████████████████████████▌ | 3699/3906 [3:34:18<01:57,  1.77it/s]Epoch 2 Iter 3700/3906 | Loss 0.2627 | 36.8 img/s\n",
      "Training:  96%|█████████████████████████▉ | 3749/3906 [3:37:57<39:31, 15.10s/it]Epoch 2 Iter 3750/3906 | Loss 0.2626 | 36.7 img/s\n",
      "Training:  97%|██████████████████████████▎| 3799/3906 [3:40:44<13:06,  7.35s/it]Epoch 2 Iter 3800/3906 | Loss 0.2625 | 36.7 img/s\n",
      "Training:  99%|██████████████████████████▌| 3849/3906 [3:43:36<03:56,  4.15s/it]Epoch 2 Iter 3850/3906 | Loss 0.2624 | 36.7 img/s\n",
      "Training: 100%|██████████████████████████▉| 3899/3906 [3:45:47<00:08,  1.26s/it]Epoch 2 Iter 3900/3906 | Loss 0.2624 | 36.8 img/s\n",
      "Training: 100%|███████████████████████████| 3906/3906 [3:45:51<00:00,  3.47s/it]\n",
      "Training: 100%|███████████████████████████| 3906/3906 [3:45:51<00:00,  3.47s/it]\n",
      "\n",
      "\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Epoch completed.\n",
      "Training:   0%|                                        | 0/3906 [00:00<?, ?it/s]Saved checkpoint to outputs/ijepa/ijepa_epoch_0002.pt\n",
      "Epoch 2/5 | loss=0.2624\n",
      "Training:   1%|▎                           | 49/3906 [03:33<14:49:38, 13.84s/it]Epoch 3 Iter 50/3906 | Loss 0.2571 | 29.7 img/s\n",
      "Training:   3%|▋                            | 99/3906 [06:15<7:36:11,  7.19s/it]Epoch 3 Iter 100/3906 | Loss 0.2587 | 33.9 img/s\n",
      "Training:   3%|▉                             | 128/3906 [07:18<36:25,  1.73it/s]"
     ]
    }
   ],
   "source": [
    "# Using the cli\n",
    "\n",
    "# Clear\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "# Train using default cifar100 config + custom ViT backbone\n",
    "# !python -m wejepa.train.pretrain --print-config     # print only\n",
    "# !python -m wejepa.train.pretrain                    # train\n",
    "_\n",
    "# FIXME: bug when using .arrow files, the file path is not correctly set, workaround is to rename the arrow file\n",
    "#   cp fall2025_deeplearning-train.arrow tsbpp___fall2025_deeplearning-train.arrow\n",
    "\n",
    "# print where --config searches for config files\n",
    "!python -m wejepa.train.pretrain --config hf224_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52b311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 157,602,280 trainable parameters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb6a53003cb4d0fb0e38b0901cec4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e28e6ff28134ecb97d61d501081a58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/120M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4939b39f064456a2864eb8e574642c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/23.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e4a5344f9c4eab919368afcf3596ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f7fc4b35bf4e869c9ba03dfff901b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/long/PhD/Environments/ijepa/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/PhD/Environments/ijepa/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/src/wejepa/datasets/hf.py\", line 34, in __getitem__\n    img = self.transform(self.dataset[index][\"image\"])\n                         ~~~~~~~~~~~~~~~~~~~^^^^^^^^^\nKeyError: 'image'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwejepa\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m default_config, launch_pretraining\n\u001b[32m      3\u001b[39m cfg = default_config()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mlaunch_pretraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Coursework/Deep_Learning/Project/Code/ijepa/src/wejepa/train/pretrain.py:302\u001b[39m, in \u001b[36mlaunch_pretraining\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m    300\u001b[39m     mp.spawn(_train_worker, args=(world_size, cfg.to_dict()), nprocs=world_size, join=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     \u001b[43m_train_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Coursework/Deep_Learning/Project/Code/ijepa/src/wejepa/train/pretrain.py:270\u001b[39m, in \u001b[36m_train_worker\u001b[39m\u001b[34m(rank, world_size, cfg_dict)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    269\u001b[39m     sampler.set_epoch(epoch)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m stats = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwd_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rank == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (epoch + \u001b[32m1\u001b[39m) % cfg.hardware.checkpoint_every == \u001b[32m0\u001b[39m:\n\u001b[32m    285\u001b[39m     module = model.module \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, DDP) \u001b[38;5;28;01melse\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Coursework/Deep_Learning/Project/Code/ijepa/src/wejepa/train/pretrain.py:152\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, data_loader, optimizer, scaler, lr_schedule, wd_schedule, momentum_schedule, epoch, state, cfg, device, rank)\u001b[39m\n\u001b[32m    150\u001b[39m module = model.module \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, DDP) \u001b[38;5;28;01melse\u001b[39;00m model\n\u001b[32m    151\u001b[39m accum = \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1506\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1504\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1505\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1506\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1541\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1539\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/Environments/ijepa/lib/python3.12/site-packages/torch/_utils.py:769\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    767\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mKeyError\u001b[39m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/long/PhD/Environments/ijepa/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/PhD/Environments/ijepa/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/home/long/PhD/Coursework/Deep_Learning/Project/Code/ijepa/src/wejepa/datasets/hf.py\", line 34, in __getitem__\n    img = self.transform(self.dataset[index][\"image\"])\n                         ~~~~~~~~~~~~~~~~~~~^^^^^^^^^\nKeyError: 'image'\n"
     ]
    }
   ],
   "source": [
    "# programmatically\n",
    "from wejepa import default_config, launch_pretraining\n",
    "cfg = default_config()\n",
    "launch_pretraining(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffda647",
   "metadata": {},
   "source": [
    "### 3. Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the cli\n",
    "!python -m wejepa.train.finetune \\\n",
    "    --checkpoint outputs/ijepa/ijepa_epoch_0005.pt \\\n",
    "    --epochs 10 \\\n",
    "    --batch-size 256 \\\n",
    "    --lr 3e-4 \\\n",
    "    --num-classes 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# programmatically\n",
    "from wejepa.train import FinetuneConfig, train_linear_probe\n",
    "\n",
    "ft_cfg = FinetuneConfig(\n",
    "    checkpoint_path=\"outputs/ijepa/ijepa_epoch_0005.pt\",\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "train_linear_probe(ft_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4317f0",
   "metadata": {},
   "source": [
    "### 4. Running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from wejepa.train import load_backbone_from_checkpoint\n",
    "from wejepa import default_config\n",
    "\n",
    "cfg = default_config()\n",
    "backbone = load_backbone_from_checkpoint(\"outputs/ijepa/ijepa_epoch_0005.pt\", cfg)\n",
    "backbone.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(cfg.data.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cfg.data.normalization_mean, cfg.data.normalization_std),\n",
    "])\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"./data/tsbpp___fall2025_deeplearning\",\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "label_feature = ds.features[\"label\"] if hasattr(ds, \"features\") else None\n",
    "label_names = label_feature.names if label_feature is not None else None\n",
    "num_classes = len(label_names) if label_names is not None else 100 # default to 100 classes\n",
    "\n",
    "decoder = LinearProbe(backbone, num_classes)\n",
    "decoder.load_state_dict(torch.load(\"outputs/ijepa/linear_probe.pt\", map_location=\"cpu\"))\n",
    "decoder.eval()\n",
    "\n",
    "# grab an image from the dataset\n",
    "image = transform(ds[0][\"image\"]).unsqueeze(0)\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = decoder(image)\n",
    "    probs = torch.softmax(logits,dim=1)\n",
    "    pred_ind = int(probs.argmax(dim=1).item())\n",
    "\n",
    "pred_label = label_names[pred_ind] if label_names is not None else str(pred_ind)\n",
    "top5_inds = probs.topk(5).indices.squeeze(0).tolist()\n",
    "top5_labels = [label_names[i] if label_names is not None else str(i) for i in top5_inds]\n",
    "print(f\"Predicted label: {pred_label}\")\n",
    "print(f\"Top-5 predicted labels: {top5_labels}\")\n",
    "\n",
    "# remove batch dimension and convert to numpy\n",
    "img_np = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# undo normalization for display\n",
    "mean = np.array(cfg.data.normalization_mean)\n",
    "std = np.array(cfg.data.normalization_std)\n",
    "img_np = (img_np * std) + mean\n",
    "img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "plt.imshow(img_np)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = backbone(image)\n",
    "    pooled = tokens.mean(dim=1)  # embeddings for downstream heads\n",
    "\n",
    "# TODO: use the embeddings `pooled` for downstream tasks like classification \n",
    "print(f\"Extracted embeddings shape: {pooled.shape}\")\n",
    "\n",
    "num_classes = 100  # adjust based on your dataset\n",
    "classifier = torch.nn.Linear(pooled.size(1), num_classes)\n",
    "logits = classifier(pooled)\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "# display the classified scores\n",
    "print(f\"Classified scores: {logits}\")\n",
    "\n",
    "# assign predicted class\n",
    "predicted_class = torch.argmax(logits, dim=1)\n",
    "print(f\"Predicted class: {predicted_class.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e75d3",
   "metadata": {},
   "source": [
    "### 5. Different Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf2e234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered backbones: \n",
      "- convnext_tiny\n",
      "- resnet50\n",
      "- resnext50_32x4d\n",
      "- swin_t\n",
      "- vit_b_16\n",
      "- vit_l_16\n",
      "\n",
      "Pretraining with backbone: vit_b_16\n",
      "Saved config for vit_b_16 at configs/pretrain_vit_b_16.json\n",
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /home/long/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /home/long/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /home/long/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /home/long/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330M/330M [00:11<00:00, 31.1MB/s] \n",
      "100%|██████████| 330M/330M [00:11<00:00, 31.0MB/s]\n",
      "100%|██████████| 330M/330M [00:11<00:00, 29.6MB/s]\n",
      "100%|██████████| 330M/330M [00:11<00:00, 29.1MB/s]\n",
      "/home/long/code/dl_project1/src/wejepa/train/pretrain.py:230: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n",
      "/home/long/code/dl_project1/src/wejepa/train/pretrain.py:230: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n",
      "/home/long/code/dl_project1/src/wejepa/train/pretrain.py:230: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n",
      "/home/long/code/dl_project1/src/wejepa/train/pretrain.py:230: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 86,567,656 trainable parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 57.1M/169M [00:00<00:01, 89.4MB/s]W1120 01:53:48.654000 2726638 torch/multiprocessing/spawn.py:174] Terminating process 2728889 via signal SIGTERM\n",
      "W1120 01:53:48.656000 2726638 torch/multiprocessing/spawn.py:174] Terminating process 2728892 via signal SIGTERM\n",
      "W1120 01:53:48.657000 2726638 torch/multiprocessing/spawn.py:174] Terminating process 2728894 via signal SIGTERM\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 2 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 95, in _wrap\n    fn(i, *args)\n  File \"/home/long/code/dl_project1/src/wejepa/train/pretrain.py\", line 231, in _train_worker\n    data_loader, sampler = create_pretraining_dataloader(cfg, rank=rank, world_size=world_size)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/dl_project1/src/wejepa/datasets/cifar.py\", line 107, in create_pretraining_dataloader\n    dataset = IJEPADataset(cfg, train=True, download=rank == 0)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/dl_project1/src/wejepa/datasets/cifar.py\", line 42, in __init__\n    self.dataset = torchvision.datasets.CIFAR100(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torchvision/datasets/cifar.py\", line 69, in __init__\n    raise RuntimeError(\"Dataset not found or corrupted. You can use download=True to download it\")\nRuntimeError: Dataset not found or corrupted. You can use download=True to download it\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mProcessRaisedException\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m cfg_path.write_text(json.dumps(cfg.to_dict(), indent=\u001b[32m2\u001b[39m))\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved config for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackbone\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mlaunch_pretraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/dl_project1/src/wejepa/train/pretrain.py:290\u001b[39m, in \u001b[36mlaunch_pretraining\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m    288\u001b[39m     world_size = torch.cuda.device_count() \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m world_size > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[43mmp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_train_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    292\u001b[39m     _train_worker(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, cfg.to_dict())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/environments/wejepa/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:364\u001b[39m, in \u001b[36mspawn\u001b[39m\u001b[34m(fn, args, nprocs, join, daemon, start_method)\u001b[39m\n\u001b[32m    358\u001b[39m     msg = (\n\u001b[32m    359\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    361\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m     )\n\u001b[32m    363\u001b[39m     warnings.warn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspawn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/environments/wejepa/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:320\u001b[39m, in \u001b[36mstart_processes\u001b[39m\u001b[34m(fn, args, nprocs, join, daemon, start_method, numa_options)\u001b[39m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[32m    319\u001b[39m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/environments/wejepa/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:220\u001b[39m, in \u001b[36mProcessContext.join\u001b[39m\u001b[34m(self, timeout, grace_period)\u001b[39m\n\u001b[32m    218\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m-- Process \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_index\u001b[38;5;132;01m:\u001b[39;00m\u001b[33md\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    219\u001b[39m msg += original_trace\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "\u001b[31mProcessRaisedException\u001b[39m: \n\n-- Process 2 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 95, in _wrap\n    fn(i, *args)\n  File \"/home/long/code/dl_project1/src/wejepa/train/pretrain.py\", line 231, in _train_worker\n    data_loader, sampler = create_pretraining_dataloader(cfg, rank=rank, world_size=world_size)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/dl_project1/src/wejepa/datasets/cifar.py\", line 107, in create_pretraining_dataloader\n    dataset = IJEPADataset(cfg, train=True, download=rank == 0)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/dl_project1/src/wejepa/datasets/cifar.py\", line 42, in __init__\n    self.dataset = torchvision.datasets.CIFAR100(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torchvision/datasets/cifar.py\", line 69, in __init__\n    raise RuntimeError(\"Dataset not found or corrupted. You can use download=True to download it\")\nRuntimeError: Dataset not found or corrupted. You can use download=True to download it\n"
     ]
    }
   ],
   "source": [
    "from wejepa.backbones import available_backbones\n",
    "from wejepa.config import IJepaConfig\n",
    "from wejepa import default_config, launch_pretraining, IJEPA_base\n",
    "from pathlib import Path\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "print(\"Registered backbones: \")\n",
    "for backbone in available_backbones():\n",
    "    print(f\"- {backbone}\")\n",
    "\n",
    "candidates = [\"vit_b_16\", \"swin_t\", \"convnext_tiny\"]\n",
    "for backbone in candidates:\n",
    "    print(f\"\\nPretraining with backbone: {backbone}\")\n",
    "\n",
    "    with open(\"hf224_config.json\", \"r\") as f:\n",
    "        cfg_dict = json.load(f)\n",
    "    cfg = IJepaConfig.from_dict(cfg_dict)\n",
    "\n",
    "    cfg.model.classification_backbone = backbone\n",
    "    cfg.model.classification_pretrained = True\n",
    "    cfg.hardware.output_dir = f\"./outputs/ijepa/{backbone}\"\n",
    "    cfg_path = Path(f\"configs/pretrain_{backbone}.json\")\n",
    "    cfg_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cfg_path.write_text(json.dumps(cfg.to_dict(), indent=2))\n",
    "    print(f\"Saved config for {backbone} at {cfg_path}\")\n",
    "\n",
    "    launch_pretraining(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74790eb",
   "metadata": {},
   "source": [
    "### 6. Visualizing Backbone Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a77ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59791046",
   "metadata": {},
   "source": [
    "### 7. Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7406f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wejepa.analysis.visualization import (\n",
    "    extract_backbone_features,\n",
    "    plot_tsne_embeddings,\n",
    "    run_tsne_projection,\n",
    ")\n",
    "from wejepa.backbones import build_backbone\n",
    "\n",
    "backbone_names = [\"vit_b_16\", \"swin_t\", \"convnext_tiny\"]\n",
    "tsne_results = {}\n",
    "\n",
    "for backbone_name in backbone_names:\n",
    "    print(f\"Projecting embeddings for {backbone_name} ...\")\n",
    "    backbone, feature_dim = build_backbone(backbone_name, pretrained=True, freeze_backbone=True)\n",
    "\n",
    "    # use a small slice of the dataset to keep visualization quick.\n",
    "    dataloader = build_dataloader(backbone_name, batch_size=24, split=\"train[:10]\")\n",
    "    features, labels = extract_backbone_features(backbone, dataloader, max_batches=4)\n",
    "\n",
    "    embedding = run_tsne_projection(features, perplexity=20.0, random_state=42)\n",
    "    fig = plot_tsne_embeddings(embedding, labels)\n",
    "    fig.suptitle(f\"{backbone_name} TSNE\", y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    tsne_results[backbone_name] = embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ijepa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
