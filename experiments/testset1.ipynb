{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb4d4a64-ba34-4b86-af65-5ac9e19b7540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import argparse\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1205dc4d-7661-42c9-97f1-8573b6b4ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle PIL images\"\"\"\n",
    "    if len(batch[0]) == 3:  # train/val (image, label, filename)\n",
    "        images = [item[0] for item in batch]\n",
    "        labels = [item[1] for item in batch]\n",
    "        filenames = [item[2] for item in batch]\n",
    "        return images, labels, filenames\n",
    "    else:  # test (image, filename)\n",
    "        images = [item[0] for item in batch]\n",
    "        filenames = [item[1] for item in batch]\n",
    "        return images, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dc7ccae-af4c-4557-8936-1463d1815d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_list, labels=None,\n",
    "                 resolution=224, split=\"train\", apply_transforms=True):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.image_list = image_list\n",
    "        self.labels = labels\n",
    "        self.split = split\n",
    "        self.resolution = resolution\n",
    "        self.apply_transforms = apply_transforms\n",
    "\n",
    "        imagenet_mean = [0.485, 0.456, 0.406]\n",
    "        imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        if apply_transforms:\n",
    "            if split == \"train\":\n",
    "                self.transform = v2.Compose([\n",
    "                    v2.RandomResizedCrop(resolution, scale=(0.8, 1.0)),\n",
    "                    v2.RandomHorizontalFlip(p=0.5),\n",
    "                    v2.ColorJitter(\n",
    "                        brightness=0.4,\n",
    "                        contrast=0.4,\n",
    "                        saturation=0.4,\n",
    "                        hue=0.1\n",
    "                    ),\n",
    "                    v2.ToImage(),\n",
    "                    v2.ToDtype(torch.float32, scale=True),\n",
    "                    v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = v2.Compose([\n",
    "                    v2.Resize(256),\n",
    "                    v2.CenterCrop(resolution),\n",
    "                    v2.ToImage(),\n",
    "                    v2.ToDtype(torch.float32, scale=True),\n",
    "                    v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = None   # <-- important\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_list[idx]\n",
    "        img_path = self.image_dir / img_name\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)   # -> Tensor (for supervised)\n",
    "        # else: keep img as PIL (for SSL)\n",
    "\n",
    "        if self.labels is not None:\n",
    "            return img, self.labels[idx], img_name\n",
    "        return img, img_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83a1c40e-1ed3-4236-a171-588ce5effbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset metadata...\n",
      "  Train: 8232 images\n",
      "  Val:   1727 images\n",
      "  Test:  1829 images\n",
      "  Classes: 200\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters (replace args.*)\n",
    "# ============================================================\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "resolution = 224   # or whatever you want for training\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = Path(\"/home/long/code/amogh/data/testset_1\")\n",
    "\n",
    "print(\"\\nLoading dataset metadata...\")\n",
    "train_df = pd.read_csv(data_dir / 'train_labels.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val_labels.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} images\")\n",
    "print(f\"  Val:   {len(val_df)} images\")\n",
    "print(f\"  Test:  {len(test_df)} images\")\n",
    "print(f\"  Classes: {train_df['class_id'].nunique()}\")\n",
    "\n",
    "train_dataset1 = ImageDataset(\n",
    "    data_dir / 'train',\n",
    "    train_df['filename'].tolist(),\n",
    "    train_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=True,\n",
    ")\n",
    "\n",
    "val_dataset1 = ImageDataset(\n",
    "    data_dir / 'val',\n",
    "    val_df['filename'].tolist(),\n",
    "    val_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=True,\n",
    ")\n",
    "\n",
    "test_dataset1 = ImageDataset(\n",
    "    data_dir / 'test',\n",
    "    test_df['filename'].tolist(),\n",
    "    labels=test_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=True,\n",
    ")\n",
    "\n",
    "train_loader1 = DataLoader(\n",
    "    train_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader1 = DataLoader(\n",
    "    val_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader1 = DataLoader(\n",
    "    test_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e71497-2c80-4223-b30d-0533ee502646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8c9ada3-b996-4ddd-b083-102b59ccd5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "586bb79d-14f9-456c-83ac-e1246b11bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from timm.models.vision_transformer import vit_base_patch32_224\n",
    "from torch import nn\n",
    "from lightly.models import utils\n",
    "from lightly.models.modules import MAEDecoderTIMM, MaskedVisionTransformerTIMM\n",
    "from lightly.transforms import MAETransform\n",
    "import copy\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.loss import DINOLoss  # only needed if you re-train SSL\n",
    "from lightly.models.utils import deactivate_requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d556f0f8-1529-4ad7-b756-35fba3d49a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINO(nn.Module):\n",
    "    def __init__(self, backbone, input_dim):\n",
    "        super().__init__()\n",
    "        self.student_backbone = backbone\n",
    "        self.student_head = DINOProjectionHead(\n",
    "            input_dim, 512, 64, 2048, freeze_last_layer=1\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(backbone)\n",
    "        self.teacher_head = DINOProjectionHead(input_dim, 512, 64, 2048)\n",
    "        deactivate_requires_grad(self.teacher_backbone)\n",
    "        deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.student_backbone(x).flatten(start_dim=1)\n",
    "        z = self.student_head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf47d403-477b-4e14-b6af-dde0175adc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "# --- Build same backbone as used for DINO pretraining ---\n",
    "resnet = torchvision.models.resnet18()\n",
    "# resnet = torchvision.models.resnet34()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])  # (B, 512, 1, 1)\n",
    "input_dim = 512\n",
    "\n",
    "dino_model = DINO(backbone, input_dim)\n",
    "\n",
    "# --- Load your pre-trained DINO checkpoint ---\n",
    "ckpt = torch.load(\n",
    "    \"/home/long/code/dl_project1/experiments/outputs/dino-v1/dino-v1_small_100.pt\",\n",
    "    map_location=\"cpu\",\n",
    ")\n",
    "\n",
    "dino_model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dae36a9-0540-4dc5-b4a2-01fddd499bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.loss import DINOLoss\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "from lightly.utils.scheduler import cosine_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdcecb7b-5bbb-4850-a1a0-0c7e88381add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Freeze everything (linear probe only)\n",
    "for p in dino_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "dino_model.eval()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "837cfdc7-c819-4b7e-a544-9796e7311316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200  total classes\n"
     ]
    }
   ],
   "source": [
    "class DINOEncoderWrapper(nn.Module):\n",
    "    \"\"\"Wraps the DINO student backbone and returns a flat feature vector.\"\"\"\n",
    "\n",
    "    def __init__(self, dino_model):\n",
    "        super().__init__()\n",
    "        self.backbone = dino_model.student_backbone\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)          # (B, 512, 1, 1) for ResNet18 backbone\n",
    "        if isinstance(feats, (list, tuple)):\n",
    "            feats = feats[0]\n",
    "        feats = feats.flatten(1)          # (B, 512)\n",
    "        return feats\n",
    "\n",
    "class LinearProbeModel(nn.Module):\n",
    "    def __init__(self, encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():              # encoder frozen\n",
    "            feats = self.encoder(x)\n",
    "            # feats = feats.flatten(1)\n",
    "        logits = self.classifier(feats)\n",
    "        return logits\n",
    "\n",
    "NUM_CLASSES = train_df['class_id'].nunique()\n",
    "\n",
    "print(NUM_CLASSES,\" total classes\")\n",
    "\n",
    "feat_dim   = 512  # ResNet18 backbone\n",
    "classifier = nn.Linear(feat_dim, NUM_CLASSES)\n",
    "# model = dino_model.to(device)\n",
    "encoder = DINOEncoderWrapper(dino_model)\n",
    "model   = LinearProbeModel(encoder, classifier).to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1856c89-b694-4b4c-b633-afbf2e9a673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_496601/3125480575.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Only train the classifier\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "params = list(model.classifier.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params,\n",
    "    lr=3e-3,          # tune between 1e-3 and 3e-3\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=50,         # num_epochs\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13bb5d93-0bf1-41df-a5f7-5a7a10ef5f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Combine train + val ---\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "trainval_ds = ConcatDataset([train_dataset1, val_dataset1])\n",
    "\n",
    "trainval_loader = DataLoader(\n",
    "    trainval_ds,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728f793-196e-4576-a017-9908934c2d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train+val features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train/Val:  96%|█████████████████████████████████████████████████████████████████████████████████▏   | 149/156 [00:16<00:00,  8.42it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "encoder = model.encoder.to(device)\n",
    "encoder.eval()\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(encoder, loader, device, desc=\"Extracting\"):\n",
    "    encoder.eval()\n",
    "    all_feats = []\n",
    "    all_labels = []\n",
    "\n",
    "    # tqdm wrapper around the dataloader\n",
    "    for batch in tqdm(loader, desc=desc):\n",
    "        if len(batch) == 3:\n",
    "            images, labels, _ = batch\n",
    "        else:\n",
    "            images, labels = batch\n",
    "\n",
    "        images = images.to(device, non_blocking=True)\n",
    "\n",
    "        feats = encoder(images)   # shape: (B, C, H, W) or (B, D)\n",
    "        if feats.dim() > 2:\n",
    "            feats = feats.flatten(1)  # (B, D)\n",
    "\n",
    "        all_feats.append(feats.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    all_feats = torch.cat(all_feats, dim=0)   # (N, D)\n",
    "    all_labels = torch.cat(all_labels, dim=0) # (N,)\n",
    "    return all_feats, all_labels\n",
    "\n",
    "\n",
    "print(\"Extracting train+val features...\")\n",
    "trainval_feats, trainval_labels = extract_features(\n",
    "    encoder, trainval_loader, device, desc=\"Train/Val\"\n",
    ")\n",
    "\n",
    "print(\"Extracting test features...\")\n",
    "test_feats, test_labels = extract_features(\n",
    "    encoder, test_loader1, device, desc=\"Test\"\n",
    ")\n",
    "\n",
    "feat_dim = trainval_feats.shape[1]\n",
    "num_classes = int(trainval_labels.max().item() + 1)\n",
    "\n",
    "print(f\"Feature dim = {feat_dim}, num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15721149-f88e-4c87-b860-1408c22ab212",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_feat_ds = TensorDataset(trainval_feats, trainval_labels)\n",
    "test_feat_ds     = TensorDataset(test_feats,     test_labels)\n",
    "\n",
    "feat_batch_size = 512  # can be big, it's cheap now\n",
    "\n",
    "trainval_feat_loader = DataLoader(\n",
    "    trainval_feat_ds, batch_size=feat_batch_size,\n",
    "    shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "test_feat_loader = DataLoader(\n",
    "    test_feat_ds, batch_size=feat_batch_size,\n",
    "    shuffle=False, num_workers=0\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
