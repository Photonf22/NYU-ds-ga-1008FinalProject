{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d4a64-ba34-4b86-af65-5ac9e19b7540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import argparse\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc7ccae-af4c-4557-8936-1463d1815d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_list, labels=None,\n",
    "                 resolution=224, split=\"train\", apply_transforms=True):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.image_list = image_list\n",
    "        self.labels = labels\n",
    "        self.split = split\n",
    "        self.resolution = resolution\n",
    "        self.apply_transforms = apply_transforms\n",
    "\n",
    "        imagenet_mean = [0.485, 0.456, 0.406]\n",
    "        imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        if apply_transforms:\n",
    "            if split == \"train\":\n",
    "                self.transform = v2.Compose([\n",
    "                    v2.RandomResizedCrop(resolution, scale=(0.8, 1.0)),\n",
    "                    v2.RandomHorizontalFlip(p=0.5),\n",
    "                    v2.ColorJitter(\n",
    "                        brightness=0.4,\n",
    "                        contrast=0.4,\n",
    "                        saturation=0.4,\n",
    "                        hue=0.1\n",
    "                    ),\n",
    "                    v2.ToImage(),\n",
    "                    v2.ToDtype(torch.float32, scale=True),\n",
    "                    v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = v2.Compose([\n",
    "                    v2.Resize(256),\n",
    "                    v2.CenterCrop(resolution),\n",
    "                    v2.ToImage(),\n",
    "                    v2.ToDtype(torch.float32, scale=True),\n",
    "                    v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_list[idx]\n",
    "        img_path = self.image_dir / img_name\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.labels is not None:\n",
    "            if self.split == \"test\":\n",
    "                # For test: return (img, label, idx) so we can align with test_df\n",
    "                return img, self.labels[idx], idx\n",
    "            else:\n",
    "                # For train/val: keep old behavior (img, label, img_name)\n",
    "                return img, self.labels[idx], img_name\n",
    "\n",
    "        # Unlabeled case (e.g., SSL)\n",
    "        if self.split == \"test\":\n",
    "            return img, idx\n",
    "        else:\n",
    "            return img, img_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1c40e-1ed3-4236-a171-588ce5effbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters (replace args.*)\n",
    "# ============================================================\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "resolution = 224   # or whatever you want for training\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = Path(\"/home/long/code/amogh/data/testset_2\")\n",
    "\n",
    "print(\"\\nLoading dataset metadata...\")\n",
    "train_df = pd.read_csv(data_dir / 'train_labels.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val_labels.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} images\")\n",
    "print(f\"  Val:   {len(val_df)} images\")\n",
    "print(f\"  Test:  {len(test_df)} images\")\n",
    "print(f\"  Classes: {train_df['class_id'].nunique()}\")\n",
    "\n",
    "train_dataset1 = ImageDataset(\n",
    "    data_dir / 'train',\n",
    "    train_df['filename'].tolist(),\n",
    "    train_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=True,\n",
    ")\n",
    "\n",
    "val_dataset1 = ImageDataset(\n",
    "    data_dir / 'val',\n",
    "    val_df['filename'].tolist(),\n",
    "    val_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=True,\n",
    ")\n",
    "\n",
    "test_dataset1 = ImageDataset(\n",
    "    data_dir / 'test',\n",
    "    test_df['filename'].tolist(),\n",
    "    labels=test_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=True,\n",
    "    split=\"test\"\n",
    ")\n",
    "\n",
    "train_loader1 = DataLoader(\n",
    "    train_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    # collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader1 = DataLoader(\n",
    "    val_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    # collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader1 = DataLoader(\n",
    "    test_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    # collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c9ada3-b996-4ddd-b083-102b59ccd5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586bb79d-14f9-456c-83ac-e1246b11bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from timm.models.vision_transformer import vit_base_patch32_224\n",
    "from torch import nn\n",
    "from lightly.models import utils\n",
    "from lightly.models.modules import MAEDecoderTIMM, MaskedVisionTransformerTIMM\n",
    "from lightly.transforms import MAETransform\n",
    "import copy\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.loss import DINOLoss  # only needed if you re-train SSL\n",
    "from lightly.models.utils import deactivate_requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556f0f8-1529-4ad7-b756-35fba3d49a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINO(nn.Module):\n",
    "    def __init__(self, backbone, input_dim):\n",
    "        super().__init__()\n",
    "        self.student_backbone = backbone\n",
    "        self.student_head = DINOProjectionHead(\n",
    "            input_dim, 512, 64, 2048, freeze_last_layer=1\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(backbone)\n",
    "        self.teacher_head = DINOProjectionHead(input_dim, 512, 64, 2048)\n",
    "        deactivate_requires_grad(self.teacher_backbone)\n",
    "        deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.student_backbone(x).flatten(start_dim=1)\n",
    "        z = self.student_head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47d403-477b-4e14-b6af-dde0175adc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# --- Build same backbone as used for DINO pretraining ---\n",
    "resnet = torchvision.models.resnet18()\n",
    "# resnet = torchvision.models.resnet34()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])  # (B, 512, 1, 1)\n",
    "input_dim = 512\n",
    "\n",
    "dino_model = DINO(backbone, input_dim)\n",
    "\n",
    "# --- Load your pre-trained DINO checkpoint ---\n",
    "# ckpt = torch.load(\n",
    "#     \"/home/long/code/dl_project1/experiments/outputs/dino-v1/dino-v1_small_100.pt\",\n",
    "#     map_location=\"cpu\",\n",
    "# )\n",
    "\n",
    "# ckpt = torch.load(\n",
    "#     \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_18.pt\",\n",
    "#     map_location=\"cpu\",\n",
    "# )\n",
    "\n",
    "# ckpt18_path = \n",
    "ckpt = torch.load(\n",
    "    \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data3.pt\",\n",
    "    map_location=\"cpu\",\n",
    ")\n",
    "\n",
    "dino_model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dae36a9-0540-4dc5-b4a2-01fddd499bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.loss import DINOLoss\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "from lightly.utils.scheduler import cosine_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcecb7b-5bbb-4850-a1a0-0c7e88381add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze everything (linear probe only)\n",
    "for p in dino_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "dino_model.eval()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be510667-fc3a-46e8-8d21-55ad73ba99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only train the classifier\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837cfdc7-c819-4b7e-a544-9796e7311316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOEncoderWrapper(nn.Module):\n",
    "    \"\"\"Wraps the DINO student backbone and returns a flat feature vector.\"\"\"\n",
    "\n",
    "    def __init__(self, dino_model):\n",
    "        super().__init__()\n",
    "        self.backbone = dino_model.student_backbone\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)          # (B, 512, 1, 1) for ResNet18 backbone\n",
    "        if isinstance(feats, (list, tuple)):\n",
    "            feats = feats[0]\n",
    "        feats = feats.flatten(1)          # (B, 512)\n",
    "        return feats\n",
    "\n",
    "class LinearProbeModel(nn.Module):\n",
    "    def __init__(self, encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():              # encoder frozen\n",
    "            feats = self.encoder(x)\n",
    "            # feats = feats.flatten(1)\n",
    "        logits = self.classifier(feats)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DINOEncoderWrapper(nn.Module):\n",
    "    \"\"\"Wraps the DINO student backbone and returns a 1024-dim flat feature vector.\"\"\"\n",
    "    def __init__(self, dino_model, in_dim=512, proj_dim=1024):\n",
    "        super().__init__()\n",
    "        self.backbone = dino_model.student_backbone\n",
    "\n",
    "        # ----- Fixed, non-trainable projection: (in_dim -> proj_dim) -----\n",
    "        proj = torch.randn(in_dim, proj_dim) / math.sqrt(in_dim)\n",
    "        # register as a buffer so it's moved with .to(device) but NOT a parameter\n",
    "        self.register_buffer(\"proj\", proj)\n",
    "\n",
    "        # (optional) also freeze backbone params explicitly\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)          # (B, 512, 1, 1) for ResNet18 backbone\n",
    "        if isinstance(feats, (list, tuple)):\n",
    "            feats = feats[0]\n",
    "        feats = feats.flatten(1)          # (B, 512)\n",
    "        feats = feats @ self.proj         # (B, 1024), fixed linear map\n",
    "        return feats\n",
    "\n",
    "\n",
    "NUM_CLASSES = train_df['class_id'].nunique()\n",
    "\n",
    "print(NUM_CLASSES,\" total classes\")\n",
    "\n",
    "# feat_dim   = 512  # ResNet18 backbone\n",
    "feat_dim   = 512  # ResNet18 backbone\n",
    "classifier = nn.Linear(feat_dim, NUM_CLASSES)\n",
    "# model = dino_model.to(device)\n",
    "encoder = DINOEncoderWrapper(dino_model)\n",
    "model   = LinearProbeModel(encoder, classifier).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270336e9-217d-4400-98d1-ba13b9ff36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------------------------------\n",
    "# 1. DINO encoder wrapper (same idea as yours)\n",
    "# -------------------------------\n",
    "class DINOEncoderWrapper(nn.Module):\n",
    "    \"\"\"Wraps the DINO student backbone and returns a flat feature vector.\"\"\"\n",
    "    def __init__(self, dino_model):\n",
    "        super().__init__()\n",
    "        self.backbone = dino_model.student_backbone\n",
    "\n",
    "        # (optional) extra safety: freeze backbone params\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)          # (B, 512, 1, 1) for ResNet18/34\n",
    "        if isinstance(feats, (list, tuple)):\n",
    "            feats = feats[0]\n",
    "        feats = feats.flatten(1)          # (B, 512)\n",
    "        return feats\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Build DINO18 and DINO34 models from ckpts\n",
    "# -------------------------------\n",
    "# paths to your checkpoints\n",
    "# ckpt18_path = \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_18.pt\"\n",
    "# ckpt18_path = \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data2.pt\"\n",
    "ckpt18_path = \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data3.pt\"\n",
    "\n",
    "ckpt34_path = \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_big_boi_test_fixed.pt\"\n",
    "\n",
    "# ---- ResNet18 backbone ----\n",
    "resnet18 = torchvision.models.resnet18()\n",
    "backbone18 = nn.Sequential(*list(resnet18.children())[:-1])  # (B, 512, 1, 1)\n",
    "in_dim18 = 512\n",
    "\n",
    "dino18_model = DINO(backbone18, in_dim18)\n",
    "ckpt18 = torch.load(ckpt18_path, map_location=\"cpu\")\n",
    "dino18_model.load_state_dict(ckpt18[\"model_state\"], strict=True)\n",
    "dino18_model.to(device)\n",
    "\n",
    "# ---- ResNet34 backbone ----\n",
    "resnet34 = torchvision.models.resnet34()\n",
    "backbone34 = nn.Sequential(*list(resnet34.children())[:-1])  # (B, 512, 1, 1)\n",
    "in_dim34 = 512\n",
    "\n",
    "dino34_model = DINO(backbone34, in_dim34)\n",
    "ckpt34 = torch.load(ckpt34_path, map_location=\"cpu\")\n",
    "dino34_model.load_state_dict(ckpt34[\"model_state\"], strict=True)\n",
    "dino34_model.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Wrap each into an encoder\n",
    "# -------------------------------\n",
    "encoder18 = DINOEncoderWrapper(dino18_model).to(device)\n",
    "encoder34 = DINOEncoderWrapper(dino34_model).to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Concat encoder\n",
    "# -------------------------------\n",
    "class ConcatEncoder(nn.Module):\n",
    "    \"\"\"Runs both encoders and concatenates their features.\"\"\"\n",
    "    def __init__(self, enc18, enc34):\n",
    "        super().__init__()\n",
    "        self.enc18 = enc18\n",
    "        self.enc34 = enc34\n",
    "\n",
    "        # freeze everything inside concat encoder (just in case)\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        f18 = self.enc18(x)   # (B, 512)\n",
    "        f34 = self.enc34(x)   # (B, 512)\n",
    "        return torch.cat([f18, f34], dim=1)  # (B, 1024)\n",
    "\n",
    "concat_encoder = ConcatEncoder(encoder18, encoder34).to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Linear probe on top of concatenated features\n",
    "# -------------------------------\n",
    "NUM_CLASSES = train_df[\"class_id\"].nunique()\n",
    "feat_dim_concat = 512 * 2  # 1024\n",
    "\n",
    "classifier_concat = nn.Linear(feat_dim_concat, NUM_CLASSES).to(device)\n",
    "\n",
    "class LinearProbeModel(nn.Module):\n",
    "    def __init__(self, encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():          # encoder frozen\n",
    "            feats = self.encoder(x)    # (B, 1024)\n",
    "        logits = self.classifier(feats)\n",
    "        return logits\n",
    "\n",
    "model = LinearProbeModel(concat_encoder, classifier_concat).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e6985-b2dc-4825-83fc-f03a9882102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------------------------------\n",
    "# 1. DINO encoder wrapper\n",
    "# -------------------------------\n",
    "class DINOEncoderWrapper(nn.Module):\n",
    "    \"\"\"Wraps the DINO student backbone and returns a flat feature vector.\"\"\"\n",
    "    def __init__(self, dino_model):\n",
    "        super().__init__()\n",
    "        self.backbone = dino_model.student_backbone\n",
    "\n",
    "        # freeze backbone params\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)          # (B, 512, 1, 1)\n",
    "        if isinstance(feats, (list, tuple)):\n",
    "            feats = feats[0]\n",
    "        feats = feats.flatten(1)          # (B, 512)\n",
    "        return feats\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Build DINO18 (A), DINO18 (B) and DINO34 from ckpts\n",
    "# -------------------------------\n",
    "# ckpt18_a_path = \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data2.pt\"\n",
    "ckpt18_a_path = \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data3.pt\"\n",
    "\n",
    "ckpt18_b_path = \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed.pt\"\n",
    "ckpt34_path   = \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_big_boi_test_fixed.pt\"\n",
    "\n",
    "# ---- ResNet18 backbone A ----\n",
    "resnet18_a = torchvision.models.resnet18()\n",
    "backbone18_a = nn.Sequential(*list(resnet18_a.children())[:-1])  # (B, 512, 1, 1)\n",
    "in_dim18 = 512\n",
    "\n",
    "dino18_a = DINO(backbone18_a, in_dim18)\n",
    "ckpt18_a = torch.load(ckpt18_a_path, map_location=\"cpu\")\n",
    "dino18_a.load_state_dict(ckpt18_a[\"model_state\"], strict=True)\n",
    "dino18_a.to(device)\n",
    "\n",
    "# ---- ResNet18 backbone B (second checkpoint) ----\n",
    "resnet18_b = torchvision.models.resnet18()\n",
    "backbone18_b = nn.Sequential(*list(resnet18_b.children())[:-1])\n",
    "dino18_b = DINO(backbone18_b, in_dim18)\n",
    "ckpt18_b = torch.load(ckpt18_b_path, map_location=\"cpu\")\n",
    "dino18_b.load_state_dict(ckpt18_b[\"model_state\"], strict=True)\n",
    "dino18_b.to(device)\n",
    "\n",
    "# ---- ResNet34 backbone ----\n",
    "resnet34 = torchvision.models.resnet34()\n",
    "backbone34 = nn.Sequential(*list(resnet34.children())[:-1])  # (B, 512, 1, 1)\n",
    "in_dim34 = 512\n",
    "\n",
    "dino34 = DINO(backbone34, in_dim34)\n",
    "ckpt34 = torch.load(ckpt34_path, map_location=\"cpu\")\n",
    "dino34.load_state_dict(ckpt34[\"model_state\"], strict=True)\n",
    "dino34.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Wrap each into an encoder\n",
    "# -------------------------------\n",
    "encoder18_a = DINOEncoderWrapper(dino18_a).to(device)\n",
    "encoder18_b = DINOEncoderWrapper(dino18_b).to(device)\n",
    "encoder34   = DINOEncoderWrapper(dino34).to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Concat encoder (3-way)\n",
    "# -------------------------------\n",
    "class ConcatEncoder(nn.Module):\n",
    "    \"\"\"Runs three encoders and concatenates their features.\"\"\"\n",
    "    def __init__(self, enc18_a, enc18_b, enc34):\n",
    "        super().__init__()\n",
    "        self.enc18_a = enc18_a\n",
    "        self.enc18_b = enc18_b\n",
    "        self.enc34   = enc34\n",
    "\n",
    "        # freeze everything inside concat encoder\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        f18_a = self.enc18_a(x)   # (B, 512)\n",
    "        f18_b = self.enc18_b(x)   # (B, 512)\n",
    "        f34   = self.enc34(x)     # (B, 512)\n",
    "        return torch.cat([f18_a, f18_b, f34], dim=1)  # (B, 1536)\n",
    "\n",
    "concat_encoder = ConcatEncoder(encoder18_a, encoder18_b, encoder34).to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Linear probe on top of concatenated features\n",
    "# -------------------------------\n",
    "NUM_CLASSES = train_df[\"class_id\"].nunique()\n",
    "feat_dim_concat = 512 * 3  # 1536\n",
    "\n",
    "classifier_concat = nn.Linear(feat_dim_concat, NUM_CLASSES).to(device)\n",
    "\n",
    "class LinearProbeModel(nn.Module):\n",
    "    def __init__(self, encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():          # encoder frozen\n",
    "            feats = self.encoder(x)    # (B, 1536)\n",
    "        logits = self.classifier(feats)\n",
    "        return logits\n",
    "\n",
    "model = LinearProbeModel(concat_encoder, classifier_concat).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460fd10-8867-494f-b42c-98575dca0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all encoders\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------------------------------\n",
    "# 1. DINO encoder wrapper\n",
    "# -------------------------------\n",
    "class DINOEncoderWrapper(nn.Module):\n",
    "    \"\"\"Wraps the DINO student backbone and returns a flat feature vector.\"\"\"\n",
    "    def __init__(self, dino_model):\n",
    "        super().__init__()\n",
    "        self.backbone = dino_model.student_backbone\n",
    "\n",
    "        # freeze backbone params\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)          # (B, 512, 1, 1)\n",
    "        if isinstance(feats, (list, tuple)):\n",
    "            feats = feats[0]\n",
    "        feats = feats.flatten(1)          # (B, 512)\n",
    "        return feats\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Build DINO18 (A,B,C,D) and DINO34 from ckpts\n",
    "# -------------------------------\n",
    "ckpt18_a_path = \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data3.pt\"\n",
    "ckpt18_b_path = \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed.pt\"\n",
    "ckpt18_c_path = \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data2.pt\"\n",
    "ckpt18_d_path = \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_18_test2fixed_data1_data1againidk.pt\"\n",
    "ckpt34_path   = \"/home/long/code/amogh/data/models/dino-v1_full_finetuned_big_boi_test_fixed.pt\"\n",
    "\n",
    "in_dim18 = 512\n",
    "in_dim34 = 512\n",
    "\n",
    "# ---- ResNet18 backbone A ----\n",
    "resnet18_a = torchvision.models.resnet18()\n",
    "backbone18_a = nn.Sequential(*list(resnet18_a.children())[:-1])  # (B, 512, 1, 1)\n",
    "dino18_a = DINO(backbone18_a, in_dim18)\n",
    "ckpt18_a = torch.load(ckpt18_a_path, map_location=\"cpu\")\n",
    "dino18_a.load_state_dict(ckpt18_a[\"model_state\"], strict=True)\n",
    "dino18_a.to(device)\n",
    "\n",
    "# ---- ResNet18 backbone B ----\n",
    "resnet18_b = torchvision.models.resnet18()\n",
    "backbone18_b = nn.Sequential(*list(resnet18_b.children())[:-1])\n",
    "dino18_b = DINO(backbone18_b, in_dim18)\n",
    "ckpt18_b = torch.load(ckpt18_b_path, map_location=\"cpu\")\n",
    "dino18_b.load_state_dict(ckpt18_b[\"model_state\"], strict=True)\n",
    "dino18_b.to(device)\n",
    "\n",
    "# ---- ResNet18 backbone C ----\n",
    "resnet18_c = torchvision.models.resnet18()\n",
    "backbone18_c = nn.Sequential(*list(resnet18_c.children())[:-1])\n",
    "dino18_c = DINO(backbone18_c, in_dim18)\n",
    "ckpt18_c = torch.load(ckpt18_c_path, map_location=\"cpu\")\n",
    "dino18_c.load_state_dict(ckpt18_c[\"model_state\"], strict=True)\n",
    "dino18_c.to(device)\n",
    "\n",
    "# ---- ResNet18 backbone D ----\n",
    "resnet18_d = torchvision.models.resnet18()\n",
    "backbone18_d = nn.Sequential(*list(resnet18_d.children())[:-1])\n",
    "dino18_d = DINO(backbone18_d, in_dim18)\n",
    "ckpt18_d = torch.load(ckpt18_d_path, map_location=\"cpu\")\n",
    "dino18_d.load_state_dict(ckpt18_d[\"model_state\"], strict=True)\n",
    "dino18_d.to(device)\n",
    "\n",
    "# ---- ResNet34 backbone ----\n",
    "resnet34 = torchvision.models.resnet34()\n",
    "backbone34 = nn.Sequential(*list(resnet34.children())[:-1])  # (B, 512, 1, 1)\n",
    "dino34 = DINO(backbone34, in_dim34)\n",
    "ckpt34 = torch.load(ckpt34_path, map_location=\"cpu\")\n",
    "dino34.load_state_dict(ckpt34[\"model_state\"], strict=True)\n",
    "dino34.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Wrap each into an encoder\n",
    "# -------------------------------\n",
    "encoder18_a = DINOEncoderWrapper(dino18_a).to(device)\n",
    "encoder18_b = DINOEncoderWrapper(dino18_b).to(device)\n",
    "encoder18_c = DINOEncoderWrapper(dino18_c).to(device)\n",
    "encoder18_d = DINOEncoderWrapper(dino18_d).to(device)\n",
    "encoder34   = DINOEncoderWrapper(dino34).to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Concat encoder (5-way)\n",
    "# -------------------------------\n",
    "class ConcatEncoder(nn.Module):\n",
    "    \"\"\"Runs five encoders and concatenates their features.\"\"\"\n",
    "    def __init__(self, enc18_a, enc18_b, enc18_c, enc18_d, enc34):\n",
    "        super().__init__()\n",
    "        self.enc18_a = enc18_a\n",
    "        self.enc18_b = enc18_b\n",
    "        self.enc18_c = enc18_c\n",
    "        self.enc18_d = enc18_d\n",
    "        self.enc34   = enc34\n",
    "\n",
    "        # freeze everything inside concat encoder\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        f18_a = self.enc18_a(x)   # (B, 512)\n",
    "        f18_b = self.enc18_b(x)   # (B, 512)\n",
    "        f18_c = self.enc18_c(x)   # (B, 512)\n",
    "        f18_d = self.enc18_d(x)   # (B, 512)\n",
    "        f34   = self.enc34(x)     # (B, 512)\n",
    "        return torch.cat([f18_a, f18_b, f18_c, f18_d, f34], dim=1)  # (B, 2560)\n",
    "\n",
    "concat_encoder = ConcatEncoder(\n",
    "    encoder18_a, encoder18_b, encoder18_c, encoder18_d, encoder34\n",
    ").to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Linear probe on top of concatenated features\n",
    "# -------------------------------\n",
    "NUM_CLASSES = train_df[\"class_id\"].nunique()\n",
    "feat_dim_concat = 512 * 5  # 2560\n",
    "\n",
    "classifier_concat = nn.Linear(feat_dim_concat, NUM_CLASSES).to(device)\n",
    "\n",
    "class LinearProbeModel(nn.Module):\n",
    "    def __init__(self, encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():          # encoder frozen\n",
    "            feats = self.encoder(x)    # (B, 2560)\n",
    "        logits = self.classifier(feats)\n",
    "        return logits\n",
    "\n",
    "model = LinearProbeModel(concat_encoder, classifier_concat).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3047e016-7b3e-4e8f-b933-97cf59015039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32329774-0cd7-47d7-99a7-5280efa87952",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = count_params(concat_encoder)\n",
    "print(\"Total parameters in concat encoder:\", total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1856c89-b694-4b4c-b633-afbf2e9a673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "params = list(model.classifier.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params,\n",
    "    lr=3e-3,          # tune between 1e-3 and 3e-3\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=50,         # num_epochs\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb5d93-0bf1-41df-a5f7-5a7a10ef5f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Combine train + val ---\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "trainval_ds = ConcatDataset([train_dataset1, val_dataset1])\n",
    "\n",
    "trainval_loader = DataLoader(\n",
    "    trainval_ds,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c394b6a5-80f2-4630-9d18-0bf7584b5b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only train the classifier\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "params = list(model.classifier.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params,\n",
    "    lr=3e-3,          # tune between 1e-3 and 3e-3\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=50,         # num_epochs\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728f793-196e-4576-a017-9908934c2d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "encoder = model.encoder.to(device)\n",
    "encoder.eval()\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(encoder, loader, device, desc=\"Extracting\"):\n",
    "    encoder.eval()\n",
    "    all_feats = []\n",
    "    all_labels = []\n",
    "\n",
    "    # tqdm wrapper around the dataloader\n",
    "    for batch in tqdm(loader, desc=desc):\n",
    "        if len(batch) == 3:\n",
    "            images, labels, _ = batch\n",
    "        else:\n",
    "            images, labels = batch\n",
    "\n",
    "        images = images.to(device, non_blocking=True)\n",
    "\n",
    "        feats = encoder(images)   # shape: (B, C, H, W) or (B, D)\n",
    "        if feats.dim() > 2:\n",
    "            feats = feats.flatten(1)  # (B, D)\n",
    "\n",
    "        all_feats.append(feats.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    all_feats = torch.cat(all_feats, dim=0)   # (N, D)\n",
    "    all_labels = torch.cat(all_labels, dim=0) # (N,)\n",
    "    return all_feats, all_labels\n",
    "\n",
    "\n",
    "print(\"Extracting train+val features...\")\n",
    "trainval_feats, trainval_labels = extract_features(\n",
    "    encoder, trainval_loader, device, desc=\"Train/Val\"\n",
    ")\n",
    "\n",
    "print(\"Extracting test features...\")\n",
    "test_feats, test_labels = extract_features(\n",
    "    encoder, test_loader1, device, desc=\"Test\"\n",
    ")\n",
    "\n",
    "feat_dim = trainval_feats.shape[1]\n",
    "num_classes = int(trainval_labels.max().item() + 1)\n",
    "\n",
    "print(f\"Feature dim = {feat_dim}, num_classes = {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15721149-f88e-4c87-b860-1408c22ab212",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_feat_ds = TensorDataset(trainval_feats, trainval_labels)\n",
    "test_feat_ds     = TensorDataset(test_feats,     test_labels)\n",
    "\n",
    "feat_batch_size = 512  # can be big, it's cheap now\n",
    "# feat_batch_size = 512  # can be big, it's cheap now\n",
    "\n",
    "trainval_feat_loader = DataLoader(\n",
    "    trainval_feat_ds, batch_size=feat_batch_size,\n",
    "    shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "test_feat_loader = DataLoader(\n",
    "    test_feat_ds, batch_size=feat_batch_size,\n",
    "    shuffle=False, num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b090a047-2d87-434d-8c4e-a3bc0fdc3559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# linear_head = nn.Linear(feat_dim, num_classes).to(device)\n",
    "import torch.nn as nn\n",
    "\n",
    "dropout_p = 0.1  # try 0.2 / 0.3 / 0.5\n",
    "\n",
    "linear_head = nn.Sequential(\n",
    "    nn.Dropout(p=dropout_p),\n",
    "    nn.Linear(feat_dim, num_classes),\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(linear_head.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
    "scaler = torch.cuda.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8616577-a99e-4d8e-bf6a-214bd1c6efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch_head(head, loader, train=True):\n",
    "    if train:\n",
    "        head.train()\n",
    "    else:\n",
    "        head.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for feats, labels in loader:\n",
    "        feats = feats.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = head(feats)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    acc = 100.0 * correct / total\n",
    "    return avg_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4465fb99-90de-4def-ac9d-4bbc285e71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "best_test_acc = 0.0\n",
    "best_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bf458-248b-444a-8d4e-45ab4e78272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 500\n",
    "# best_test_acc = 0.0\n",
    "# best_epoch = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = run_epoch_head(linear_head, trainval_feat_loader, train=True)\n",
    "    test_loss, test_acc   = run_epoch_head(linear_head, test_feat_loader,   train=False)\n",
    "    scheduler.step()\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch\n",
    "        # you can save the head weights here if you want\n",
    "        torch.save(linear_head.state_dict(), \"best_linear_head.pt\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"train loss: {train_loss:.4f}, acc: {train_acc:.2f}% | \"\n",
    "        f\"test loss: {test_loss:.4f}, acc: {test_acc:.2f}%\"\n",
    "    )\n",
    "\n",
    "print(f\"BEST TEST ACC = {best_test_acc:.2f}% at epoch {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee7ace-bc86-4edd-a0a9-3a0c4cae25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def create_submission_from_features(head, test_feats, test_df, device, output_dir,\n",
    "                                    batch_size=256):\n",
    "    \"\"\"\n",
    "    Uses precomputed test_feats (N, D) and a trained head to create submission.csv.\n",
    "    This exactly matches the sanity-checked evaluation.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    head = head.to(device).eval()\n",
    "\n",
    "    N = test_feats.shape[0]\n",
    "    assert N == len(test_df), f\"Mismatch: test_feats={N}, test_df={len(test_df)}\"\n",
    "\n",
    "    # Dataset: (feats, idx) so we preserve order\n",
    "    idx_tensor = torch.arange(N, dtype=torch.long)\n",
    "    ds = TensorDataset(test_feats, idx_tensor)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    pred_array = torch.zeros(N, dtype=torch.long)\n",
    "\n",
    "    for feats_batch, idx in tqdm(loader, desc=\"Submission (features)\"):\n",
    "        feats_batch = feats_batch.to(device, non_blocking=True)\n",
    "        logits = head(feats_batch)\n",
    "        preds  = torch.argmax(logits, dim=1).cpu()\n",
    "        pred_array[idx] = preds\n",
    "\n",
    "    # optional sanity: check class distribution\n",
    "    bincount = torch.bincount(pred_array)\n",
    "    print(\"Prediction class counts:\", bincount.tolist())\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": test_df[\"filename\"].values,\n",
    "        \"class_id\": pred_array.numpy(),\n",
    "    })\n",
    "\n",
    "    out_path = output_dir / \"submission.csv\"\n",
    "    submission.to_csv(out_path, index=False)\n",
    "    print(f\"\\nSubmission written to: {out_path}\")\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807975c-53f9-4dc1-ab77-d887af247391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7664cac-d651-40fa-b9fb-6de223ea76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# IMPORTANT: make sure you're using the trained/best head\n",
    "linear_head.load_state_dict(torch.load(\"best_linear_head.pt\", map_location=device))\n",
    "\n",
    "submission = create_submission_from_features(\n",
    "    head=linear_head,\n",
    "    test_feats=test_feats,       # from your extract_features(...)\n",
    "    test_df=test_df,\n",
    "    device=device,\n",
    "    output_dir=\".\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f137fb-4d83-446f-b9c3-413f94b95eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_head.eval()\n",
    "with torch.no_grad():\n",
    "    logits = linear_head(test_feats.to(device))\n",
    "    preds  = torch.argmax(logits, dim=1).cpu()\n",
    "\n",
    "correct = (preds == test_labels).sum().item()\n",
    "acc = 100.0 * correct / len(test_labels)\n",
    "print(f\"Sanity check on stored test_feats: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90c464-5c05-4def-aaec-b3674a862ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def create_submission(encoder, head, test_loader, test_df, device, output_dir):\n",
    "    \"\"\"\n",
    "    Runs encoder + head on test_loader and writes submission.csv,\n",
    "    guaranteeing exact 1:1 alignment with test_df.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    encoder = encoder.to(device).eval()\n",
    "    head    = head.to(device).eval()\n",
    "\n",
    "    N = len(test_df)\n",
    "    pred_array = torch.zeros(N, dtype=torch.long)\n",
    "\n",
    "    for batch in tqdm(test_loader, desc=\"Test inference (order-safe)\"):\n",
    "        # After the ImageDataset change, test batch = (images, labels, idx)\n",
    "        images, _, idx = batch\n",
    "\n",
    "        images = images.to(device, non_blocking=True)\n",
    "\n",
    "        feats = encoder(images)\n",
    "        if feats.dim() > 2:\n",
    "            feats = feats.flatten(1)\n",
    "\n",
    "        logits = head(feats)\n",
    "        preds  = torch.argmax(logits, dim=1).cpu()\n",
    "\n",
    "        # Place predictions into the correct positions\n",
    "        pred_array[idx] = preds\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": test_df[\"filename\"],      # matches the Kaggle-style column\n",
    "        \"class_id\": pred_array.numpy(), # one prediction per row\n",
    "    })\n",
    "\n",
    "    out_path = output_dir / \"submission.csv\"\n",
    "    submission.to_csv(out_path, index=False)\n",
    "    print(f\"\\nSubmission written to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c60b7c-7481-4d88-9ac2-558237bde3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = model.encoder     # frozen encoder from your LinearProbeModel\n",
    "head    = linear_head       # or model.classifier, whichever you trained\n",
    "\n",
    "create_submission(\n",
    "    encoder=encoder,\n",
    "    head=head,\n",
    "    test_loader=test_loader1,\n",
    "    test_df=test_df,\n",
    "    device=device,\n",
    "    output_dir=\".\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b74d2-ad17-4cfa-82a7-47e3041f7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def create_submission(\n",
    "    test_loader,\n",
    "    head,                         # this is your linear_head\n",
    "    output_path=\"submission.csv\",\n",
    "    device=\"cuda\",\n",
    "    encoder=None,                 # <-- NEW: optional encoder\n",
    "):\n",
    "    \"\"\"\n",
    "    Create submission.csv from test_loader and a trained linear head.\n",
    "\n",
    "    Args:\n",
    "        test_loader:\n",
    "            - If encoder is not None:\n",
    "                yields (images, filenames) or (images, labels, filenames)\n",
    "                images: preprocessed tensors ready for encoder\n",
    "            - If encoder is None:\n",
    "                yields (features, filenames) or (features, labels, filenames)\n",
    "                features: outputs of encoder (flattened)\n",
    "        head: trained linear head; takes encoder features as input and outputs logits\n",
    "        encoder: optional frozen encoder. If provided, we do:\n",
    "                 feats = encoder(images); feats = feats.flatten(1) if needed\n",
    "        output_path: where to save submission CSV\n",
    "        device: 'cuda' or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "        submission_df: DataFrame with 'id' and 'class_id'\n",
    "        accuracy: test accuracy if labels are present, else None\n",
    "    \"\"\"\n",
    "\n",
    "    device = device if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    head = head.to(device)\n",
    "    head.eval()\n",
    "\n",
    "    if encoder is not None:\n",
    "        encoder = encoder.to(device)\n",
    "        encoder.eval()\n",
    "        for p in encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # Check if dataloader is shuffled (common issue)\n",
    "    if hasattr(test_loader, \"sampler\") and hasattr(test_loader.sampler, \"shuffle\"):\n",
    "        if test_loader.sampler.shuffle:\n",
    "            print(\"⚠️  WARNING: test_loader appears to be shuffled!\")\n",
    "            print(\"Make sure shuffle=False for submission.\")\n",
    "\n",
    "    all_predictions = []\n",
    "    all_filenames = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    has_labels = False\n",
    "\n",
    "    print(\"Generating predictions...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Inference\"):\n",
    "            # -------------------------\n",
    "            # 1. Unpack batch\n",
    "            # -------------------------\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                if len(batch) == 3:\n",
    "                    x, labels, filenames = batch\n",
    "                    has_labels = True\n",
    "                elif len(batch) == 2:\n",
    "                    x, second = batch\n",
    "                    # Heuristic: tensor → labels, list/str → filenames\n",
    "                    if isinstance(second, torch.Tensor):\n",
    "                        x, labels = batch\n",
    "                        filenames = None\n",
    "                        has_labels = True\n",
    "                    else:\n",
    "                        x, filenames = batch\n",
    "                        labels = None\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected batch length: {len(batch)}\")\n",
    "            else:\n",
    "                # Dict style (not typical in your code, but kept for safety)\n",
    "                x = batch[\"image\"]\n",
    "                filenames = batch.get(\"filename\", batch.get(\"id\"))\n",
    "                labels = batch.get(\"label\", batch.get(\"class_id\"))\n",
    "                if labels is not None:\n",
    "                    has_labels = True\n",
    "\n",
    "            # -------------------------\n",
    "            # 2. Move to device\n",
    "            # -------------------------\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                x = x.to(device, non_blocking=True)\n",
    "\n",
    "            if labels is not None and isinstance(labels, torch.Tensor):\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            # -------------------------\n",
    "            # 3. Forward: encoder (optional) + head\n",
    "            # -------------------------\n",
    "            with torch.cuda.amp.autocast():\n",
    "                if encoder is not None:\n",
    "                    feats = encoder(x)\n",
    "                    if feats.dim() > 2:\n",
    "                        feats = feats.flatten(1)\n",
    "                    logits = head(feats)\n",
    "                else:\n",
    "                    # x is already features\n",
    "                    logits = head(x)\n",
    "\n",
    "            # -------------------------\n",
    "            # 4. Predictions\n",
    "            # -------------------------\n",
    "            if logits.dim() == 2:\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "            else:  # already predictions\n",
    "                preds = logits\n",
    "\n",
    "            # accuracy if labels available\n",
    "            if labels is not None:\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "            # -------------------------\n",
    "            # 5. Filenames\n",
    "            # -------------------------\n",
    "            clean_names = []\n",
    "\n",
    "            if filenames is None:\n",
    "                # If you ever call this on a feature-loader without filenames,\n",
    "                # you should instead pass filenames separately or avoid CSV here.\n",
    "                raise ValueError(\n",
    "                    \"Filenames are None. For submission, test_loader must yield filenames.\"\n",
    "                )\n",
    "\n",
    "            if isinstance(filenames, torch.Tensor):\n",
    "                for f in filenames.cpu().tolist():\n",
    "                    clean_names.append(os.path.basename(str(f)))\n",
    "            elif isinstance(filenames, list):\n",
    "                for f in filenames:\n",
    "                    clean_names.append(os.path.basename(str(f)))\n",
    "            else:  # single string\n",
    "                clean_names.append(os.path.basename(str(filenames)))\n",
    "\n",
    "            all_filenames.extend(clean_names)\n",
    "\n",
    "    # -------------------------\n",
    "    # 6. Build submission\n",
    "    # -------------------------\n",
    "\n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "    submission_df = pd.DataFrame({\n",
    "        \"id\": all_filenames,\n",
    "        \"class_id\": all_predictions,\n",
    "    })\n",
    "\n",
    "    # Deduplicate IDs if needed\n",
    "    duplicates = submission_df[submission_df.duplicated(subset=[\"id\"], keep=False)]\n",
    "    if len(duplicates) > 0:\n",
    "        print(\"\\n⚠️  WARNING: Duplicate IDs found!\")\n",
    "        print(f\"Number of duplicate entries: {len(duplicates)}\")\n",
    "        print(\"\\nRemoving duplicates (keeping first occurrence)...\")\n",
    "        submission_df = submission_df.drop_duplicates(subset=[\"id\"], keep=\"first\")\n",
    "\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "\n",
    "    # -------------------------\n",
    "    # 7. Accuracy if labels given\n",
    "    # -------------------------\n",
    "    accuracy = None\n",
    "    if total > 0:\n",
    "        accuracy = correct / total\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TEST ACCURACY: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"Correct: {correct} / {total}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ Submission saved to: {output_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total predictions: {len(submission_df)}\")\n",
    "    print(f\"Unique classes predicted: {submission_df['class_id'].nunique()}\")\n",
    "    print(f\"\\nClass distribution (top 10):\")\n",
    "    print(submission_df[\"class_id\"].value_counts().head(10))\n",
    "\n",
    "    # Validate format\n",
    "    print(\"\\nValidating submission format...\")\n",
    "    assert list(submission_df.columns) == [\"id\", \"class_id\"], \"Invalid columns!\"\n",
    "    assert submission_df[\"class_id\"].min() >= 0, \"Invalid class_id < 0\"\n",
    "    assert submission_df.isnull().sum().sum() == 0, \"Missing values found!\"\n",
    "    print(\"✓ Submission format is valid!\")\n",
    "\n",
    "    return submission_df, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de2334-4d84-494d-83a8-a94bf1197aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload best head\n",
    "linear_head.load_state_dict(torch.load(\"best_linear_head.pt\"))\n",
    "\n",
    "# test_loader_comp should yield (images, filenames) with shuffle=False\n",
    "submission_df, _ = create_submission(\n",
    "    test_loader=test_loader1,\n",
    "    head=linear_head,\n",
    "    encoder=encoder,                # the frozen encoder you used for features\n",
    "    output_path=\"submission.csv\",\n",
    "    device=\"cuda:1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb6940-b37d-4195-a3ed-5a9ddb0454aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df, _ = create_submission(\n",
    "    test_loader=,\n",
    "    head=linear_head,     # no encoder\n",
    "    encoder=None,\n",
    "    output_path=\"submission.csv\",\n",
    "    device=\"cuda\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4f8d3-f2b0-465b-92b3-258691c47b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_submission_from_feats(\n",
    "    test_feats,\n",
    "    head,\n",
    "    filenames,\n",
    "    output_path=\"submission.csv\",\n",
    "    device=\"cuda\",\n",
    "    batch_size=512,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create submission.csv using precomputed test_feats and a trained linear head.\n",
    "\n",
    "    Args:\n",
    "        test_feats: Tensor of shape (N, D), features from frozen encoder\n",
    "        head: trained linear head mapping D -> num_classes\n",
    "        filenames: list/array of length N with image ids (e.g. test_df['filename'])\n",
    "        output_path: where to save submission.csv\n",
    "        device: 'cuda' or 'cpu'\n",
    "        batch_size: inference batch size\n",
    "\n",
    "    Returns:\n",
    "        submission_df: DataFrame with columns ['id', 'class_id']\n",
    "    \"\"\"\n",
    "    device = device if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    head = head.to(device)\n",
    "    head.eval()\n",
    "\n",
    "    N = test_feats.shape[0]\n",
    "    assert N == len(filenames), f\"Mismatch: {N} feats vs {len(filenames)} filenames\"\n",
    "\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in tqdm(range(0, N, batch_size), desc=\"Inference (features)\"):\n",
    "            end = min(start + batch_size, N)\n",
    "            batch_feats = test_feats[start:end].to(device, non_blocking=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = head(batch_feats)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "    assert len(all_preds) == N\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        \"id\": filenames,\n",
    "        \"class_id\": all_preds,\n",
    "    })\n",
    "\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"\\nSaved submission to {output_path}\")\n",
    "    print(f\"Total predictions: {len(submission_df)}\")\n",
    "    print(f\"Unique classes predicted: {submission_df['class_id'].nunique()}\")\n",
    "    print(\"\\nClass distribution (top 10):\")\n",
    "    print(submission_df[\"class_id\"].value_counts().head(10))\n",
    "\n",
    "    # basic sanity checks\n",
    "    print(\"\\nValidating submission format...\")\n",
    "    assert list(submission_df.columns) == [\"id\", \"class_id\"], \"Invalid columns!\"\n",
    "    assert submission_df[\"class_id\"].min() >= 0, \"Invalid class_id < 0\"\n",
    "    assert submission_df.isnull().sum().sum() == 0, \"Missing values found!\"\n",
    "    print(\"✓ Submission format is valid!\")\n",
    "\n",
    "    return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09bfcde-0ed4-4925-ad0b-7e1b9f799a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from earlier\n",
    "test_feats, test_labels = extract_features(encoder, test_loader1, device)\n",
    "# and\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')  # or test_images.csv for Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d8a4b-1520-430d-a335-0a0f9d4d6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best head\n",
    "linear_head.load_state_dict(torch.load(\"best_linear_head.pt\"))\n",
    "\n",
    "# Filenames in the same order as test_feats / test_loader1\n",
    "filenames = test_df[\"filename\"].tolist()   # or 'id' for Kaggle test csv\n",
    "\n",
    "submission_df = create_submission_from_feats(\n",
    "    test_feats=test_feats,\n",
    "    head=linear_head,\n",
    "    filenames=filenames,\n",
    "    output_path=\"submission.csv\",\n",
    "    device=\"cuda\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e992b09-79b2-4d57-bea7-f442cecf0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dim = trainval_feats.shape[1]\n",
    "num_classes = int(trainval_labels.max().item() + 1)\n",
    "\n",
    "linear_head = nn.Linear(feat_dim, num_classes).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 🔴 IMPORTANT: optimizer must use linear_head.parameters()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    linear_head.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14865c-a0d5-48a3-8c71-deeb56f4f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch_head(head, loader, train=True):\n",
    "    if train:\n",
    "        head.train()\n",
    "    else:\n",
    "        head.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for feats, labels in loader:\n",
    "        feats = feats.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = head(feats)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    acc = 100.0 * correct / total\n",
    "    return avg_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a9446e-da5b-4392-9cb8-19edec099eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "best_test_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = run_epoch_head(linear_head, trainval_feat_loader, train=True)\n",
    "    test_loss, test_acc   = run_epoch_head(linear_head, test_feat_loader,   train=False)\n",
    "\n",
    "    # if you still want scheduler:\n",
    "    scheduler.step()\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch\n",
    "        torch.save(linear_head.state_dict(), \"best_linear_head.pt\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"train loss: {train_loss:.4f}, acc: {train_acc:.2f}% | \"\n",
    "        f\"test loss: {test_loss:.4f}, acc: {test_acc:.2f}%\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        w_norm = linear_head.weight.norm().item()\n",
    "        print(\"Head weight norm:\", w_norm)\n",
    "\n",
    "\n",
    "print(f\"BEST TEST ACC = {best_test_acc:.2f}% at epoch {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896326c4-8949-4b7d-9d36-6ddbfa701d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"num classes (unique labels):\", trainval_labels.unique().numel())\n",
    "print(\"min label:\", trainval_labels.min().item())\n",
    "print(\"max label:\", trainval_labels.max().item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
