{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7179ec-5097-454b-9ca7-b002e01796fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56ca8149-ab30-4911-bdc3-a6c6c1653f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import argparse\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a1e42b0-12e4-4414-a259-98ebf2715098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageDataset(Dataset):\n",
    "#     \"\"\"Simple dataset for loading images\"\"\"\n",
    "    \n",
    "#     def __init__(self, image_dir, image_list, labels=None, resolution=224):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             image_dir: Directory containing images\n",
    "#             image_list: List of image filenames\n",
    "#             labels: List of labels (optional, for train/val)\n",
    "#             resolution: Image resolution (96 for competition, 224 for DINO baseline)\n",
    "#         \"\"\"\n",
    "#         self.image_dir = Path(image_dir)\n",
    "#         self.image_list = image_list\n",
    "#         self.labels = labels\n",
    "#         self.resolution = resolution\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.image_list)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         img_name = self.image_list[idx]\n",
    "#         img_path = self.image_dir / img_name\n",
    "        \n",
    "#         # Load and resize image\n",
    "#         image = Image.open(img_path).convert('RGB')\n",
    "#         image = image.resize((self.resolution, self.resolution), Image.BILINEAR)\n",
    "        \n",
    "#         if self.labels is not None:\n",
    "#             return image, self.labels[idx], img_name\n",
    "#         return image, img_name\n",
    "\n",
    "# class ImageDataset(Dataset):\n",
    "#     \"\"\"Simple dataset for loading images with MAE/DINO-style transforms.\"\"\"\n",
    "\n",
    "#     def __init__(self, image_dir, image_list, labels=None, resolution=224, split=\"train\"):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             image_dir: Directory containing images\n",
    "#             image_list: List of filenames\n",
    "#             labels: Optional list of labels\n",
    "#             resolution: Base image size (224 for DINO, 96 for competition)\n",
    "#             split: \"train\" or \"val\" or \"test\"\n",
    "#         \"\"\"\n",
    "#         self.image_dir = Path(image_dir)\n",
    "#         self.image_list = image_list\n",
    "#         self.labels = labels\n",
    "#         self.split = split\n",
    "#         self.resolution = resolution\n",
    "\n",
    "#         # Same ImageNet normalization used in CUBLinearProbeDataset\n",
    "#         imagenet_mean = [0.485, 0.456, 0.406]\n",
    "#         imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "#         if split == \"train\":\n",
    "#             self.transform = v2.Compose([\n",
    "#                 v2.RandomResizedCrop(resolution, scale=(0.8, 1.0)),\n",
    "#                 v2.RandomHorizontalFlip(p=0.5),\n",
    "#                 v2.ColorJitter(\n",
    "#                     brightness=0.4,\n",
    "#                     contrast=0.4,\n",
    "#                     saturation=0.4,\n",
    "#                     hue=0.1\n",
    "#                 ),\n",
    "#                 v2.ToImage(),\n",
    "#                 v2.ToDtype(torch.float32, scale=True),\n",
    "#                 v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "#             ])\n",
    "#         else:\n",
    "#             # val/test preprocessing\n",
    "#             self.transform = v2.Compose([\n",
    "#                 v2.Resize(256),\n",
    "#                 v2.CenterCrop(resolution),\n",
    "#                 v2.ToImage(),\n",
    "#                 v2.ToDtype(torch.float32, scale=True),\n",
    "#                 v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "#             ])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_list)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_name = self.image_list[idx]\n",
    "#         img_path = self.image_dir / img_name\n",
    "\n",
    "#         # Load RGB image\n",
    "#         img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "#         # Apply SAME transforms that CUBLinearProbeDataset uses\n",
    "#         img = self.transform(img)\n",
    "\n",
    "#         if self.labels is not None:\n",
    "#             return img, self.labels[idx], img_name\n",
    "#         return img, img_name\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_list, labels=None,\n",
    "                 resolution=224, split=\"train\", apply_transforms=True):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.image_list = image_list\n",
    "        self.labels = labels\n",
    "        self.split = split\n",
    "        self.resolution = resolution\n",
    "        self.apply_transforms = apply_transforms\n",
    "\n",
    "        imagenet_mean = [0.485, 0.456, 0.406]\n",
    "        imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        if apply_transforms:\n",
    "            if split == \"train\":\n",
    "                self.transform = v2.Compose([\n",
    "                    v2.RandomResizedCrop(resolution, scale=(0.8, 1.0)),\n",
    "                    v2.RandomHorizontalFlip(p=0.5),\n",
    "                    v2.ColorJitter(\n",
    "                        brightness=0.4,\n",
    "                        contrast=0.4,\n",
    "                        saturation=0.4,\n",
    "                        hue=0.1\n",
    "                    ),\n",
    "                    v2.ToImage(),\n",
    "                    v2.ToDtype(torch.float32, scale=True),\n",
    "                    v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = v2.Compose([\n",
    "                    v2.Resize(256),\n",
    "                    v2.CenterCrop(resolution),\n",
    "                    v2.ToImage(),\n",
    "                    v2.ToDtype(torch.float32, scale=True),\n",
    "                    v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = None   # <-- important\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_list[idx]\n",
    "        img_path = self.image_dir / img_name\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)   # -> Tensor (for supervised)\n",
    "        # else: keep img as PIL (for SSL)\n",
    "\n",
    "        if self.labels is not None:\n",
    "            return img, self.labels[idx], img_name\n",
    "        return img, img_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38930ee5-8311-4029-9e83-52512b5bc917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t       test_images.csv\t\t train_labels.csv\n",
      "sample_submission.csv  test_labels_INTERNAL.csv  val\n",
      "test\t\t       train\t\t\t val_labels.csv\n"
     ]
    }
   ],
   "source": [
    "! ls /home/long/code/amogh/data/testset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fbebdfb-cd4a-46b4-ba48-5dfaa5f73b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset metadata...\n",
      "  Train: 8232 images\n",
      "  Val:   1727 images\n",
      "  Test:  1829 images\n",
      "  Classes: 200\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters (replace args.*)\n",
    "# ============================================================\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "resolution = 224   # or whatever you want for training\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = Path(\"/home/long/code/amogh/data/testset_1\")\n",
    "\n",
    "print(\"\\nLoading dataset metadata...\")\n",
    "train_df = pd.read_csv(data_dir / 'train_labels.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val_labels.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} images\")\n",
    "print(f\"  Val:   {len(val_df)} images\")\n",
    "print(f\"  Test:  {len(test_df)} images\")\n",
    "print(f\"  Classes: {train_df['class_id'].nunique()}\")\n",
    "\n",
    "train_dataset1 = ImageDataset(\n",
    "    data_dir / 'train',\n",
    "    train_df['filename'].tolist(),\n",
    "    train_df['class_id'].tolist(),\n",
    "    resolution=resolution\n",
    ")\n",
    "\n",
    "val_dataset1 = ImageDataset(\n",
    "    data_dir / 'val',\n",
    "    val_df['filename'].tolist(),\n",
    "    val_df['class_id'].tolist(),\n",
    "    resolution=resolution\n",
    ")\n",
    "\n",
    "test_dataset1 = ImageDataset(\n",
    "    data_dir / 'test',\n",
    "    test_df['filename'].tolist(),\n",
    "    labels=test_df['class_id'].tolist(),\n",
    "    resolution=resolution\n",
    ")\n",
    "\n",
    "train_loader1 = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader1 = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader1 = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e88ee4a0-c0d6-43bc-9c3a-c02b9fcf8ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset metadata...\n",
      "  Train: 42000 images\n",
      "  Val:   9000 images\n",
      "  Test:  9000 images\n",
      "  Classes: 100\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters (replace args.*)\n",
    "# ============================================================\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "resolution = 224   # or whatever you want for training\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = Path(\"/home/long/code/amogh/data/testset_2\")\n",
    "\n",
    "print(\"\\nLoading dataset metadata...\")\n",
    "train_df = pd.read_csv(data_dir / 'train_labels.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val_labels.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} images\")\n",
    "print(f\"  Val:   {len(val_df)} images\")\n",
    "print(f\"  Test:  {len(test_df)} images\")\n",
    "print(f\"  Classes: {train_df['class_id'].nunique()}\")\n",
    "\n",
    "train_dataset2 = ImageDataset(\n",
    "    data_dir / 'train',\n",
    "    train_df['filename'].tolist(),\n",
    "    train_df['class_id'].tolist(),\n",
    "    resolution=resolution\n",
    ")\n",
    "\n",
    "val_dataset2 = ImageDataset(\n",
    "    data_dir / 'val',\n",
    "    val_df['filename'].tolist(),\n",
    "    val_df['class_id'].tolist(),\n",
    "    resolution=resolution\n",
    ")\n",
    "\n",
    "test_dataset2 = ImageDataset(\n",
    "    data_dir / 'test',\n",
    "    test_df['filename'].tolist(),\n",
    "    labels=test_df['class_id'].tolist(),\n",
    "    resolution=resolution\n",
    ")\n",
    "\n",
    "train_loader2 = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader2 = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader2 = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa553af5-7b27-4817-b11b-7fa789f90444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset metadata...\n",
      "  Train: 13895 images\n",
      "  Val:   2977 images\n",
      "  Test:  2978 images\n",
      "  Classes: 397\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters (replace args.*)\n",
    "# ============================================================\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "resolution = 224   # or whatever you want for training\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = Path(\"/home/long/code/amogh/data/testset_3\")\n",
    "\n",
    "print(\"\\nLoading dataset metadata...\")\n",
    "train_df = pd.read_csv(data_dir / 'train_labels.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val_labels.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} images\")\n",
    "print(f\"  Val:   {len(val_df)} images\")\n",
    "print(f\"  Test:  {len(test_df)} images\")\n",
    "print(f\"  Classes: {train_df['class_id'].nunique()}\")\n",
    "\n",
    "train_dataset3 = ImageDataset(\n",
    "    data_dir / 'train',\n",
    "    train_df['filename'].tolist(),\n",
    "    train_df['class_id'].tolist(),\n",
    "    resolution=resolution\n",
    ")\n",
    "\n",
    "val_dataset3 = ImageDataset(\n",
    "    data_dir / 'val',\n",
    "    val_df['filename'].tolist(),\n",
    "    val_df['class_id'].tolist(),\n",
    "    resolution=resolution\n",
    ")\n",
    "\n",
    "test_dataset3 = ImageDataset(\n",
    "    data_dir / 'test',\n",
    "    test_df['filename'].tolist(),\n",
    "    labels=test_df['class_id'].tolist(),\n",
    "    resolution=resolution\n",
    ")\n",
    "\n",
    "train_loader3 = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader3 = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader3 = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49fad00f-9e89-428b-9456-e434dbb9102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lightly|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79398049-f622-4014-9603-68116d9c7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9416738c-3205-421d-b0e5-7b9fc1f1b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from timm.models.vision_transformer import vit_base_patch32_224\n",
    "from torch import nn\n",
    "from lightly.models import utils\n",
    "from lightly.models.modules import MAEDecoderTIMM, MaskedVisionTransformerTIMM\n",
    "from lightly.transforms import MAETransform\n",
    "import copy\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.loss import DINOLoss  # only needed if you re-train SSL\n",
    "from lightly.models.utils import deactivate_requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c6497ed-8306-40ca-a5c0-ac9d72db4c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINO(nn.Module):\n",
    "    def __init__(self, backbone, input_dim):\n",
    "        super().__init__()\n",
    "        self.student_backbone = backbone\n",
    "        self.student_head = DINOProjectionHead(\n",
    "            input_dim, 512, 64, 2048, freeze_last_layer=1\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(backbone)\n",
    "        self.teacher_head = DINOProjectionHead(input_dim, 512, 64, 2048)\n",
    "        deactivate_requires_grad(self.teacher_backbone)\n",
    "        deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.student_backbone(x).flatten(start_dim=1)\n",
    "        z = self.student_head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee649a90-3e50-444f-af4a-e302bb13552b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "# --- Build same backbone as used for DINO pretraining ---\n",
    "resnet = torchvision.models.resnet18()\n",
    "# resnet = torchvision.models.resnet34()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])  # (B, 512, 1, 1)\n",
    "input_dim = 512\n",
    "\n",
    "dino_model = DINO(backbone, input_dim)\n",
    "\n",
    "# --- Load your pre-trained DINO checkpoint ---\n",
    "ckpt = torch.load(\n",
    "    \"/home/long/code/dl_project1/experiments/outputs/dino-v1/dino-v1_small_100.pt\",\n",
    "    map_location=\"cpu\",\n",
    ")\n",
    "\n",
    "dino_model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "42411f41-8095-401a-aea7-02c9b1a729ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract raw subsets from train_ds and val_ds\n",
    "# raw_train_subset = train_raw_ds\n",
    "# raw_val_subset   = val_raw_ds\n",
    "\n",
    "# 2. SSL transform\n",
    "from lightly.transforms import MAETransform\n",
    "# ssl_transform = MAETransform()\n",
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "\n",
    "# ssl_transform = DINOTransform()\n",
    "# ssl_transform = get_cub_transform(split=\"train\", img_size=224)\n",
    "ssl_transform = DINOTransform()\n",
    "\n",
    "# 3. SSL dataset wrapper\n",
    "class CUB_SSL_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     data = self.subset[idx]  # (img, label, path)\n",
    "    #     img = data[0]\n",
    "    #     return self.transform(img)\n",
    "    def __getitem__(self, idx):\n",
    "        img, _, _ = self.subset[idx]\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        return self.transform(img)  # returns list[Tensor] from DINOTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "73dbb580-f0e5-4b82-8c85-6c8d915de2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL training images: 77831\n",
      "Dataset found. Total samples: 77831\n",
      "Batch size: 64\n",
      "Number of workers: 4\n"
     ]
    }
   ],
   "source": [
    "# 4. Combine train + val subsets (no labels)\n",
    "from torch.utils.data import ConcatDataset\n",
    "ssl_trainval_raw = ConcatDataset([\n",
    "    train_dataset1, val_dataset1,\n",
    "    train_dataset2, val_dataset2,\n",
    "    train_dataset3, val_dataset3,\n",
    "])\n",
    "\n",
    "\n",
    "# 5. Build SSL dataset\n",
    "ssl_trainval_ds = CUB_SSL_Dataset(\n",
    "    subset=ssl_trainval_raw,\n",
    "    transform=ssl_transform,\n",
    ")\n",
    "\n",
    "# 6. Build SSL dataloader\n",
    "ssl_loader = DataLoader(\n",
    "    ssl_trainval_ds,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(\"SSL training images:\", len(ssl_trainval_ds))\n",
    "\n",
    "if ssl_loader.dataset is not None:\n",
    "    print(f\"Dataset found. Total samples: {len(ssl_loader.dataset)}\")\n",
    "    print(f\"Batch size: {ssl_loader.batch_size}\")\n",
    "    print(f\"Number of workers: {ssl_loader.num_workers}\")\n",
    "else:\n",
    "    print(\"Dataset is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5171e5d6-9b06-4e04-824b-f19875c6efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"===== SSL Train+Val Raw Dataset =====\")\n",
    "# print(\"Type:\", type(ssl_trainval_raw))\n",
    "# print(\"Total length:\", len(ssl_trainval_raw))\n",
    "\n",
    "# print(\"\\n-- Sub-datasets inside ConcatDataset --\")\n",
    "# for i, ds in enumerate(ssl_trainval_raw.datasets):\n",
    "#     print(f\"[{i}] {type(ds)}, length = {len(ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "59bbb75d-bd9d-4056-ae93-0d0dec36a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f17650bb-5511-433d-a3e1-646f3a8879b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.loss import DINOLoss\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "from lightly.utils.scheduler import cosine_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c289ae65-63fb-4e7b-99a1-c7a7495106e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Caught OSError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_206784/2280793141.py\", line 31, in __getitem__\n    return self.transform(img)  # returns list[Tensor] from DINOTransform\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/lightly/transforms/multi_view_transform.py\", line 34, in __call__\n    return [transform(image) for transform in self.transforms]\n            ^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/lightly/transforms/dino_transform.py\", line 267, in __call__\n    transformed: Tensor = self.transform(image)\n                          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torchvision/transforms/v2/_container.py\", line 52, in forward\n    outputs = transform(*inputs)\n              ^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/lightly/transforms/solarize.py\", line 40, in __call__\n    return ImageOps.solarize(sample, threshold=self.threshold)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/PIL/ImageOps.py\", line 674, in solarize\n    return _lut(image, lut)\n           ^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/PIL/ImageOps.py\", line 63, in _lut\n    raise OSError(msg)\nOSError: not supported for mode <built-in method mode of Tensor object at 0x73a9a44aed50>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ---- Inspect one batch from SSL dataloader ----\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mssl_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== SSL Batch Debug ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/environments/wejepa/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/environments/wejepa/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1506\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1504\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1505\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1506\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/environments/wejepa/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1541\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1539\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/environments/wejepa/lib/python3.12/site-packages/torch/_utils.py:769\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    767\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mOSError\u001b[39m: Caught OSError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_206784/2280793141.py\", line 31, in __getitem__\n    return self.transform(img)  # returns list[Tensor] from DINOTransform\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/lightly/transforms/multi_view_transform.py\", line 34, in __call__\n    return [transform(image) for transform in self.transforms]\n            ^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/lightly/transforms/dino_transform.py\", line 267, in __call__\n    transformed: Tensor = self.transform(image)\n                          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torchvision/transforms/v2/_container.py\", line 52, in forward\n    outputs = transform(*inputs)\n              ^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/lightly/transforms/solarize.py\", line 40, in __call__\n    return ImageOps.solarize(sample, threshold=self.threshold)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/PIL/ImageOps.py\", line 674, in solarize\n    return _lut(image, lut)\n           ^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/PIL/ImageOps.py\", line 63, in _lut\n    raise OSError(msg)\nOSError: not supported for mode <built-in method mode of Tensor object at 0x73a9a44aed50>\n"
     ]
    }
   ],
   "source": [
    "# ---- Inspect one batch from SSL dataloader ----\n",
    "\n",
    "batch = next(iter(ssl_loader))\n",
    "\n",
    "print(\"\\n=== SSL Batch Debug ===\")\n",
    "\n",
    "if isinstance(batch, list) or isinstance(batch, tuple):\n",
    "    print(f\"Batch is a {type(batch)} with length {len(batch)}\")\n",
    "\n",
    "# DINOTransform returns a list of views per item, but DataLoader collates it into:\n",
    "# batch = list_of_views, where each element has shape (B, C, H, W)\n",
    "\n",
    "# Example: batch[0] = global crops   shape: (B, 3, 224, 224)\n",
    "#          batch[1] = global crops   shape: (B, 3, 224, 224)\n",
    "#          batch[2] = local crops    shape: (B, 3, 96, 96)\n",
    "# etc.\n",
    "\n",
    "for i, view in enumerate(batch):\n",
    "    print(f\"\\n--- View {i} ---\")\n",
    "    print(f\"Type: {type(view)}\")\n",
    "    try:\n",
    "        print(f\"Shape: {view.shape}\")\n",
    "    except Exception:\n",
    "        print(\"View has no `.shape` attribute\")\n",
    "    print(f\"Dtype: {getattr(view, 'dtype', None)}\")\n",
    "\n",
    "print(\"\\n=== End Debug ===\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "40d3614e-61f5-4bf6-a055-2838ce5c07b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save directory ready: /home/long/code/amogh/data/models\n",
      "Starting SSL Training (DINO)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Caught OSError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_206784/3452417075.py\", line 29, in __getitem__\n    return self.transform(img)  # returns list[Tensor] from DINOTransform\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/lightly/transforms/multi_view_transform.py\", line 34, in __call__\n    return [transform(image) for transform in self.transforms]\n            ^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/lightly/transforms/dino_transform.py\", line 267, in __call__\n    transformed: Tensor = self.transform(image)\n                          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torchvision/transforms/v2/_container.py\", line 52, in forward\n    outputs = transform(*inputs)\n              ^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/lightly/transforms/solarize.py\", line 40, in __call__\n    return ImageOps.solarize(sample, threshold=self.threshold)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/PIL/ImageOps.py\", line 674, in solarize\n    return _lut(image, lut)\n           ^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/PIL/ImageOps.py\", line 63, in _lut\n    raise OSError(msg)\nOSError: not supported for mode <built-in method mode of Tensor object at 0x73a9d0fc82d0>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# EMA momentum for teacher this epoch\u001b[39;00m\n\u001b[32m     48\u001b[39m momentum_val = cosine_schedule(epoch, num_epochs, \u001b[32m0.996\u001b[39m, \u001b[32m1.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mssl_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     51\u001b[39m \n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ssl_loader should give a list of crops (views)\u001b[39;49;00m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Handle both \"views\" and \"(views, _)\" cases\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mviews\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/environments/wejepa/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/environments/wejepa/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/environments/wejepa/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1506\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1504\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1505\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1506\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/environments/wejepa/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1541\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1539\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/environments/wejepa/lib/python3.12/site-packages/torch/_utils.py:769\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    767\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mOSError\u001b[39m: Caught OSError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_206784/3452417075.py\", line 29, in __getitem__\n    return self.transform(img)  # returns list[Tensor] from DINOTransform\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/lightly/transforms/multi_view_transform.py\", line 34, in __call__\n    return [transform(image) for transform in self.transforms]\n            ^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/lightly/transforms/dino_transform.py\", line 267, in __call__\n    transformed: Tensor = self.transform(image)\n                          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/torchvision/transforms/v2/_container.py\", line 52, in forward\n    outputs = transform(*inputs)\n              ^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/lightly/transforms/solarize.py\", line 40, in __call__\n    return ImageOps.solarize(sample, threshold=self.threshold)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/PIL/ImageOps.py\", line 674, in solarize\n    return _lut(image, lut)\n           ^^^^^^^^^^^^^^^^\n  File \"/home/long/code/environments/wejepa/lib/python3.12/site-packages/PIL/ImageOps.py\", line 63, in _lut\n    raise OSError(msg)\nOSError: not supported for mode <built-in method mode of Tensor object at 0x73a9d0fc82d0>\n"
     ]
    }
   ],
   "source": [
    "# ---------- Project / save dir ----------\n",
    "PROJECT_NAME = \"dino-v1\"  # folder name\n",
    "save_dir = \"/home/long/code/amogh/data/models/\"\n",
    "save_dir = Path(save_dir)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Save directory ready: {save_dir}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dino_model.to(device)\n",
    "\n",
    "ckpt_path = save_dir / f\"{PROJECT_NAME}_full_finetuned_18.pt\"\n",
    "start_epoch = 0\n",
    "\n",
    "# # --- Optimizer: only student parameters ---\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     list(dino_model.student_backbone.parameters()) +\n",
    "#     list(dino_model.student_head.parameters()),\n",
    "#     lr=1.5e-4,\n",
    "#     weight_decay=1e-4,\n",
    "# )\n",
    "\n",
    "optimizer = torch.optim.Adam(dino_model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = DINOLoss(\n",
    "    output_dim=2048,\n",
    "    warmup_teacher_temp_epochs=5,\n",
    ")\n",
    "# move loss to correct device because it also contains parameters\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# ---------- Training loop ----------\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 100\n",
    "print(\"Starting SSL Training (DINO)\")\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    dino_model.train()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # EMA momentum for teacher this epoch\n",
    "    momentum_val = cosine_schedule(epoch, num_epochs, 0.996, 1.0)\n",
    "\n",
    "    for batch in tqdm(ssl_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "\n",
    "        # ssl_loader should give a list of crops (views)\n",
    "        # Handle both \"views\" and \"(views, _)\" cases\n",
    "        if isinstance(batch, (list, tuple)) and isinstance(batch[0], torch.Tensor):\n",
    "            views = batch\n",
    "        elif isinstance(batch, (list, tuple)):\n",
    "            views = batch[0]\n",
    "        else:\n",
    "            views = batch\n",
    "\n",
    "        # move all crops to GPU\n",
    "        views = [v.to(device, non_blocking=True) for v in views]\n",
    "\n",
    "        # ---- EMA update for teacher ----\n",
    "        update_momentum(dino_model.student_backbone, dino_model.teacher_backbone, m=momentum_val)\n",
    "        update_momentum(dino_model.student_head,     dino_model.teacher_head,     m=momentum_val)\n",
    "\n",
    "        # first two are global crops for teacher\n",
    "        global_views = views[:2]\n",
    "\n",
    "        # teacher on global crops (no grad)\n",
    "        with torch.no_grad():\n",
    "            teacher_out = [dino_model.forward_teacher(v) for v in global_views]\n",
    "\n",
    "        # student on all crops (global + local)\n",
    "        student_out = [dino_model(v) for v in views]\n",
    "\n",
    "        # ---- DINO loss ----\n",
    "        loss = criterion(teacher_out, student_out, epoch=epoch)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        dino_model.student_head.cancel_last_layer_gradients(current_epoch=epoch)\n",
    "        optimizer.step()\n",
    "\n",
    "        bsz = views[0].size(0)\n",
    "        total_loss += loss.item() * bsz\n",
    "        n_samples += bsz\n",
    "        global_step += 1\n",
    "\n",
    "    avg_loss = total_loss / max(n_samples, 1)\n",
    "    elapsed = time.time() - epoch_start\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train loss: {avg_loss:.5f} | time: {elapsed:.1f}s\")\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # ---- Save checkpoint (always same filename) ----\n",
    "        ckpt = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": dino_model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"avg_loss\": avg_loss,\n",
    "        }\n",
    "        torch.save(ckpt, ckpt_path)\n",
    "        print(f\" Saved checkpoint: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72799a15-a19e-404e-8015-a5845c8f374e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wejepa",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
