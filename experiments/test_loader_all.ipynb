{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7179ec-5097-454b-9ca7-b002e01796fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ca8149-ab30-4911-bdc3-a6c6c1653f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import argparse\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1e42b0-12e4-4414-a259-98ebf2715098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageDataset(Dataset):\n",
    "#     \"\"\"Simple dataset for loading images\"\"\"\n",
    "    \n",
    "#     def __init__(self, image_dir, image_list, labels=None, resolution=224):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             image_dir: Directory containing images\n",
    "#             image_list: List of image filenames\n",
    "#             labels: List of labels (optional, for train/val)\n",
    "#             resolution: Image resolution (96 for competition, 224 for DINO baseline)\n",
    "#         \"\"\"\n",
    "#         self.image_dir = Path(image_dir)\n",
    "#         self.image_list = image_list\n",
    "#         self.labels = labels\n",
    "#         self.resolution = resolution\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.image_list)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         img_name = self.image_list[idx]\n",
    "#         img_path = self.image_dir / img_name\n",
    "        \n",
    "#         # Load and resize image\n",
    "#         image = Image.open(img_path).convert('RGB')\n",
    "#         image = image.resize((self.resolution, self.resolution), Image.BILINEAR)\n",
    "        \n",
    "#         if self.labels is not None:\n",
    "#             return image, self.labels[idx], img_name\n",
    "#         return image, img_name\n",
    "\n",
    "# class ImageDataset(Dataset):\n",
    "#     \"\"\"Simple dataset for loading images with MAE/DINO-style transforms.\"\"\"\n",
    "\n",
    "#     def __init__(self, image_dir, image_list, labels=None, resolution=224, split=\"train\"):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             image_dir: Directory containing images\n",
    "#             image_list: List of filenames\n",
    "#             labels: Optional list of labels\n",
    "#             resolution: Base image size (224 for DINO, 96 for competition)\n",
    "#             split: \"train\" or \"val\" or \"test\"\n",
    "#         \"\"\"\n",
    "#         self.image_dir = Path(image_dir)\n",
    "#         self.image_list = image_list\n",
    "#         self.labels = labels\n",
    "#         self.split = split\n",
    "#         self.resolution = resolution\n",
    "\n",
    "#         # Same ImageNet normalization used in CUBLinearProbeDataset\n",
    "#         imagenet_mean = [0.485, 0.456, 0.406]\n",
    "#         imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "#         if split == \"train\":\n",
    "#             self.transform = v2.Compose([\n",
    "#                 v2.RandomResizedCrop(resolution, scale=(0.8, 1.0)),\n",
    "#                 v2.RandomHorizontalFlip(p=0.5),\n",
    "#                 v2.ColorJitter(\n",
    "#                     brightness=0.4,\n",
    "#                     contrast=0.4,\n",
    "#                     saturation=0.4,\n",
    "#                     hue=0.1\n",
    "#                 ),\n",
    "#                 v2.ToImage(),\n",
    "#                 v2.ToDtype(torch.float32, scale=True),\n",
    "#                 v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "#             ])\n",
    "#         else:\n",
    "#             # val/test preprocessing\n",
    "#             self.transform = v2.Compose([\n",
    "#                 v2.Resize(256),\n",
    "#                 v2.CenterCrop(resolution),\n",
    "#                 v2.ToImage(),\n",
    "#                 v2.ToDtype(torch.float32, scale=True),\n",
    "#                 v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "#             ])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_list)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_name = self.image_list[idx]\n",
    "#         img_path = self.image_dir / img_name\n",
    "\n",
    "#         # Load RGB image\n",
    "#         img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "#         # Apply SAME transforms that CUBLinearProbeDataset uses\n",
    "#         img = self.transform(img)\n",
    "\n",
    "#         if self.labels is not None:\n",
    "#             return img, self.labels[idx], img_name\n",
    "#         return img, img_name\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_list, labels=None,\n",
    "                 resolution=224, split=\"train\", apply_transforms=True):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.image_list = image_list\n",
    "        self.labels = labels\n",
    "        self.split = split\n",
    "        self.resolution = resolution\n",
    "        self.apply_transforms = apply_transforms\n",
    "\n",
    "        imagenet_mean = [0.485, 0.456, 0.406]\n",
    "        imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        if apply_transforms:\n",
    "            if split == \"train\":\n",
    "                self.transform = v2.Compose([\n",
    "                    v2.RandomResizedCrop(resolution, scale=(0.8, 1.0)),\n",
    "                    v2.RandomHorizontalFlip(p=0.5),\n",
    "                    v2.ColorJitter(\n",
    "                        brightness=0.4,\n",
    "                        contrast=0.4,\n",
    "                        saturation=0.4,\n",
    "                        hue=0.1\n",
    "                    ),\n",
    "                    v2.ToImage(),\n",
    "                    v2.ToDtype(torch.float32, scale=True),\n",
    "                    v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = v2.Compose([\n",
    "                    v2.Resize(256),\n",
    "                    v2.CenterCrop(resolution),\n",
    "                    v2.ToImage(),\n",
    "                    v2.ToDtype(torch.float32, scale=True),\n",
    "                    v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = None   # <-- important\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_list[idx]\n",
    "        img_path = self.image_dir / img_name\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)   # -> Tensor (for supervised)\n",
    "        # else: keep img as PIL (for SSL)\n",
    "\n",
    "        if self.labels is not None:\n",
    "            return img, self.labels[idx], img_name\n",
    "        return img, img_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b652cf42-2018-4352-bad0-0820f878dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle PIL images\"\"\"\n",
    "    if len(batch[0]) == 3:  # train/val (image, label, filename)\n",
    "        images = [item[0] for item in batch]\n",
    "        labels = [item[1] for item in batch]\n",
    "        filenames = [item[2] for item in batch]\n",
    "        return images, labels, filenames\n",
    "    else:  # test (image, filename)\n",
    "        images = [item[0] for item in batch]\n",
    "        filenames = [item[1] for item in batch]\n",
    "        return images, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38930ee5-8311-4029-9e83-52512b5bc917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t       test_images.csv\t\t train_labels.csv\n",
      "sample_submission.csv  test_labels_INTERNAL.csv  val\n",
      "test\t\t       train\t\t\t val_labels.csv\n"
     ]
    }
   ],
   "source": [
    "! ls /home/long/code/amogh/data/testset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fbebdfb-cd4a-46b4-ba48-5dfaa5f73b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset metadata...\n",
      "  Train: 8232 images\n",
      "  Val:   1727 images\n",
      "  Test:  1829 images\n",
      "  Classes: 200\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters (replace args.*)\n",
    "# ============================================================\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "resolution = 224   # or whatever you want for training\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = Path(\"/home/long/code/amogh/data/testset_1\")\n",
    "\n",
    "print(\"\\nLoading dataset metadata...\")\n",
    "train_df = pd.read_csv(data_dir / 'train_labels.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val_labels.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} images\")\n",
    "print(f\"  Val:   {len(val_df)} images\")\n",
    "print(f\"  Test:  {len(test_df)} images\")\n",
    "print(f\"  Classes: {train_df['class_id'].nunique()}\")\n",
    "\n",
    "train_dataset1 = ImageDataset(\n",
    "    data_dir / 'train',\n",
    "    train_df['filename'].tolist(),\n",
    "    train_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "val_dataset1 = ImageDataset(\n",
    "    data_dir / 'val',\n",
    "    val_df['filename'].tolist(),\n",
    "    val_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "test_dataset1 = ImageDataset(\n",
    "    data_dir / 'test',\n",
    "    test_df['filename'].tolist(),\n",
    "    labels=test_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "train_loader1 = DataLoader(\n",
    "    train_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader1 = DataLoader(\n",
    "    val_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader1 = DataLoader(\n",
    "    test_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e88ee4a0-c0d6-43bc-9c3a-c02b9fcf8ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset metadata...\n",
      "  Train: 42000 images\n",
      "  Val:   9000 images\n",
      "  Test:  9000 images\n",
      "  Classes: 100\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters (replace args.*)\n",
    "# ============================================================\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "resolution = 224   # or whatever you want for training\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = Path(\"/home/long/code/amogh/data/testset_2\")\n",
    "\n",
    "print(\"\\nLoading dataset metadata...\")\n",
    "train_df = pd.read_csv(data_dir / 'train_labels.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val_labels.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} images\")\n",
    "print(f\"  Val:   {len(val_df)} images\")\n",
    "print(f\"  Test:  {len(test_df)} images\")\n",
    "print(f\"  Classes: {train_df['class_id'].nunique()}\")\n",
    "\n",
    "train_dataset2 = ImageDataset(\n",
    "    data_dir / 'train',\n",
    "    train_df['filename'].tolist(),\n",
    "    train_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "val_dataset2 = ImageDataset(\n",
    "    data_dir / 'val',\n",
    "    val_df['filename'].tolist(),\n",
    "    val_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "test_dataset2 = ImageDataset(\n",
    "    data_dir / 'test',\n",
    "    test_df['filename'].tolist(),\n",
    "    labels=test_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "train_loader2 = DataLoader(\n",
    "    train_dataset2,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader2 = DataLoader(\n",
    "    val_dataset2,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader2 = DataLoader(\n",
    "    test_dataset2,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa553af5-7b27-4817-b11b-7fa789f90444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset metadata...\n",
      "  Train: 13895 images\n",
      "  Val:   2977 images\n",
      "  Test:  2978 images\n",
      "  Classes: 397\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters (replace args.*)\n",
    "# ============================================================\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "resolution = 224   # or whatever you want for training\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = Path(\"/home/long/code/amogh/data/testset_3\")\n",
    "\n",
    "print(\"\\nLoading dataset metadata...\")\n",
    "train_df = pd.read_csv(data_dir / 'train_labels.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val_labels.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} images\")\n",
    "print(f\"  Val:   {len(val_df)} images\")\n",
    "print(f\"  Test:  {len(test_df)} images\")\n",
    "print(f\"  Classes: {train_df['class_id'].nunique()}\")\n",
    "\n",
    "train_dataset3 = ImageDataset(\n",
    "    data_dir / 'train',\n",
    "    train_df['filename'].tolist(),\n",
    "    train_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "val_dataset3 = ImageDataset(\n",
    "    data_dir / 'val',\n",
    "    val_df['filename'].tolist(),\n",
    "    val_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "test_dataset3 = ImageDataset(\n",
    "    data_dir / 'test',\n",
    "    test_df['filename'].tolist(),\n",
    "    labels=test_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "train_loader3 = DataLoader(\n",
    "    train_dataset3,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader3 = DataLoader(\n",
    "    val_dataset3,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader3 = DataLoader(\n",
    "    test_dataset3,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49fad00f-9e89-428b-9456-e434dbb9102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lightly|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79398049-f622-4014-9603-68116d9c7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9416738c-3205-421d-b0e5-7b9fc1f1b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from timm.models.vision_transformer import vit_base_patch32_224\n",
    "from torch import nn\n",
    "from lightly.models import utils\n",
    "from lightly.models.modules import MAEDecoderTIMM, MaskedVisionTransformerTIMM\n",
    "from lightly.transforms import MAETransform\n",
    "import copy\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.loss import DINOLoss  # only needed if you re-train SSL\n",
    "from lightly.models.utils import deactivate_requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c6497ed-8306-40ca-a5c0-ac9d72db4c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINO(nn.Module):\n",
    "    def __init__(self, backbone, input_dim):\n",
    "        super().__init__()\n",
    "        self.student_backbone = backbone\n",
    "        self.student_head = DINOProjectionHead(\n",
    "            input_dim, 512, 64, 2048, freeze_last_layer=1\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(backbone)\n",
    "        self.teacher_head = DINOProjectionHead(input_dim, 512, 64, 2048)\n",
    "        deactivate_requires_grad(self.teacher_backbone)\n",
    "        deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.student_backbone(x).flatten(start_dim=1)\n",
    "        z = self.student_head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee649a90-3e50-444f-af4a-e302bb13552b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long/code/environments/wejepa/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "# --- Build same backbone as used for DINO pretraining ---\n",
    "resnet = torchvision.models.resnet18()\n",
    "# resnet = torchvision.models.resnet34()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])  # (B, 512, 1, 1)\n",
    "input_dim = 512\n",
    "\n",
    "dino_model = DINO(backbone, input_dim)\n",
    "\n",
    "# --- Load your pre-trained DINO checkpoint ---\n",
    "ckpt = torch.load(\n",
    "    \"/home/long/code/dl_project1/experiments/outputs/dino-v1/dino-v1_small_100.pt\",\n",
    "    map_location=\"cpu\",\n",
    ")\n",
    "\n",
    "dino_model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42411f41-8095-401a-aea7-02c9b1a729ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract raw subsets from train_ds and val_ds\n",
    "# raw_train_subset = train_raw_ds\n",
    "# raw_val_subset   = val_raw_ds\n",
    "\n",
    "# 2. SSL transform\n",
    "from lightly.transforms import MAETransform\n",
    "# ssl_transform = MAETransform()\n",
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "\n",
    "# ssl_transform = DINOTransform()\n",
    "# ssl_transform = get_cub_transform(split=\"train\", img_size=224)\n",
    "ssl_transform = DINOTransform()\n",
    "\n",
    "# 3. SSL dataset wrapper\n",
    "class CUB_SSL_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     data = self.subset[idx]  # (img, label, path)\n",
    "    #     img = data[0]\n",
    "    #     return self.transform(img)\n",
    "    def __getitem__(self, idx):\n",
    "        img, _, _ = self.subset[idx]\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        return self.transform(img)  # returns list[Tensor] from DINOTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73dbb580-f0e5-4b82-8c85-6c8d915de2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSL training images: 77831\n",
      "Dataset found. Total samples: 77831\n",
      "Batch size: 64\n",
      "Number of workers: 4\n"
     ]
    }
   ],
   "source": [
    "# 4. Combine train + val subsets (no labels)\n",
    "from torch.utils.data import ConcatDataset\n",
    "ssl_trainval_raw = ConcatDataset([\n",
    "    train_dataset1, val_dataset1,\n",
    "    train_dataset2, val_dataset2,\n",
    "    train_dataset3, val_dataset3,\n",
    "])\n",
    "\n",
    "\n",
    "# 5. Build SSL dataset\n",
    "ssl_trainval_ds = CUB_SSL_Dataset(\n",
    "    subset=ssl_trainval_raw,\n",
    "    transform=ssl_transform,\n",
    ")\n",
    "\n",
    "# 6. Build SSL dataloader\n",
    "ssl_loader = DataLoader(\n",
    "    ssl_trainval_ds,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    # collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(\"SSL training images:\", len(ssl_trainval_ds))\n",
    "\n",
    "if ssl_loader.dataset is not None:\n",
    "    print(f\"Dataset found. Total samples: {len(ssl_loader.dataset)}\")\n",
    "    print(f\"Batch size: {ssl_loader.batch_size}\")\n",
    "    print(f\"Number of workers: {ssl_loader.num_workers}\")\n",
    "else:\n",
    "    print(\"Dataset is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5171e5d6-9b06-4e04-824b-f19875c6efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"===== SSL Train+Val Raw Dataset =====\")\n",
    "# print(\"Type:\", type(ssl_trainval_raw))\n",
    "# print(\"Total length:\", len(ssl_trainval_raw))\n",
    "\n",
    "# print(\"\\n-- Sub-datasets inside ConcatDataset --\")\n",
    "# for i, ds in enumerate(ssl_trainval_raw.datasets):\n",
    "#     print(f\"[{i}] {type(ds)}, length = {len(ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59bbb75d-bd9d-4056-ae93-0d0dec36a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f17650bb-5511-433d-a3e1-646f3a8879b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.loss import DINOLoss\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "from lightly.utils.scheduler import cosine_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c289ae65-63fb-4e7b-99a1-c7a7495106e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SSL Batch Debug ===\n",
      "Batch is a <class 'list'> with length 8\n",
      "\n",
      "--- View 0 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 224, 224])\n",
      "Dtype: torch.float32\n",
      "\n",
      "--- View 1 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 224, 224])\n",
      "Dtype: torch.float32\n",
      "\n",
      "--- View 2 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 96, 96])\n",
      "Dtype: torch.float32\n",
      "\n",
      "--- View 3 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 96, 96])\n",
      "Dtype: torch.float32\n",
      "\n",
      "--- View 4 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 96, 96])\n",
      "Dtype: torch.float32\n",
      "\n",
      "--- View 5 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 96, 96])\n",
      "Dtype: torch.float32\n",
      "\n",
      "--- View 6 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 96, 96])\n",
      "Dtype: torch.float32\n",
      "\n",
      "--- View 7 ---\n",
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([64, 3, 96, 96])\n",
      "Dtype: torch.float32\n",
      "\n",
      "=== End Debug ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- Inspect one batch from SSL dataloader ----\n",
    "\n",
    "batch = next(iter(ssl_loader))\n",
    "\n",
    "print(\"\\n=== SSL Batch Debug ===\")\n",
    "\n",
    "if isinstance(batch, list) or isinstance(batch, tuple):\n",
    "    print(f\"Batch is a {type(batch)} with length {len(batch)}\")\n",
    "\n",
    "# DINOTransform returns a list of views per item, but DataLoader collates it into:\n",
    "# batch = list_of_views, where each element has shape (B, C, H, W)\n",
    "\n",
    "# Example: batch[0] = global crops   shape: (B, 3, 224, 224)\n",
    "#          batch[1] = global crops   shape: (B, 3, 224, 224)\n",
    "#          batch[2] = local crops    shape: (B, 3, 96, 96)\n",
    "# etc.\n",
    "\n",
    "for i, view in enumerate(batch):\n",
    "    print(f\"\\n--- View {i} ---\")\n",
    "    print(f\"Type: {type(view)}\")\n",
    "    try:\n",
    "        print(f\"Shape: {view.shape}\")\n",
    "    except Exception:\n",
    "        print(\"View has no `.shape` attribute\")\n",
    "    print(f\"Dtype: {getattr(view, 'dtype', None)}\")\n",
    "\n",
    "print(\"\\n=== End Debug ===\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0ee457d6-f863-4636-bf19-933cbda66fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ab1a8389-884b-4c35-b74b-89a52a8cb037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of batch: <class 'list'>\n",
      "Batch length: 8\n",
      "  item[0] type: <class 'torch.Tensor'>\n",
      "  item[0] shape: torch.Size([64, 3, 224, 224])\n",
      "  item[1] type: <class 'torch.Tensor'>\n",
      "  item[1] shape: torch.Size([64, 3, 224, 224])\n",
      "  item[2] type: <class 'torch.Tensor'>\n",
      "  item[2] shape: torch.Size([64, 3, 96, 96])\n",
      "  item[3] type: <class 'torch.Tensor'>\n",
      "  item[3] shape: torch.Size([64, 3, 96, 96])\n",
      "  item[4] type: <class 'torch.Tensor'>\n",
      "  item[4] shape: torch.Size([64, 3, 96, 96])\n",
      "  item[5] type: <class 'torch.Tensor'>\n",
      "  item[5] shape: torch.Size([64, 3, 96, 96])\n",
      "  item[6] type: <class 'torch.Tensor'>\n",
      "  item[6] shape: torch.Size([64, 3, 96, 96])\n",
      "  item[7] type: <class 'torch.Tensor'>\n",
      "  item[7] shape: torch.Size([64, 3, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(ssl_loader))  # or ssl_loader\n",
    "\n",
    "print(\"Type of batch:\", type(batch))\n",
    "\n",
    "if isinstance(batch, (list, tuple)):\n",
    "    print(\"Batch length:\", len(batch))\n",
    "    for i, x in enumerate(batch):\n",
    "        print(f\"  item[{i}] type: {type(x)}\")\n",
    "        if hasattr(x, \"shape\"):\n",
    "            print(f\"  item[{i}] shape:\", x.shape)\n",
    "else:\n",
    "    print(\"Batch shape:\", batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3614e-61f5-4bf6-a055-2838ce5c07b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save directory ready: /home/long/code/amogh/data/models\n",
      "Starting SSL Training (DINO)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 1217/1217 [06:04<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | train loss: 2.45309 | time: 364.8s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 1217/1217 [06:05<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss: 2.08134 | time: 365.9s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 1217/1217 [06:07<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train loss: 2.05473 | time: 367.9s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|█████████████████████████████████████████████████████████████████████████████████| 1217/1217 [06:06<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | train loss: 2.02415 | time: 366.4s\n",
      "✓ Saved checkpoint: /home/long/code/amogh/data/models/dino-v1_full_finetuned_18.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100:  24%|███████████████████▍                                                              | 288/1217 [01:36<03:54,  3.95it/s]"
     ]
    }
   ],
   "source": [
    "# ---------- Project / save dir ----------\n",
    "PROJECT_NAME = \"dino-v1\"  # folder name\n",
    "save_dir = \"/home/long/code/amogh/data/models/\"\n",
    "save_dir = Path(save_dir)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Save directory ready: {save_dir}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dino_model.to(device)\n",
    "\n",
    "ckpt_path = save_dir / f\"{PROJECT_NAME}_full_finetuned_18.pt\"\n",
    "start_epoch = 0\n",
    "\n",
    "# # --- Optimizer: only student parameters ---\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     list(dino_model.student_backbone.parameters()) +\n",
    "#     list(dino_model.student_head.parameters()),\n",
    "#     lr=1.5e-4,\n",
    "#     weight_decay=1e-4,\n",
    "# )\n",
    "\n",
    "optimizer = torch.optim.Adam(dino_model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = DINOLoss(\n",
    "    output_dim=2048,\n",
    "    warmup_teacher_temp_epochs=5,\n",
    ")\n",
    "# move loss to correct device because it also contains parameters\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# ---------- Training loop ----------\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 100\n",
    "print(\"Starting SSL Training (DINO)\")\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    dino_model.train()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # EMA momentum for teacher this epoch\n",
    "    momentum_val = cosine_schedule(epoch, num_epochs, 0.996, 1.0)\n",
    "\n",
    "    for batch in tqdm(ssl_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "\n",
    "        # ssl_loader should give a list of crops (views)\n",
    "        # Handle both \"views\" and \"(views, _)\" cases\n",
    "        if isinstance(batch, (list, tuple)) and isinstance(batch[0], torch.Tensor):\n",
    "            views = batch\n",
    "        elif isinstance(batch, (list, tuple)):\n",
    "            views = batch[0]\n",
    "        else:\n",
    "            views = batch\n",
    "\n",
    "        # move all crops to GPU\n",
    "        views = [v.to(device, non_blocking=True) for v in views]\n",
    "\n",
    "        # ---- EMA update for teacher ----\n",
    "        update_momentum(dino_model.student_backbone, dino_model.teacher_backbone, m=momentum_val)\n",
    "        update_momentum(dino_model.student_head,     dino_model.teacher_head,     m=momentum_val)\n",
    "\n",
    "        # first two are global crops for teacher\n",
    "        global_views = views[:2]\n",
    "\n",
    "        # teacher on global crops (no grad)\n",
    "        with torch.no_grad():\n",
    "            teacher_out = [dino_model.forward_teacher(v) for v in global_views]\n",
    "\n",
    "        # student on all crops (global + local)\n",
    "        student_out = [dino_model(v) for v in views]\n",
    "\n",
    "        # ---- DINO loss ----\n",
    "        loss = criterion(teacher_out, student_out, epoch=epoch)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        dino_model.student_head.cancel_last_layer_gradients(current_epoch=epoch)\n",
    "        optimizer.step()\n",
    "\n",
    "        bsz = views[0].size(0)\n",
    "        total_loss += loss.item() * bsz\n",
    "        n_samples += bsz\n",
    "        global_step += 1\n",
    "\n",
    "    avg_loss = total_loss / max(n_samples, 1)\n",
    "    elapsed = time.time() - epoch_start\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train loss: {avg_loss:.5f} | time: {elapsed:.1f}s\")\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        # ---- Save checkpoint (always same filename) ----\n",
    "        ckpt = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": dino_model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"avg_loss\": avg_loss,\n",
    "        }\n",
    "        torch.save(ckpt, ckpt_path)\n",
    "        print(f\"✓ Saved checkpoint: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72799a15-a19e-404e-8015-a5845c8f374e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wejepa",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
