{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca8149-ab30-4911-bdc3-a6c6c1653f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "import torch.distributed as dist\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "from huggingface_hub import snapshot_download\n",
    "from timm.models.vision_transformer import vit_base_patch32_224\n",
    "\n",
    "\n",
    "from lightly.loss import DINOLoss\n",
    "from lightly.models.modules import DINOProjectionHead\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "from lightly.transforms.dino_transform import DINOTransform\n",
    "from lightly.utils.scheduler import cosine_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "691317f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "dataset_1_dir = './data/testset_1'\n",
    "dataset_2_dir = './data/testset_2'\n",
    "dataset_3_dir = './data/testset_3'\n",
    "output_dir = './outputs'\n",
    "pretrain_weights = 'dino-v1/dino-v1_small_100.pt'\n",
    "resnet = torchvision.models.resnet18()\n",
    "# resnet = torchvision.models.resnet34()\n",
    "per_gpu_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a1e42b0-12e4-4414-a259-98ebf2715098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_list, labels=None,\n",
    "                 resolution=224, split=\"train\", apply_transforms=True):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.image_list = image_list\n",
    "        self.labels = labels\n",
    "        self.split = split\n",
    "        self.resolution = resolution\n",
    "        self.apply_transforms = apply_transforms\n",
    "\n",
    "        imagenet_mean = [0.485, 0.456, 0.406]\n",
    "        imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        if apply_transforms:\n",
    "            if split == \"train\":\n",
    "                self.transform = v2.Compose([\n",
    "                    v2.RandomResizedCrop(resolution, scale=(0.8, 1.0)),\n",
    "                    v2.RandomHorizontalFlip(p=0.5),\n",
    "                    v2.ColorJitter(\n",
    "                        brightness=0.4,\n",
    "                        contrast=0.4,\n",
    "                        saturation=0.4,\n",
    "                        hue=0.1\n",
    "                    ),\n",
    "                    v2.ToImage(),\n",
    "                    v2.ToDtype(torch.float32, scale=True),\n",
    "                    v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = v2.Compose([\n",
    "                    v2.Resize(256),\n",
    "                    v2.CenterCrop(resolution),\n",
    "                    v2.ToImage(),\n",
    "                    v2.ToDtype(torch.float32, scale=True),\n",
    "                    v2.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = None   # <-- important\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_list[idx]\n",
    "        img_path = self.image_dir / img_name\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)   # -> Tensor (for supervised)\n",
    "        # else: keep img as PIL (for SSL)\n",
    "\n",
    "        if self.labels is not None:\n",
    "            return img, self.labels[idx], img_name\n",
    "        return img, img_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b652cf42-2018-4352-bad0-0820f878dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle PIL images\"\"\"\n",
    "    if len(batch[0]) == 3:  # train/val (image, label, filename)\n",
    "        images = [item[0] for item in batch]\n",
    "        labels = [item[1] for item in batch]\n",
    "        filenames = [item[2] for item in batch]\n",
    "        return images, labels, filenames\n",
    "    else:  # test (image, filename)\n",
    "        images = [item[0] for item in batch]\n",
    "        filenames = [item[1] for item in batch]\n",
    "        return images, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbebdfb-cd4a-46b4-ba48-5dfaa5f73b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset metadata...\n",
      "  Train: 8232 images\n",
      "  Val:   1727 images\n",
      "  Test:  1829 images\n",
      "  Classes: 200\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Hyperparameters (replace args.*)\n",
    "# ============================================================\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "resolution = 224   # or whatever you want for training\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = Path(dataset_1_dir)\n",
    "\n",
    "print(\"\\nLoading dataset metadata...\")\n",
    "train_df = pd.read_csv(data_dir / 'train_labels.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val_labels.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} images\")\n",
    "print(f\"  Val:   {len(val_df)} images\")\n",
    "print(f\"  Test:  {len(test_df)} images\")\n",
    "print(f\"  Classes: {train_df['class_id'].nunique()}\")\n",
    "\n",
    "train_dataset1 = ImageDataset(\n",
    "    data_dir / 'train',\n",
    "    train_df['filename'].tolist(),\n",
    "    train_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "val_dataset1 = ImageDataset(\n",
    "    data_dir / 'val',\n",
    "    val_df['filename'].tolist(),\n",
    "    val_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "test_dataset1 = ImageDataset(\n",
    "    data_dir / 'test',\n",
    "    test_df['filename'].tolist(),\n",
    "    labels=test_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "train_loader1 = DataLoader(\n",
    "    train_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader1 = DataLoader(\n",
    "    val_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader1 = DataLoader(\n",
    "    test_dataset1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88ee4a0-c0d6-43bc-9c3a-c02b9fcf8ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset metadata...\n",
      "  Train: 26880 images\n",
      "  Val:   5760 images\n",
      "  Test:  5760 images\n",
      "  Classes: 64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Hyperparameters (replace args.*)\n",
    "# ============================================================\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "resolution = 224   # or whatever you want for training\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = Path(dataset_2_dir)\n",
    "\n",
    "print(\"\\nLoading dataset metadata...\")\n",
    "train_df = pd.read_csv(data_dir / 'train_labels.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val_labels.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} images\")\n",
    "print(f\"  Val:   {len(val_df)} images\")\n",
    "print(f\"  Test:  {len(test_df)} images\")\n",
    "print(f\"  Classes: {train_df['class_id'].nunique()}\")\n",
    "\n",
    "train_dataset2 = ImageDataset(\n",
    "    data_dir / 'train',\n",
    "    train_df['filename'].tolist(),\n",
    "    train_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "val_dataset2 = ImageDataset(\n",
    "    data_dir / 'val',\n",
    "    val_df['filename'].tolist(),\n",
    "    val_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "test_dataset2 = ImageDataset(\n",
    "    data_dir / 'test',\n",
    "    test_df['filename'].tolist(),\n",
    "    labels=test_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "train_loader2 = DataLoader(\n",
    "    train_dataset2,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader2 = DataLoader(\n",
    "    val_dataset2,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_loader2 = DataLoader(\n",
    "    test_dataset2,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa553af5-7b27-4817-b11b-7fa789f90444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset metadata...\n",
      "  Train: 13895 images\n",
      "  Val:   2977 images\n",
      "  Test:  2978 images\n",
      "  Classes: 397\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Hyperparameters (replace args.*)\n",
    "# ============================================================\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "resolution = 224   # or whatever you want for training\n",
    "# ============================================================\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = Path(dataset_3_dir)\n",
    "\n",
    "print(\"\\nLoading dataset metadata...\")\n",
    "train_df = pd.read_csv(data_dir / 'train_labels.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val_labels.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test_labels_INTERNAL.csv')\n",
    "\n",
    "print(f\"  Train: {len(train_df)} images\")\n",
    "print(f\"  Val:   {len(val_df)} images\")\n",
    "print(f\"  Test:  {len(test_df)} images\")\n",
    "print(f\"  Classes: {train_df['class_id'].nunique()}\")\n",
    "\n",
    "train_dataset3 = ImageDataset(\n",
    "    data_dir / 'train',\n",
    "    train_df['filename'].tolist(),\n",
    "    train_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "val_dataset3 = ImageDataset(\n",
    "    data_dir / 'val',\n",
    "    val_df['filename'].tolist(),\n",
    "    val_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "test_dataset3 = ImageDataset(\n",
    "    data_dir / 'test',\n",
    "    test_df['filename'].tolist(),\n",
    "    labels=test_df['class_id'].tolist(),\n",
    "    resolution=resolution,\n",
    "    apply_transforms=False,\n",
    ")\n",
    "\n",
    "train_loader3 = DataLoader(\n",
    "    train_dataset3,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader3 = DataLoader(\n",
    "    val_dataset3,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader3 = DataLoader(\n",
    "    test_dataset3,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eacb123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_distributed=False, rank=0, world_size=1, device=cuda:0\n"
     ]
    }
   ],
   "source": [
    "def init_distributed():\n",
    "    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n",
    "        rank = int(os.environ[\"RANK\"])\n",
    "        world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "        dist.init_process_group(backend=\"nccl\")\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.device(f\"cuda:{local_rank}\")\n",
    "        is_distributed = True\n",
    "    else:\n",
    "        # Fallback: single GPU / CPU\n",
    "        rank = 0\n",
    "        world_size = 1\n",
    "        is_distributed = False\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda:0\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "    return is_distributed, rank, world_size, device\n",
    "\n",
    "is_distributed, rank, world_size, device = init_distributed()\n",
    "print(f\"is_distributed={is_distributed}, rank={rank}, world_size={world_size}, device={device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6497ed-8306-40ca-a5c0-ac9d72db4c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINO(nn.Module):\n",
    "    def __init__(self, backbone, input_dim):\n",
    "        super().__init__()\n",
    "        self.student_backbone = backbone\n",
    "        self.student_head = DINOProjectionHead(\n",
    "            input_dim, 512, 64, 2048, freeze_last_layer=1\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(backbone)\n",
    "        self.teacher_head = DINOProjectionHead(input_dim, 512, 64, 2048)\n",
    "        deactivate_requires_grad(self.teacher_backbone)\n",
    "        deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.student_backbone(x).flatten(start_dim=1)\n",
    "        z = self.student_head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee649a90-3e50-444f-af4a-e302bb13552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build same backbone as used for DINO pretraining ---\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])  # (B, 512, 1, 1)\n",
    "input_dim = 512\n",
    "\n",
    "dino_model = DINO(backbone, input_dim)\n",
    "\n",
    "# --- Load your pre-trained DINO checkpoint ---\n",
    "ckpt = torch.load(\n",
    "    output_dir + pretrain_weights,\n",
    "    map_location=\"cpu\",\n",
    ")\n",
    "\n",
    "dino_model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42411f41-8095-401a-aea7-02c9b1a729ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SSL transform\n",
    "ssl_transform = DINOTransform()\n",
    "\n",
    "# 3. SSL dataset wrapper\n",
    "class CUB_SSL_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _, _ = self.subset[idx]\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        return self.transform(img)  # returns list[Tensor] from DINOTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dbb580-f0e5-4b82-8c85-6c8d915de2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Combine train + val subsets (no labels)\n",
    "from torch.utils.data import ConcatDataset\n",
    "ssl_trainval_raw = ConcatDataset([\n",
    "    train_dataset1, val_dataset1,\n",
    "    train_dataset2, val_dataset2,\n",
    "    train_dataset3, val_dataset3,\n",
    "])\n",
    "\n",
    "\n",
    "# 5. Build SSL dataset\n",
    "ssl_trainval_ds = CUB_SSL_Dataset(\n",
    "    subset=ssl_trainval_raw,\n",
    "    transform=ssl_transform,\n",
    ")\n",
    "\n",
    "if is_distributed:\n",
    "    train_sampler = DistributedSampler(\n",
    "        ssl_trainval_ds,\n",
    "        num_replicas=world_size,\n",
    "        rank=rank,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    shuffle = False  # sampler handles shuffling\n",
    "else:\n",
    "    train_sampler = None\n",
    "    shuffle = True\n",
    "\n",
    "# 6. Build SSL dataloader\n",
    "ssl_loader = DataLoader(\n",
    "    ssl_trainval_ds,\n",
    "    batch_size=per_gpu_batch_size,\n",
    "    shuffle=shuffle,\n",
    "    drop_last=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    # collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(\"SSL training images:\", len(ssl_trainval_ds))\n",
    "\n",
    "if ssl_loader.dataset is not None:\n",
    "    print(f\"Dataset found. Total samples: {len(ssl_loader.dataset)}\")\n",
    "    print(f\"Batch size: {ssl_loader.batch_size}\")\n",
    "    print(f\"Number of workers: {ssl_loader.num_workers}\")\n",
    "else:\n",
    "    print(\"Dataset is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289ae65-63fb-4e7b-99a1-c7a7495106e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Inspect one batch from SSL dataloader ----\n",
    "\n",
    "batch = next(iter(ssl_loader))\n",
    "\n",
    "print(\"\\n=== SSL Batch Debug ===\")\n",
    "\n",
    "if isinstance(batch, list) or isinstance(batch, tuple):\n",
    "    print(f\"Batch is a {type(batch)} with length {len(batch)}\")\n",
    "\n",
    "# DINOTransform returns a list of views per item, but DataLoader collates it into:\n",
    "# batch = list_of_views, where each element has shape (B, C, H, W)\n",
    "\n",
    "# Example: batch[0] = global crops   shape: (B, 3, 224, 224)\n",
    "#          batch[1] = global crops   shape: (B, 3, 224, 224)\n",
    "#          batch[2] = local crops    shape: (B, 3, 96, 96)\n",
    "# etc.\n",
    "\n",
    "for i, view in enumerate(batch):\n",
    "    print(f\"\\n--- View {i} ---\")\n",
    "    print(f\"Type: {type(view)}\")\n",
    "    try:\n",
    "        print(f\"Shape: {view.shape}\")\n",
    "    except Exception:\n",
    "        print(\"View has no `.shape` attribute\")\n",
    "    print(f\"Dtype: {getattr(view, 'dtype', None)}\")\n",
    "\n",
    "print(\"\\n=== End Debug ===\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a8389-884b-4c35-b74b-89a52a8cb037",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(ssl_loader))  # or ssl_loader\n",
    "\n",
    "print(\"Type of batch:\", type(batch))\n",
    "\n",
    "if isinstance(batch, (list, tuple)):\n",
    "    print(\"Batch length:\", len(batch))\n",
    "    for i, x in enumerate(batch):\n",
    "        print(f\"  item[{i}] type: {type(x)}\")\n",
    "        if hasattr(x, \"shape\"):\n",
    "            print(f\"  item[{i}] shape:\", x.shape)\n",
    "else:\n",
    "    print(\"Batch shape:\", batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3614e-61f5-4bf6-a055-2838ce5c07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Project / save dir ----------\n",
    "PROJECT_NAME = \"dino-v1\"  # folder name\n",
    "save_dir = output_dir / \"dino-v1\"\n",
    "save_dir = Path(save_dir)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Save directory ready: {save_dir}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dino_model.to(device)\n",
    "\n",
    "ckpt_path = save_dir / f\"{PROJECT_NAME}_full_finetuned_{backbone}.pt\"\n",
    "start_epoch = 0\n",
    "\n",
    "optimizer = torch.optim.Adam(dino_model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = DINOLoss(\n",
    "    output_dim=2048,\n",
    "    warmup_teacher_temp_epochs=5,\n",
    ")\n",
    "# move loss to correct device because it also contains parameters\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# ---------- Training loop ----------\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 100\n",
    "print(\"Starting SSL Training (DINO)\")\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    dino_model.train()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # EMA momentum for teacher this epoch\n",
    "    momentum_val = cosine_schedule(epoch, num_epochs, 0.996, 1.0)\n",
    "\n",
    "    for batch in tqdm(ssl_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "\n",
    "        # ssl_loader should give a list of crops (views)\n",
    "        # Handle both \"views\" and \"(views, _)\" cases\n",
    "        if isinstance(batch, (list, tuple)) and isinstance(batch[0], torch.Tensor):\n",
    "            views = batch\n",
    "        elif isinstance(batch, (list, tuple)):\n",
    "            views = batch[0]\n",
    "        else:\n",
    "            views = batch\n",
    "\n",
    "        # move all crops to GPU\n",
    "        views = [v.to(device, non_blocking=True) for v in views]\n",
    "\n",
    "        # ---- EMA update for teacher ----\n",
    "        update_momentum(dino_model.student_backbone, dino_model.teacher_backbone, m=momentum_val)\n",
    "        update_momentum(dino_model.student_head,     dino_model.teacher_head,     m=momentum_val)\n",
    "\n",
    "        # first two are global crops for teacher\n",
    "        global_views = views[:2]\n",
    "\n",
    "        # teacher on global crops (no grad)\n",
    "        with torch.no_grad():\n",
    "            teacher_out = [dino_model.forward_teacher(v) for v in global_views]\n",
    "\n",
    "        # student on all crops (global + local)\n",
    "        student_out = [dino_model(v) for v in views]\n",
    "\n",
    "        # ---- DINO loss ----\n",
    "        loss = criterion(teacher_out, student_out, epoch=epoch)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        dino_model.student_head.cancel_last_layer_gradients(current_epoch=epoch)\n",
    "        optimizer.step()\n",
    "\n",
    "        bsz = views[0].size(0)\n",
    "        total_loss += loss.item() * bsz\n",
    "        n_samples += bsz\n",
    "        global_step += 1\n",
    "\n",
    "    avg_loss = total_loss / max(n_samples, 1)\n",
    "    elapsed = time.time() - epoch_start\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train loss: {avg_loss:.5f} | time: {elapsed:.1f}s\")\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        # ---- Save checkpoint (always same filename) ----\n",
    "        ckpt = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": dino_model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"avg_loss\": avg_loss,\n",
    "        }\n",
    "        torch.save(ckpt, ckpt_path)\n",
    "        print(f\"âœ“ Saved checkpoint: {ckpt_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ijepa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
